{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the syntax to load the different autoencoders that we support. Autoencoders are \"attached\" to the model using the `edit` method from NNsight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GemmaScope SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from sae_auto_interp.autoencoders import load_gemma_autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "model = LanguageModel(\"google/gemma-2-9b\", device_map=\"cuda\", dispatch=True,torch_dtype=\"bfloat16\")\n",
    "\n",
    "# Load the autoencoders, the function returns a dictionary of the submodules with the autoencoders and the edited model.\n",
    "# it takes as arguments the model, the layers to load the autoencoders into,\n",
    "# the average L0 sparsity per layer, the size of the autoencoders and the type of autoencoders (residuals or MLPs).\n",
    "\n",
    "submodule_dict,model = load_gemma_autoencoders(\n",
    "            model,\n",
    "            layers=[10],\n",
    "            average_l0s={10: 47},\n",
    "            size=\"131k\",\n",
    "            type=\"res\"\n",
    "        )\n",
    "\n",
    "# The autoencoder is loaded into the submodules dictionary and the model is edited in place.\n",
    "autoencoder = submodule_dict[\".model.layers.10\"].ae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EleutherAI SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from sae_auto_interp.autoencoders import load_eai_autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e136989aab454fbea2be68c6c17bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141aa6f78df6400a96baa84c5d8f938c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec3eda1c197496ca349c4ca74c33ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530206fe2e7c477088441ac434e23282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e02673baa04dc681d44908ab5442bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1ae6510ee1464f94f84ac6313b2ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c0d87ca89445ea3d8f5f1cc4ea238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58be29a509354d40a30940b9b16e3a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7a705d804e4100a18238e0c9bbe9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9166df9db6574f9098318c5a941b272e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b79a7f3a88e46b3b3185e266693767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa844ea6e0b49ed8b64dd855df775fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef05d74669b34f919f5a57199326c5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ec0ded3a74401d8a772c4666f081a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layers.23.mlp/cfg.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bf1e2ebd104618babc6cca399e0754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01778513a814db98e9584087d3776eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c41369b127f414abe862315fe287e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sae.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'.model.layers.10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m submodule_dict,model \u001b[38;5;241m=\u001b[39m load_eai_autoencoders(\n\u001b[1;32m     10\u001b[0m     model,\n\u001b[1;32m     11\u001b[0m     [\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m29\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# The autoencoder is loaded into the submodules dictionary and the model is edited in place.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43msubmodule_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.model.layers.10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mae\n",
      "\u001b[0;31mKeyError\u001b[0m: '.model.layers.10'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3.1-8B\", device_map=\"cpu\",dispatch=True,torch_dtype=\"bfloat16\")\n",
    "    \n",
    "# The load function takes as arguments the model, the layers to load the autoencoders into,\n",
    "# The path of the weights (it can be a huggingface path or a local path),\n",
    "# the type of module to load the autoencoders into (residuals or MLPs),\n",
    "# whether to randomize the autoencoders or not,\n",
    "# whether to use the trained k or a specific k on the top-k activation.\n",
    "\n",
    "submodule_dict,model = load_eai_autoencoders(\n",
    "    model,\n",
    "    [23,29],\n",
    "    weight_dir=\"EleutherAI/sae-llama-3.1-8b-64x\",\n",
    "    module=\"mlp\",\n",
    "    randomize=False,\n",
    "    k=None\n",
    ")\n",
    "\n",
    "# The autoencoder is loaded into the submodules dictionary and the model is edited in place.\n",
    "autoencoder = submodule_dict[\".model.layers.23\"].ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
