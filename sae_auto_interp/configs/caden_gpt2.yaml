# logger output path
log_path: "logs/run.log"

cache:
    dataset_repo: "kh4dien/fineweb-100m-sample"
    dataset_split: "train[:15%]"

    # Not all datasets use this
    dataset_name: ""

    # tokens_per_batch per batch is minibatch_size * batch_len. n_batches is n_tokens // tokens_per_batch
    minibatch_size: 32

    # n tokens per index per batch
    batch_len: 64

    # width of the autoencoder
    n_features: 32768

    # n_tokens we'd like to process
    n_tokens: 10_240_000

    # shuffle token seed for reproducibility
    seed: 22

    # Number of tokens to the left of each activation
    l_ctx: 15

    # Numeber of tokens to the right of each activation
    r_ctx: 4

cot_explainer: 
    temperature: 0.0
    max_tokens: 1000
    l: "<<"
    r: ">>"
    threshold: 0.4

detection:
    max_tokens: 1000
    temperature: 0.0
    seed: 22
    batch_size: 30
    threshold: 0.3

    l: "<<"
    r: ">>"

simple_explainer:
    temperature: 0.0
    max_tokens: 100
    
generation: 
    n_tests: 10
    temperature: 0.0