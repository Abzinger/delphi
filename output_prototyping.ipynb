{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_correlation_score</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature0</th>\n",
       "      <td>0.970093</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature19</th>\n",
       "      <td>0.966378</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature0</th>\n",
       "      <td>0.952401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature4</th>\n",
       "      <td>0.952061</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature5</th>\n",
       "      <td>0.949993</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature4</th>\n",
       "      <td>0.941871</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature11</th>\n",
       "      <td>0.930066</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature19</th>\n",
       "      <td>0.918787</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature14</th>\n",
       "      <td>0.906342</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature3</th>\n",
       "      <td>0.897080</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature11</th>\n",
       "      <td>0.883083</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature9</th>\n",
       "      <td>0.868529</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature3</th>\n",
       "      <td>0.810241</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature18</th>\n",
       "      <td>0.805457</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature16</th>\n",
       "      <td>0.782019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature11</th>\n",
       "      <td>0.779133</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature10</th>\n",
       "      <td>0.772255</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature7</th>\n",
       "      <td>0.764754</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature7</th>\n",
       "      <td>0.712047</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature12</th>\n",
       "      <td>0.711850</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature0</th>\n",
       "      <td>0.698656</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature12</th>\n",
       "      <td>0.678797</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature15</th>\n",
       "      <td>0.668821</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature13</th>\n",
       "      <td>0.668621</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature1</th>\n",
       "      <td>0.662348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature2</th>\n",
       "      <td>0.659443</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature13</th>\n",
       "      <td>0.649270</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature3</th>\n",
       "      <td>0.645487</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature6</th>\n",
       "      <td>0.643440</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature17</th>\n",
       "      <td>0.627347</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature8</th>\n",
       "      <td>0.583940</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature15</th>\n",
       "      <td>0.556673</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature4</th>\n",
       "      <td>0.470279</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature6</th>\n",
       "      <td>0.447680</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature7</th>\n",
       "      <td>0.390438</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature18</th>\n",
       "      <td>0.378467</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature14</th>\n",
       "      <td>0.316718</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature9</th>\n",
       "      <td>0.272719</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature10</th>\n",
       "      <td>0.228020</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature13</th>\n",
       "      <td>0.167355</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature5</th>\n",
       "      <td>0.095751</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature2</th>\n",
       "      <td>0.023048</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature15</th>\n",
       "      <td>-0.060541</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ev_correlation_score  layer  feature\n",
       ".transformer.h.2_feature0               0.970093      2        0\n",
       ".transformer.h.2_feature19              0.966378      2       19\n",
       ".transformer.h.0_feature0               0.952401      0        0\n",
       ".transformer.h.4_feature4               0.952061      4        4\n",
       ".transformer.h.0_feature5               0.949993      0        5\n",
       ".transformer.h.2_feature4               0.941871      2        4\n",
       ".transformer.h.2_feature11              0.930066      2       11\n",
       ".transformer.h.4_feature19              0.918787      4       19\n",
       ".transformer.h.0_feature14              0.906342      0       14\n",
       ".transformer.h.0_feature3               0.897080      0        3\n",
       ".transformer.h.0_feature11              0.883083      0       11\n",
       ".transformer.h.0_feature9               0.868529      0        9\n",
       ".transformer.h.2_feature3               0.810241      2        3\n",
       ".transformer.h.2_feature18              0.805457      2       18\n",
       ".transformer.h.0_feature16              0.782019      0       16\n",
       ".transformer.h.4_feature11              0.779133      4       11\n",
       ".transformer.h.0_feature10              0.772255      0       10\n",
       ".transformer.h.0_feature7               0.764754      0        7\n",
       ".transformer.h.2_feature7               0.712047      2        7\n",
       ".transformer.h.0_feature12              0.711850      0       12\n",
       ".transformer.h.4_feature0               0.698656      4        0\n",
       ".transformer.h.4_feature12              0.678797      4       12\n",
       ".transformer.h.4_feature15              0.668821      4       15\n",
       ".transformer.h.4_feature13              0.668621      4       13\n",
       ".transformer.h.2_feature1               0.662348      2        1\n",
       ".transformer.h.2_feature2               0.659443      2        2\n",
       ".transformer.h.0_feature13              0.649270      0       13\n",
       ".transformer.h.4_feature3               0.645487      4        3\n",
       ".transformer.h.2_feature6               0.643440      2        6\n",
       ".transformer.h.4_feature17              0.627347      4       17\n",
       ".transformer.h.4_feature8               0.583940      4        8\n",
       ".transformer.h.2_feature15              0.556673      2       15\n",
       ".transformer.h.0_feature4               0.470279      0        4\n",
       ".transformer.h.0_feature6               0.447680      0        6\n",
       ".transformer.h.4_feature7               0.390438      4        7\n",
       ".transformer.h.0_feature18              0.378467      0       18\n",
       ".transformer.h.4_feature14              0.316718      4       14\n",
       ".transformer.h.4_feature9               0.272719      4        9\n",
       ".transformer.h.4_feature10              0.228020      4       10\n",
       ".transformer.h.2_feature13              0.167355      2       13\n",
       ".transformer.h.2_feature5               0.095751      2        5\n",
       ".transformer.h.4_feature2               0.023048      4        2\n",
       ".transformer.h.0_feature15             -0.060541      0       15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "results_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/results/gpt2_simulation/all_at_once\"\n",
    "results = dict()\n",
    "for fname in Path(results_dir).iterdir():\n",
    "    with open(fname, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "    last = fname.stem.split(\".\")[-1]\n",
    "    layer = int(last.split(\"_\")[0])\n",
    "    feat = int(last[last.index(\"_feature\") + len(\"_feature\"):])\n",
    "    results[fname.stem] = {\"ev_correlation_score\": r[\"ev_correlation_score\"], \"layer\": layer, \"feature\": feat}\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df[\"layer\"] = results_df[\"layer\"].astype(int)\n",
    "results_df[\"feature\"] = results_df[\"feature\"].astype(int)\n",
    "results_df = results_df.sort_values(\"ev_correlation_score\", ascending=False)\n",
    "unq_layers = results_df[\"layer\"].unique()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/autointerp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "# Load the GPT-2 model\n",
    "# dispatch=True is necessary to load the weights\n",
    "model = LanguageModel('openai-community/gpt2', device_map='cuda', dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"pile.jsonl\", \"r\") as f:\n",
    "    pile = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3484 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 5000/5000 [00:17<00:00, 279.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "n_samples = 5000\n",
    "sample_texts = [s[\"text\"] for s in random.sample(pile, n_samples)]\n",
    "freqs = defaultdict(int)\n",
    "for text in tqdm(sample_texts):\n",
    "    tok_ids = model.tokenizer.encode(text, add_special_tokens=False)\n",
    "    for tok_id in tok_ids:\n",
    "        freqs[tok_id] += 1\n",
    "unigram_p = torch.zeros(model.tokenizer.vocab_size, device=\"cuda:0\")\n",
    "total = sum(freqs.values())\n",
    "for tok_id, freq in freqs.items():\n",
    "    unigram_p[tok_id] = freq / total\n",
    "unigram_p.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1973e-04, 1.8643e-03, 9.3495e-04,  ..., 7.8832e-07, 5.6309e-07,\n",
       "        0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del pile\n",
    "unigram_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "explainer_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "explainer = AutoModelForCausalLM.from_pretrained(explainer_name).to(torch.bfloat16).to(\"cuda:0\")\n",
    "explainer_tokenizer = AutoTokenizer.from_pretrained(explainer_name)\n",
    "explainer_tokenizer.pad_token = explainer_tokenizer.eos_token\n",
    "explainer_tokenizer.pad_token_id = explainer_tokenizer.eos_token_id\n",
    "explainer.generation_config.pad_token_id = explainer_tokenizer.eos_token_id\n",
    "explainer.config.pad_token_id = explainer_tokenizer.eos_token_id\n",
    "\n",
    "# NOTE: scorer is explainer for now, to save space\n",
    "scorer = explainer\n",
    "scorer_tokenizer = explainer_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ' Apples' (+10), 'BASKET' (+8), ' orange' (+8), ' Salad' (+4), ' rosemary' (+3), ' far' (+3), 'Field' (+3)\n",
      "Explanation: fruits and vegetables\n",
      "\n",
      "Tokens: 'ateg' (+5), ' vulnerabilities' (+5), 'agame' (+4), ' geek' (+4), 'arily' (+4), ' eclectic' (+4), ' mole' (+4), 'WAY' (+4), ' angle' (+4), ' Warcraft' (+4)\n",
      "Explanation: ateg\n",
      "\n",
      "Tokens: ' she' (+11), ' her' (+10), ' hers' (+10), ' she's' (+7), ' Her' (+5), ' She' (+3), ' Ms.' (+3), ' maj' (+2)\n",
      "Explanation: she/her pronouns\n",
      "\n",
      "Tokens: ' Apples' (+10), 'BASKET' (+8), ' orange' (+8), ' Salad' (+4), ' rosemary' (+3), ' far' (+3), 'Field' (+3)\n",
      "Explanation:\n",
      "Explanation: fruits and vegetables\n",
      "Sample: ' Oranges'\n",
      "\n",
      "Explanation: ateg\n",
      "Sample: 'WAY'\n",
      "\n",
      "Explanation: she/her pronouns\n",
      "Sample: ' hers'\n",
      "\n",
      "Explanation: fruits and vegetables\n",
      "Sample: '\n"
     ]
    }
   ],
   "source": [
    "def get_explainer_prompt(tokens, logit_contributions, few_shot_tokens_lists=None, few_shot_logit_contributions_lists=None, few_shot_explanations=None, logits_prompt_scale=1.0):\n",
    "    \n",
    "    if few_shot_tokens_lists is not None:\n",
    "        assert few_shot_logit_contributions_lists is not None and few_shot_explanations is not None\n",
    "        assert len(few_shot_tokens_lists) == len(few_shot_logit_contributions_lists) == len(few_shot_explanations)\n",
    "        few_shot_prompt = \"\\n\\n\".join(get_explainer_prompt(toks, logits, logits_prompt_scale=logits_prompt_scale) + f\" {expl.strip()}\" for toks, logits, expl in zip(few_shot_tokens_lists, few_shot_logit_contributions_lists, few_shot_explanations)) + \"\\n\\n\"\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    tokens_str = \", \".join(f\"'{tok}' (+{round(logit * logits_prompt_scale)})\" for tok, logit in zip(tokens, logit_contributions))\n",
    "    return few_shot_prompt + f\"Tokens: {tokens_str}\\nExplanation:\"\n",
    "\n",
    "def get_scorer_simplicity_prompt(explanation):\n",
    "    prefix = \"Explanation\\n\\n\"\n",
    "    return f\"{prefix}{explanation}{scorer_tokenizer.eos_token}\", prefix\n",
    "\n",
    "def get_scorer_predictiveness_prompt(explanation, few_shot_explanations=None, few_shot_tokens=None):\n",
    "    if few_shot_explanations is not None:\n",
    "        assert few_shot_tokens is not None\n",
    "        assert len(few_shot_explanations) == len(few_shot_tokens)\n",
    "        few_shot_prompt = \"'\\n\\n\".join(get_scorer_predictiveness_prompt(explanation) + token for explanation, token in zip(few_shot_explanations, few_shot_tokens)) + \"'\\n\\n\"\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    return few_shot_prompt + f\"Explanation: {explanation}\\nSample: '\"\n",
    "\n",
    "few_shot_tokens_lists = [[\" Apples\", \"BASKET\", \" orange\", \" Salad\", \" rosemary\", \" far\", \"Field\"],\n",
    "                         [\"ateg\", \" vulnerabilities\", \"agame\", \" geek\", \"arily\", \" eclectic\", \" mole\", \"WAY\", \" angle\", \" Warcraft\"],\n",
    "                         [\" she\", \" her\", \" hers\", \" she's\", \" Her\", \" She\", \" Ms.\", \" maj\"]]\n",
    "few_shot_logit_contributions_lists = [[10, 8, 8, 4, 3, 3, 3],\n",
    "                                    [5, 5, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "                                    [11, 10, 10, 7, 5, 3, 3, 2]]\n",
    "few_shot_explanations = [\"fruits and vegetables\", \"ateg\", \"she/her pronouns\"]\n",
    "print(get_explainer_prompt(few_shot_tokens_lists[0], few_shot_logit_contributions_lists[0], few_shot_tokens_lists, few_shot_logit_contributions_lists, few_shot_explanations))\n",
    "\n",
    "few_shot_tokens = [\" Oranges\", \"WAY\", \" hers\"]\n",
    "print(get_scorer_predictiveness_prompt(few_shot_explanations[0], few_shot_explanations, few_shot_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.autoencoders import load_oai_autoencoders\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "k = 10\n",
    "logits_prompt_scale = 10\n",
    "\n",
    "lm_head = model.lm_head\n",
    "aes = load_oai_autoencoders(model, unq_layers.tolist(), \"/mnt/ssd-1/gpaulo/SAE-Zoology/weights/gpt2_128k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "  2%|▏         | 1/43 [00:09<06:29,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 0.0, score: -43.077545166015625, explanation: acknow, predictiveness_score: -15.30282211303711, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'lehem' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/43 [00:17<06:06,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 19.0, score: -43.0294189453125, explanation: acknow, predictiveness_score: -15.254693031311035, simplicity_score: -27.77472496032715\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/43 [00:30<06:59, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 0.0, score: -26.621410369873047, explanation: the, predictiveness_score: -8.606150817871093, simplicity_score: -18.0152587890625\n",
      "Tokens: ' the' (+9), ' a' (+8), ',' (+8), ' \"' (+7), ' and' (+7), '-' (+7), '\n",
      "' (+7), '.' (+7), ' in' (+7), ' (' (+6)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4/43 [00:42<07:15, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 4.0, score: -22.36276626586914, explanation: the, predictiveness_score: -4.347506976127624, simplicity_score: -18.0152587890625\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5/43 [00:51<06:30, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 5.0, score: -45.819496154785156, explanation: Azerb, predictiveness_score: -15.66209602355957, simplicity_score: -30.15740203857422\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6/43 [00:59<06:00,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 4.0, score: -42.97443389892578, explanation: acknow, predictiveness_score: -15.199707221984863, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 7/43 [01:08<05:39,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 11.0, score: -42.92322540283203, explanation: acknow, predictiveness_score: -15.148501777648926, simplicity_score: -27.77472496032715\n",
      "Tokens: 'lehem' (+9), ' Yanuk' (+9), ' acknow' (+9), ' Azerb' (+9), 'Buyable' (+9), ' misunder' (+9), 'iscons' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 8/43 [01:17<05:22,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 19.0, score: -38.95695495605469, explanation: lehem, predictiveness_score: -15.664071655273437, simplicity_score: -23.29288101196289\n",
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'lehem' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 9/43 [01:26<05:09,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 14.0, score: -43.068092346191406, explanation: acknow, predictiveness_score: -15.293366050720214, simplicity_score: -27.77472496032715\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 10/43 [01:38<05:32, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 3.0, score: -33.97047424316406, explanation: punctuation, predictiveness_score: -9.611821365356445, simplicity_score: -24.358652114868164\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 11/43 [01:47<05:08,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 11.0, score: -42.934959411621094, explanation: acknow, predictiveness_score: -15.160234832763672, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'lehem' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 12/43 [01:55<04:50,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 9.0, score: -42.99859619140625, explanation: acknow, predictiveness_score: -15.223869132995606, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 13/43 [02:04<04:34,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 3.0, score: -42.9338264465332, explanation: acknow, predictiveness_score: -15.159101104736328, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 14/43 [02:13<04:21,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 18.0, score: -42.95878219604492, explanation: acknow, predictiveness_score: -15.184056282043457, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 15/43 [02:22<04:10,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 16.0, score: -42.91505432128906, explanation: acknow, predictiveness_score: -15.14033145904541, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' Azerb' (+10), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), 'Buyable' (+9), ' indo' (+9), 'MpServer' (+9), 'rongh' (+9), 'umbn' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 16/43 [02:30<04:00,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 11.0, score: -41.858028411865234, explanation: acknow, predictiveness_score: -14.08330421447754, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 17/43 [02:40<03:53,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 10.0, score: -42.91513442993164, explanation: acknow, predictiveness_score: -15.140408897399903, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 18/43 [02:49<03:44,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 7.0, score: -42.90428161621094, explanation: acknow, predictiveness_score: -15.129556655883789, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9), 'lehem' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 19/43 [02:58<03:37,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 7.0, score: -44.48109436035156, explanation: azerbaijan, predictiveness_score: -14.972125625610351, simplicity_score: -29.50896644592285\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'lehem' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 20/43 [03:07<03:25,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 12.0, score: -42.9997444152832, explanation: acknow, predictiveness_score: -15.225020790100098, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), ' misunder' (+9), 'iscons' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'lehem' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 21/43 [03:15<03:16,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 0.0, score: -43.00634765625, explanation: acknow, predictiveness_score: -15.231623458862305, simplicity_score: -27.77472496032715\n",
      "Tokens: 'iscons' (+9), ' acknow' (+9), 'Buyable' (+9), ' Azerb' (+9), ' Yanuk' (+9), 'BuyableInstoreAndOnline' (+9), 'lehem' (+9), ' misunder' (+9), 'MpServer' (+9), ' indo' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 22/43 [03:24<03:05,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 12.0, score: -39.59349822998047, explanation: iscons, predictiveness_score: -12.894300079345703, simplicity_score: -26.699199676513672\n",
      "Tokens: 'lehem' (+9), ' showc' (+9), ' acknow' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), ' Yanuk' (+9), ' Azerb' (+9), 'iscons' (+9), 'Buyable' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 23/43 [03:33<02:55,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 15.0, score: -39.116241455078125, explanation: lehem, predictiveness_score: -15.823362159729005, simplicity_score: -23.29288101196289\n",
      "Tokens: 'lehem' (+9), ' Yanuk' (+9), ' acknow' (+9), ' showc' (+9), ' misunder' (+9), 'Buyable' (+9), 'FontSize' (+8), 'iscons' (+8), 'BuyableInstoreAndOnline' (+8), ' Azerb' (+8)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 24/43 [03:41<02:47,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 13.0, score: -39.10308074951172, explanation: lehem, predictiveness_score: -15.810199165344239, simplicity_score: -23.29288101196289\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 25/43 [03:50<02:39,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 1.0, score: -38.84651565551758, explanation: Acknowledgment, predictiveness_score: -15.88178997039795, simplicity_score: -22.964725494384766\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 26/43 [04:00<02:32,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 2.0, score: -42.907447814941406, explanation: acknow, predictiveness_score: -15.132724189758301, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 27/43 [04:09<02:24,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 13.0, score: -42.94390106201172, explanation: acknow, predictiveness_score: -15.16917724609375, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' Azerb' (+10), ' indo' (+10), 'BuyableInstoreAndOnline' (+10), ' misunder' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 28/43 [04:18<02:15,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 3.0, score: -37.55492401123047, explanation: Acknowledgement, predictiveness_score: -15.264116477966308, simplicity_score: -22.29080581665039\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' Azerb' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 29/43 [04:27<02:06,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 6.0, score: -42.961177825927734, explanation: acknow, predictiveness_score: -15.186453437805175, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), ' indo' (+10), 'BuyableInstoreAndOnline' (+10), ' Azerb' (+10), ' misunder' (+10), 'iscons' (+10), 'ngth' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 30/43 [04:39<02:09,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 17.0, score: -38.08049392700195, explanation: Acknowledgement, predictiveness_score: -15.789687728881836, simplicity_score: -22.29080581665039\n",
      "Tokens: ' acknow' (+10), ' misunder' (+10), ' indo' (+10), 'BuyableInstoreAndOnline' (+10), ' Azerb' (+10), 'lehem' (+10), 'iscons' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 31/43 [04:48<01:56,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 8.0, score: -42.92267990112305, explanation: acknow, predictiveness_score: -15.147956085205077, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 32/43 [04:57<01:44,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 15.0, score: -42.86839294433594, explanation: acknow, predictiveness_score: -15.093669509887695, simplicity_score: -27.77472496032715\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 33/43 [05:09<01:43, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 4.0, score: -28.19586944580078, explanation: the, predictiveness_score: -10.180610466003419, simplicity_score: -18.0152587890625\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), 'lehem' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 34/43 [05:18<01:28,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 6.0, score: -43.02333450317383, explanation: acknow, predictiveness_score: -15.248610496520996, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+9), 'lehem' (+9), ' indo' (+9), ' misunder' (+9), ' showc' (+9), 'BuyableInstoreAndOnline' (+9), ' Yanuk' (+9), ' Azerb' (+9), 'iscons' (+9), 'Buyable' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 35/43 [05:27<01:16,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 7.0, score: -43.20270538330078, explanation: acknow, predictiveness_score: -15.42798023223877, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'lehem' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 36/43 [05:36<01:05,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 18.0, score: -43.0321159362793, explanation: acknow, predictiveness_score: -15.257390594482422, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+10), ' indo' (+10), ' Azerb' (+10), 'BuyableInstoreAndOnline' (+10), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 37/43 [05:45<00:54,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 14.0, score: -42.80078125, explanation: acknow, predictiveness_score: -15.026054763793946, simplicity_score: -27.77472496032715\n",
      "Tokens: ' Azerb' (+10), ' acknow' (+9), ' indo' (+9), 'Buyable' (+9), 'iscons' (+9), ' Yanuk' (+9), ' misunder' (+9), 'lehem' (+9), 'BuyableInstoreAndOnline' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 38/43 [05:53<00:45,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 9.0, score: -45.88768005371094, explanation: Azerb, predictiveness_score: -15.730278396606446, simplicity_score: -30.15740203857422\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' Azerb' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 39/43 [06:02<00:35,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 10.0, score: -43.00431442260742, explanation: acknow, predictiveness_score: -15.229588508605957, simplicity_score: -27.77472496032715\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'ngth' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 40/43 [06:11<00:26,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 13.0, score: -42.746604919433594, explanation: acknow, predictiveness_score: -14.97188205718994, simplicity_score: -27.77472496032715\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 41/43 [06:19<00:17,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 2.0, feature: 5.0, score: -26.735610961914062, explanation: the, predictiveness_score: -8.720352077484131, simplicity_score: -18.0152587890625\n",
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' misunder' (+9), 'lehem' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' showc' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 42/43 [06:28<00:08,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 4.0, feature: 2.0, score: -43.19207000732422, explanation: acknow, predictiveness_score: -15.41734275817871, simplicity_score: -27.77472496032715\n",
      "Tokens: 'lehem' (+9), 'iscons' (+9), ' acknow' (+9), ' Yanuk' (+9), ' Azerb' (+9), 'Buyable' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'MpServer' (+9)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [06:37<00:00,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0.0, feature: 15.0, score: -38.13185119628906, explanation: lehem, predictiveness_score: -14.83897132873535, simplicity_score: -23.29288101196289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_scores(feat, layer):\n",
    "    # simple logit lens without accounting for layer norm\n",
    "    with torch.inference_mode():\n",
    "        feat = feat / feat.norm()\n",
    "        for l in range(layer + 1, len(model.transformer.h)):\n",
    "            # feat = model.transformer.h[l].ln_1.weight * feat  # only ln2 is applied to the residual stream\n",
    "            feat = model.transformer.h[l].ln_2.weight * feat\n",
    "            # the weight changes direction and the norm (it's a diagonal transformation)\n",
    "            # but we don't care about the norm, so we normalize it for numerical stability\n",
    "            feat = feat / feat.norm()  \n",
    "        logit_contributions = lm_head.weight.data @ feat\n",
    "\n",
    "\n",
    "    top_logits = logit_contributions.topk(k)\n",
    "    # bottom_logits = logit_contributions.topk(k, largest=False)\n",
    "    top_tokens = [model.tokenizer.decode(tok) for tok in top_logits.indices]\n",
    "    print(get_explainer_prompt(top_tokens, top_logits.values.tolist(), logits_prompt_scale=logits_prompt_scale))\n",
    "\n",
    "    # get explanation\n",
    "    explainer_prompt = get_explainer_prompt(top_tokens, top_logits.values.tolist(), few_shot_tokens_lists, few_shot_logit_contributions_lists, few_shot_explanations, logits_prompt_scale=logits_prompt_scale)\n",
    "    explainer_input_ids = explainer_tokenizer(explainer_prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "    with torch.inference_mode():  # TODO: cache the few-shot prompt\n",
    "        samples = explainer.generate(explainer_input_ids, max_new_tokens=100, eos_token_id=explainer_tokenizer.encode(\"\\n\\n\")[-1], num_return_sequences=10)[:, explainer_input_ids.shape[1]:]\n",
    "\n",
    "    explanations = Counter([explainer_tokenizer.decode(sample).split(\"\\n\\n\")[0].strip() for sample in samples])\n",
    "    explanation = explanations.most_common(1)[0][0]\n",
    "\n",
    "    # get the explanation simplicity\n",
    "    simplicity_prompt, simplicity_prompt_prefix = get_scorer_simplicity_prompt(explanation)\n",
    "    simplicity_prefix_input_ids = scorer_tokenizer(simplicity_prompt_prefix, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "    simplicity_explanation_input_ids = scorer_tokenizer(simplicity_prompt[len(simplicity_prompt_prefix):], return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda:0\")\n",
    "    simplicity_input_ids = torch.cat([simplicity_prefix_input_ids, simplicity_explanation_input_ids], dim=1)\n",
    "    with torch.inference_mode():\n",
    "        # the loss is averaged over the sequence length, so we need to scale it back\n",
    "        labels = simplicity_input_ids.clone()\n",
    "        labels[:, :len(simplicity_prefix_input_ids)] = -100\n",
    "        logp_explanation = -scorer(simplicity_input_ids, labels=labels).loss * simplicity_explanation_input_ids.shape[1]\n",
    "\n",
    "    # get the explanation predictiveness\n",
    "    scorer_predictiveness_prompt = get_scorer_predictiveness_prompt(explanation, few_shot_explanations, few_shot_tokens)\n",
    "    scorer_input_ids = scorer_tokenizer(scorer_predictiveness_prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "    with torch.inference_mode():\n",
    "        scorer_logits = scorer(scorer_input_ids).logits[0, -1, :]\n",
    "        scorer_logp = scorer_logits.log_softmax(dim=-1)\n",
    "\n",
    "    # here is where we assume that the \"clean\" output distribution is uniform, \n",
    "    # so that the intervened distribtion is entirely determined by the logit contributions and the intervention strength (equivalent to 1/temperature)\n",
    "    predictiveness_scores = []\n",
    "    max_intervened_probs = []\n",
    "    for intervention_strength in intervention_strengths:\n",
    "        intervened_probs = (logit_contributions * intervention_strength + unigram_p.log()).softmax(dim=-1)\n",
    "        max_intervened_probs.append(max(intervened_probs).item())\n",
    "\n",
    "\n",
    "        scorer_vocab = scorer_tokenizer.vocab  # for some reason this takes 34 ms so we need to factor it out\n",
    "        predictiveness_score = torch.tensor(0.0, device=\"cuda:0\")\n",
    "        for subj_tok, subj_id in model.tokenizer.vocab.items():\n",
    "            if subj_tok in scorer_vocab:\n",
    "                scorer_tok = subj_tok\n",
    "            else:\n",
    "                # we need to map the subject model's tokens to the scorer model token that has the longest common prefix\n",
    "                for i in range(len(subj_tok) - 1, 0, -1):\n",
    "                    if subj_tok[:i] in scorer_vocab:\n",
    "                        scorer_tok = subj_tok[:i]\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"No scorer token found for {subj_tok}\")\n",
    "            predictiveness_score += intervened_probs[subj_id] * scorer_logp[scorer_vocab[scorer_tok]]\n",
    "        \n",
    "        predictiveness_scores.append(predictiveness_score.item())\n",
    "    \n",
    "    predictiveness_score = sum(predictiveness_scores) / len(predictiveness_scores)\n",
    "\n",
    "    return {\n",
    "        \"predictiveness_score\": predictiveness_score,\n",
    "        \"simplicity_score\": logp_explanation.item(),\n",
    "        \"score\": (predictiveness_score + logp_explanation).item(),\n",
    "        \"explanation\": explanation,\n",
    "        \"top_tokens\": top_tokens,\n",
    "        \"top_logits\": top_logits.values.tolist(),\n",
    "        \"max_intervened_probs\": max_intervened_probs,\n",
    "        \"predictiveness_scores\": predictiveness_scores,\n",
    "        \"intervention_strengths\": intervention_strengths,\n",
    "    }\n",
    "\n",
    "# Intervention strength is equivalent to 1/temperature\n",
    "# tuned so that top tokens have a reasonable probability\n",
    "# If this is too high, it's just the argmax and then every feature trivially can get a high score (expl = argmax)\n",
    "# If this is too low, it's a uniform output distribution and every feature gets roughly the same score\n",
    "intervention_strengths = [3, 10, 32, 100, 320]\n",
    "results = []\n",
    "for row in tqdm(results_df.iloc, total=len(results_df)):\n",
    "    feat = aes[f\".transformer.h.{int(row['layer'])}\"].ae.ae.decoder.weight.data[:, int(row['feature'])]\n",
    "    result = get_scores(feat, int(row['layer']))\n",
    "    print(f\"layer: {row['layer']}, feature: {row['feature']}, score: {result['score']}, explanation: {result['explanation']}, predictiveness_score: {result['predictiveness_score']}, simplicity_score: {result['simplicity_score']}\")\n",
    "    results.append({\n",
    "        **result,\n",
    "        **row.to_dict()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>simplicity_score</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "      <th>top_tokens</th>\n",
       "      <th>top_logits</th>\n",
       "      <th>max_intervened_probs</th>\n",
       "      <th>predictiveness_scores</th>\n",
       "      <th>intervention_strengths</th>\n",
       "      <th>ev_correlation_score</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-15.302822</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.077545</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9483156800270081, 0.9328060746192932, 0.927...</td>\n",
       "      <td>[0.03869502246379852, 0.09587770700454712, 0.2...</td>\n",
       "      <td>[-12.488578796386719, -13.774748802185059, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-15.254693</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.029419</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9549833536148071, 0.9397922158241272, 0.934...</td>\n",
       "      <td>[0.03839239105582237, 0.09543728828430176, 0.2...</td>\n",
       "      <td>[-12.4965181350708, -13.78377628326416, -15.51...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.966378</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.606151</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-26.621410</td>\n",
       "      <td>the</td>\n",
       "      <td>[ the, ,, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8696240782737732, 0.8686282634735107, 0.800...</td>\n",
       "      <td>[0.10830873996019363, 0.2725183069705963, 0.47...</td>\n",
       "      <td>[-9.63817024230957, -9.544039726257324, -8.534...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.952401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.347507</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-22.362766</td>\n",
       "      <td>the</td>\n",
       "      <td>[ the,  a, ,,  \",  and, -, \\n, .,  in,  (]</td>\n",
       "      <td>[0.9005581736564636, 0.8006896376609802, 0.798...</td>\n",
       "      <td>[0.11017955094575882, 0.4186162054538727, 0.93...</td>\n",
       "      <td>[-9.352375984191895, -7.171655178070068, -2.17...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.952061</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-15.662096</td>\n",
       "      <td>-30.157402</td>\n",
       "      <td>-45.819496</td>\n",
       "      <td>Azerb</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9656920433044434, 0.9509235620498657, 0.943...</td>\n",
       "      <td>[0.03807094693183899, 0.09270680695772171, 0.1...</td>\n",
       "      <td>[-13.297449111938477, -14.434858322143555, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.949993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-15.199707</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.974434</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9568272233009338, 0.9426589608192444, 0.936...</td>\n",
       "      <td>[0.038299739360809326, 0.09453219175338745, 0....</td>\n",
       "      <td>[-12.49765396118164, -13.786548614501953, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.941871</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-15.148502</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.923225</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9582076072692871, 0.9447682499885559, 0.937...</td>\n",
       "      <td>[0.03821738809347153, 0.0941627025604248, 0.21...</td>\n",
       "      <td>[-12.498421669006348, -13.787280082702637, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.930066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-15.664072</td>\n",
       "      <td>-23.292881</td>\n",
       "      <td>-38.956955</td>\n",
       "      <td>lehem</td>\n",
       "      <td>[lehem,  Yanuk,  acknow,  Azerb, Buyable,  mis...</td>\n",
       "      <td>[0.9485020637512207, 0.9185827374458313, 0.918...</td>\n",
       "      <td>[0.03955426812171936, 0.10300098359584808, 0.3...</td>\n",
       "      <td>[-12.697884559631348, -13.941573143005371, -16...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.918787</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-15.293366</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.068092</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, lehem, Bu...</td>\n",
       "      <td>[0.9497959017753601, 0.9347649216651917, 0.929...</td>\n",
       "      <td>[0.03854604810476303, 0.09645634889602661, 0.2...</td>\n",
       "      <td>[-12.493782043457031, -13.779804229736328, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.906342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-9.611821</td>\n",
       "      <td>-24.358652</td>\n",
       "      <td>-33.970474</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>[,,  the, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8676905035972595, 0.8666391372680664, 0.799...</td>\n",
       "      <td>[0.10844941437244415, 0.274082750082016, 0.492...</td>\n",
       "      <td>[-10.045668601989746, -10.225157737731934, -9....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.897080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-15.160235</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.934959</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9580494165420532, 0.9447274804115295, 0.936...</td>\n",
       "      <td>[0.03824266046285629, 0.09407132863998413, 0.2...</td>\n",
       "      <td>[-12.498202323913574, -13.787192344665527, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.883083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-15.223869</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.998596</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9520716071128845, 0.9396489262580872, 0.932...</td>\n",
       "      <td>[0.03840366005897522, 0.09522771090269089, 0.2...</td>\n",
       "      <td>[-12.496079444885254, -13.782938957214355, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.868529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-15.159101</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.933826</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9582335948944092, 0.9448711276054382, 0.937...</td>\n",
       "      <td>[0.03825424611568451, 0.09419670701026917, 0.2...</td>\n",
       "      <td>[-12.49836254119873, -13.78726863861084, -15.4...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.810241</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-15.184056</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.958782</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9603358507156372, 0.9447248578071594, 0.939...</td>\n",
       "      <td>[0.03824012354016304, 0.09421104192733765, 0.2...</td>\n",
       "      <td>[-12.498703956604004, -13.788411140441895, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.805457</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-15.140331</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.915054</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9590000510215759, 0.9457425475120544, 0.938...</td>\n",
       "      <td>[0.038216713815927505, 0.09384005516767502, 0....</td>\n",
       "      <td>[-12.498709678649902, -13.788159370422363, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.782019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-14.083304</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-41.858028</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.977673351764679, 0.9704841375350952, 0.9505...</td>\n",
       "      <td>[0.03704534471035004, 0.08393309265375137, 0.2...</td>\n",
       "      <td>[-12.507652282714844, -13.804536819458008, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.779133</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-15.140409</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.915134</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9582928419113159, 0.9454248547554016, 0.937...</td>\n",
       "      <td>[0.038226962089538574, 0.0939604714512825, 0.2...</td>\n",
       "      <td>[-12.498562812805176, -13.78759765625, -15.392...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.772255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-15.129557</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.904282</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.961625874042511, 0.9473548531532288, 0.9406...</td>\n",
       "      <td>[0.038172997534275055, 0.09366320818662643, 0....</td>\n",
       "      <td>[-12.499435424804688, -13.789433479309082, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.764754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-14.972126</td>\n",
       "      <td>-29.508966</td>\n",
       "      <td>-44.481094</td>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9722120761871338, 0.9567747116088867, 0.949...</td>\n",
       "      <td>[0.037902381271123886, 0.09168701618909836, 0....</td>\n",
       "      <td>[-12.842306137084961, -14.023512840270996, -14...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.712047</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-15.225021</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.999744</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.951954185962677, 0.9393918514251709, 0.9321...</td>\n",
       "      <td>[0.03840024024248123, 0.09517060965299606, 0.2...</td>\n",
       "      <td>[-12.49584674835205, -13.782621383666992, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.711850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-15.231623</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.006348</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow,  misunder, iscons,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9625124931335449, 0.9413374662399292, 0.941...</td>\n",
       "      <td>[0.038348373025655746, 0.09499509632587433, 0....</td>\n",
       "      <td>[-12.49891471862793, -13.790125846862793, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.698656</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-12.894300</td>\n",
       "      <td>-26.699200</td>\n",
       "      <td>-39.593498</td>\n",
       "      <td>iscons</td>\n",
       "      <td>[iscons,  acknow, Buyable,  Azerb,  Yanuk, Buy...</td>\n",
       "      <td>[0.9420055150985718, 0.9242556095123291, 0.917...</td>\n",
       "      <td>[0.03882623463869095, 0.09793886542320251, 0.1...</td>\n",
       "      <td>[-11.733702659606934, -13.06739616394043, -14....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.678797</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-15.823362</td>\n",
       "      <td>-23.292881</td>\n",
       "      <td>-39.116241</td>\n",
       "      <td>lehem</td>\n",
       "      <td>[lehem,  showc,  acknow,  misunder, BuyableIns...</td>\n",
       "      <td>[0.9451609253883362, 0.941726565361023, 0.9383...</td>\n",
       "      <td>[0.04000771418213844, 0.10464046895503998, 0.3...</td>\n",
       "      <td>[-12.692943572998047, -13.95312213897705, -16....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.668821</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-15.810199</td>\n",
       "      <td>-23.292881</td>\n",
       "      <td>-39.103081</td>\n",
       "      <td>lehem</td>\n",
       "      <td>[lehem,  Yanuk,  acknow,  showc,  misunder, Bu...</td>\n",
       "      <td>[0.8941332101821899, 0.886613130569458, 0.8652...</td>\n",
       "      <td>[0.040968816727399826, 0.104144386947155, 0.52...</td>\n",
       "      <td>[-12.661088943481445, -13.908308982849121, -16...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.668621</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-15.881790</td>\n",
       "      <td>-22.964725</td>\n",
       "      <td>-38.846516</td>\n",
       "      <td>Acknowledgment</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9578370451927185, 0.9433807134628296, 0.937...</td>\n",
       "      <td>[0.03829643875360489, 0.09444781392812729, 0.2...</td>\n",
       "      <td>[-12.877379417419434, -14.37542724609375, -16....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.662348</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-15.132724</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.907448</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9609753489494324, 0.9467436075210571, 0.939...</td>\n",
       "      <td>[0.03818373382091522, 0.09362656623125076, 0.2...</td>\n",
       "      <td>[-12.499165534973145, -13.789440155029297, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.659443</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-15.169177</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.943901</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9564383029937744, 0.9436343312263489, 0.936...</td>\n",
       "      <td>[0.038276661187410355, 0.09434808790683746, 0....</td>\n",
       "      <td>[-12.497870445251465, -13.78623104095459, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.649270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-15.264116</td>\n",
       "      <td>-22.290806</td>\n",
       "      <td>-37.554924</td>\n",
       "      <td>Acknowledgement</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  indo, BuyableInstor...</td>\n",
       "      <td>[0.9819160103797913, 0.966741681098938, 0.9618...</td>\n",
       "      <td>[0.03763261064887047, 0.0876690223813057, 0.13...</td>\n",
       "      <td>[-12.880313873291016, -14.390179634094238, -16...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.645487</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-15.186453</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.961178</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder, BuyableInstoreAnd...</td>\n",
       "      <td>[0.962743878364563, 0.9502370357513428, 0.9432...</td>\n",
       "      <td>[0.03821570798754692, 0.09497226029634476, 0.2...</td>\n",
       "      <td>[-12.499345779418945, -13.785490989685059, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.643440</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-15.789688</td>\n",
       "      <td>-22.290806</td>\n",
       "      <td>-38.080494</td>\n",
       "      <td>Acknowledgement</td>\n",
       "      <td>[ acknow,  indo, BuyableInstoreAndOnline,  Aze...</td>\n",
       "      <td>[0.9930897355079651, 0.975553572177887, 0.9715...</td>\n",
       "      <td>[0.03736412152647972, 0.09466077387332916, 0.1...</td>\n",
       "      <td>[-12.884515762329102, -14.394420623779297, -16...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.627347</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-15.147956</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.922680</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow,  misunder,  indo, BuyableInstoreAndO...</td>\n",
       "      <td>[0.9745880961418152, 0.9608204960823059, 0.958...</td>\n",
       "      <td>[0.03824687749147415, 0.09801542013883591, 0.2...</td>\n",
       "      <td>[-12.502922058105469, -13.802064895629883, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.583940</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-15.093670</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.868393</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9600481390953064, 0.9472014904022217, 0.938...</td>\n",
       "      <td>[0.03816710412502289, 0.09355087578296661, 0.2...</td>\n",
       "      <td>[-12.49895191192627, -13.788128852844238, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.556673</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-10.180610</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-28.195869</td>\n",
       "      <td>the</td>\n",
       "      <td>[,,  the, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8666245937347412, 0.8614035248756409, 0.798...</td>\n",
       "      <td>[0.10881121456623077, 0.2775222063064575, 0.51...</td>\n",
       "      <td>[-9.654411315917969, -9.676199913024902, -9.14...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.470279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-15.248610</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.023335</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  Azerb,  misunder, BuyableIn...</td>\n",
       "      <td>[0.9517747163772583, 0.9379661679267883, 0.932...</td>\n",
       "      <td>[0.03842366486787796, 0.09558430314064026, 0.2...</td>\n",
       "      <td>[-12.495956420898438, -13.782286643981934, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.447680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-15.427980</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.202705</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, lehem,  indo,  misunder,  showc, Buy...</td>\n",
       "      <td>[0.9401822090148926, 0.930865466594696, 0.9234...</td>\n",
       "      <td>[0.03992108628153801, 0.099322110414505, 0.362...</td>\n",
       "      <td>[-12.472635269165039, -13.771761894226074, -16...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.390438</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-15.257391</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.032116</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9549230933189392, 0.9387116432189941, 0.935...</td>\n",
       "      <td>[0.038449306041002274, 0.09533718973398209, 0....</td>\n",
       "      <td>[-12.495715141296387, -13.78416633605957, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.378467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-15.026055</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.800781</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  indo,  Azerb, Bu...</td>\n",
       "      <td>[0.9757594466209412, 0.9573994874954224, 0.955...</td>\n",
       "      <td>[0.038015078753232956, 0.09202282875776291, 0....</td>\n",
       "      <td>[-12.502655029296875, -13.800891876220703, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.316718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-15.730278</td>\n",
       "      <td>-30.157402</td>\n",
       "      <td>-45.887680</td>\n",
       "      <td>Azerb</td>\n",
       "      <td>[ Azerb,  acknow,  indo, Buyable, iscons,  Yan...</td>\n",
       "      <td>[0.9511327743530273, 0.9451335072517395, 0.939...</td>\n",
       "      <td>[0.03815960884094238, 0.09626717865467072, 0.3...</td>\n",
       "      <td>[-13.298213958740234, -14.427967071533203, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.272719</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-15.229589</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.004314</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder, BuyableInstoreAnd...</td>\n",
       "      <td>[0.9623012542724609, 0.9460921883583069, 0.944...</td>\n",
       "      <td>[0.03853874281048775, 0.09384673088788986, 0.2...</td>\n",
       "      <td>[-12.495795249938965, -13.78832721710205, -15....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.228020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-14.971882</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-42.746605</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder,  Azerb, BuyableIn...</td>\n",
       "      <td>[0.9702160954475403, 0.9534894227981567, 0.946...</td>\n",
       "      <td>[0.03793982416391373, 0.09217415750026703, 0.1...</td>\n",
       "      <td>[-12.501554489135742, -13.795710563659668, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.167355</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-8.720352</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-26.735611</td>\n",
       "      <td>the</td>\n",
       "      <td>[ the, ,, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8690021634101868, 0.8683966994285583, 0.800...</td>\n",
       "      <td>[0.10831275582313538, 0.27274632453918457, 0.4...</td>\n",
       "      <td>[-9.639199256896973, -9.552501678466797, -8.57...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-15.417343</td>\n",
       "      <td>-27.774725</td>\n",
       "      <td>-43.192070</td>\n",
       "      <td>acknow</td>\n",
       "      <td>[ acknow, iscons,  misunder, lehem,  Azerb, Bu...</td>\n",
       "      <td>[0.9425803422927856, 0.9344441294670105, 0.929...</td>\n",
       "      <td>[0.03921667858958244, 0.09728263318538666, 0.3...</td>\n",
       "      <td>[-12.482281684875488, -13.762645721435547, -15...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-14.838971</td>\n",
       "      <td>-23.292881</td>\n",
       "      <td>-38.131851</td>\n",
       "      <td>lehem</td>\n",
       "      <td>[lehem, iscons,  acknow,  Yanuk,  Azerb, Buyab...</td>\n",
       "      <td>[0.9255759716033936, 0.9220585227012634, 0.910...</td>\n",
       "      <td>[0.039168890565633774, 0.10017517954111099, 0....</td>\n",
       "      <td>[-12.696868896484375, -13.9252347946167, -15.7...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>-0.060541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predictiveness_score  simplicity_score      score      explanation  \\\n",
       "0             -15.302822        -27.774725 -43.077545           acknow   \n",
       "1             -15.254693        -27.774725 -43.029419           acknow   \n",
       "2              -8.606151        -18.015259 -26.621410              the   \n",
       "3              -4.347507        -18.015259 -22.362766              the   \n",
       "4             -15.662096        -30.157402 -45.819496            Azerb   \n",
       "5             -15.199707        -27.774725 -42.974434           acknow   \n",
       "6             -15.148502        -27.774725 -42.923225           acknow   \n",
       "7             -15.664072        -23.292881 -38.956955            lehem   \n",
       "8             -15.293366        -27.774725 -43.068092           acknow   \n",
       "9              -9.611821        -24.358652 -33.970474      punctuation   \n",
       "10            -15.160235        -27.774725 -42.934959           acknow   \n",
       "11            -15.223869        -27.774725 -42.998596           acknow   \n",
       "12            -15.159101        -27.774725 -42.933826           acknow   \n",
       "13            -15.184056        -27.774725 -42.958782           acknow   \n",
       "14            -15.140331        -27.774725 -42.915054           acknow   \n",
       "15            -14.083304        -27.774725 -41.858028           acknow   \n",
       "16            -15.140409        -27.774725 -42.915134           acknow   \n",
       "17            -15.129557        -27.774725 -42.904282           acknow   \n",
       "18            -14.972126        -29.508966 -44.481094       azerbaijan   \n",
       "19            -15.225021        -27.774725 -42.999744           acknow   \n",
       "20            -15.231623        -27.774725 -43.006348           acknow   \n",
       "21            -12.894300        -26.699200 -39.593498           iscons   \n",
       "22            -15.823362        -23.292881 -39.116241            lehem   \n",
       "23            -15.810199        -23.292881 -39.103081            lehem   \n",
       "24            -15.881790        -22.964725 -38.846516   Acknowledgment   \n",
       "25            -15.132724        -27.774725 -42.907448           acknow   \n",
       "26            -15.169177        -27.774725 -42.943901           acknow   \n",
       "27            -15.264116        -22.290806 -37.554924  Acknowledgement   \n",
       "28            -15.186453        -27.774725 -42.961178           acknow   \n",
       "29            -15.789688        -22.290806 -38.080494  Acknowledgement   \n",
       "30            -15.147956        -27.774725 -42.922680           acknow   \n",
       "31            -15.093670        -27.774725 -42.868393           acknow   \n",
       "32            -10.180610        -18.015259 -28.195869              the   \n",
       "33            -15.248610        -27.774725 -43.023335           acknow   \n",
       "34            -15.427980        -27.774725 -43.202705           acknow   \n",
       "35            -15.257391        -27.774725 -43.032116           acknow   \n",
       "36            -15.026055        -27.774725 -42.800781           acknow   \n",
       "37            -15.730278        -30.157402 -45.887680            Azerb   \n",
       "38            -15.229589        -27.774725 -43.004314           acknow   \n",
       "39            -14.971882        -27.774725 -42.746605           acknow   \n",
       "40             -8.720352        -18.015259 -26.735611              the   \n",
       "41            -15.417343        -27.774725 -43.192070           acknow   \n",
       "42            -14.838971        -23.292881 -38.131851            lehem   \n",
       "\n",
       "                                           top_tokens  \\\n",
       "0   [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "1   [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "2          [ the, ,, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "3          [ the,  a, ,,  \",  and, -, \\n, .,  in,  (]   \n",
       "4   [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "5   [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "6   [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "7   [lehem,  Yanuk,  acknow,  Azerb, Buyable,  mis...   \n",
       "8   [ acknow, iscons,  misunder,  Azerb, lehem, Bu...   \n",
       "9          [,,  the, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "10  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "11  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "12  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "13  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "14  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "15  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "16  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "17  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "18  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "19  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "20  [ acknow,  misunder, iscons,  Azerb, BuyableIn...   \n",
       "21  [iscons,  acknow, Buyable,  Azerb,  Yanuk, Buy...   \n",
       "22  [lehem,  showc,  acknow,  misunder, BuyableIns...   \n",
       "23  [lehem,  Yanuk,  acknow,  showc,  misunder, Bu...   \n",
       "24  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "25  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "26  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "27  [ acknow, iscons,  Azerb,  indo, BuyableInstor...   \n",
       "28  [ acknow, iscons,  misunder, BuyableInstoreAnd...   \n",
       "29  [ acknow,  indo, BuyableInstoreAndOnline,  Aze...   \n",
       "30  [ acknow,  misunder,  indo, BuyableInstoreAndO...   \n",
       "31  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "32         [,,  the, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "33  [ acknow, iscons,  Azerb,  misunder, BuyableIn...   \n",
       "34  [ acknow, lehem,  indo,  misunder,  showc, Buy...   \n",
       "35  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "36  [ acknow, iscons,  misunder,  indo,  Azerb, Bu...   \n",
       "37  [ Azerb,  acknow,  indo, Buyable, iscons,  Yan...   \n",
       "38  [ acknow, iscons,  misunder, BuyableInstoreAnd...   \n",
       "39  [ acknow, iscons,  misunder,  Azerb, BuyableIn...   \n",
       "40         [ the, ,, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "41  [ acknow, iscons,  misunder, lehem,  Azerb, Bu...   \n",
       "42  [lehem, iscons,  acknow,  Yanuk,  Azerb, Buyab...   \n",
       "\n",
       "                                           top_logits  \\\n",
       "0   [0.9483156800270081, 0.9328060746192932, 0.927...   \n",
       "1   [0.9549833536148071, 0.9397922158241272, 0.934...   \n",
       "2   [0.8696240782737732, 0.8686282634735107, 0.800...   \n",
       "3   [0.9005581736564636, 0.8006896376609802, 0.798...   \n",
       "4   [0.9656920433044434, 0.9509235620498657, 0.943...   \n",
       "5   [0.9568272233009338, 0.9426589608192444, 0.936...   \n",
       "6   [0.9582076072692871, 0.9447682499885559, 0.937...   \n",
       "7   [0.9485020637512207, 0.9185827374458313, 0.918...   \n",
       "8   [0.9497959017753601, 0.9347649216651917, 0.929...   \n",
       "9   [0.8676905035972595, 0.8666391372680664, 0.799...   \n",
       "10  [0.9580494165420532, 0.9447274804115295, 0.936...   \n",
       "11  [0.9520716071128845, 0.9396489262580872, 0.932...   \n",
       "12  [0.9582335948944092, 0.9448711276054382, 0.937...   \n",
       "13  [0.9603358507156372, 0.9447248578071594, 0.939...   \n",
       "14  [0.9590000510215759, 0.9457425475120544, 0.938...   \n",
       "15  [0.977673351764679, 0.9704841375350952, 0.9505...   \n",
       "16  [0.9582928419113159, 0.9454248547554016, 0.937...   \n",
       "17  [0.961625874042511, 0.9473548531532288, 0.9406...   \n",
       "18  [0.9722120761871338, 0.9567747116088867, 0.949...   \n",
       "19  [0.951954185962677, 0.9393918514251709, 0.9321...   \n",
       "20  [0.9625124931335449, 0.9413374662399292, 0.941...   \n",
       "21  [0.9420055150985718, 0.9242556095123291, 0.917...   \n",
       "22  [0.9451609253883362, 0.941726565361023, 0.9383...   \n",
       "23  [0.8941332101821899, 0.886613130569458, 0.8652...   \n",
       "24  [0.9578370451927185, 0.9433807134628296, 0.937...   \n",
       "25  [0.9609753489494324, 0.9467436075210571, 0.939...   \n",
       "26  [0.9564383029937744, 0.9436343312263489, 0.936...   \n",
       "27  [0.9819160103797913, 0.966741681098938, 0.9618...   \n",
       "28  [0.962743878364563, 0.9502370357513428, 0.9432...   \n",
       "29  [0.9930897355079651, 0.975553572177887, 0.9715...   \n",
       "30  [0.9745880961418152, 0.9608204960823059, 0.958...   \n",
       "31  [0.9600481390953064, 0.9472014904022217, 0.938...   \n",
       "32  [0.8666245937347412, 0.8614035248756409, 0.798...   \n",
       "33  [0.9517747163772583, 0.9379661679267883, 0.932...   \n",
       "34  [0.9401822090148926, 0.930865466594696, 0.9234...   \n",
       "35  [0.9549230933189392, 0.9387116432189941, 0.935...   \n",
       "36  [0.9757594466209412, 0.9573994874954224, 0.955...   \n",
       "37  [0.9511327743530273, 0.9451335072517395, 0.939...   \n",
       "38  [0.9623012542724609, 0.9460921883583069, 0.944...   \n",
       "39  [0.9702160954475403, 0.9534894227981567, 0.946...   \n",
       "40  [0.8690021634101868, 0.8683966994285583, 0.800...   \n",
       "41  [0.9425803422927856, 0.9344441294670105, 0.929...   \n",
       "42  [0.9255759716033936, 0.9220585227012634, 0.910...   \n",
       "\n",
       "                                 max_intervened_probs  \\\n",
       "0   [0.03869502246379852, 0.09587770700454712, 0.2...   \n",
       "1   [0.03839239105582237, 0.09543728828430176, 0.2...   \n",
       "2   [0.10830873996019363, 0.2725183069705963, 0.47...   \n",
       "3   [0.11017955094575882, 0.4186162054538727, 0.93...   \n",
       "4   [0.03807094693183899, 0.09270680695772171, 0.1...   \n",
       "5   [0.038299739360809326, 0.09453219175338745, 0....   \n",
       "6   [0.03821738809347153, 0.0941627025604248, 0.21...   \n",
       "7   [0.03955426812171936, 0.10300098359584808, 0.3...   \n",
       "8   [0.03854604810476303, 0.09645634889602661, 0.2...   \n",
       "9   [0.10844941437244415, 0.274082750082016, 0.492...   \n",
       "10  [0.03824266046285629, 0.09407132863998413, 0.2...   \n",
       "11  [0.03840366005897522, 0.09522771090269089, 0.2...   \n",
       "12  [0.03825424611568451, 0.09419670701026917, 0.2...   \n",
       "13  [0.03824012354016304, 0.09421104192733765, 0.2...   \n",
       "14  [0.038216713815927505, 0.09384005516767502, 0....   \n",
       "15  [0.03704534471035004, 0.08393309265375137, 0.2...   \n",
       "16  [0.038226962089538574, 0.0939604714512825, 0.2...   \n",
       "17  [0.038172997534275055, 0.09366320818662643, 0....   \n",
       "18  [0.037902381271123886, 0.09168701618909836, 0....   \n",
       "19  [0.03840024024248123, 0.09517060965299606, 0.2...   \n",
       "20  [0.038348373025655746, 0.09499509632587433, 0....   \n",
       "21  [0.03882623463869095, 0.09793886542320251, 0.1...   \n",
       "22  [0.04000771418213844, 0.10464046895503998, 0.3...   \n",
       "23  [0.040968816727399826, 0.104144386947155, 0.52...   \n",
       "24  [0.03829643875360489, 0.09444781392812729, 0.2...   \n",
       "25  [0.03818373382091522, 0.09362656623125076, 0.2...   \n",
       "26  [0.038276661187410355, 0.09434808790683746, 0....   \n",
       "27  [0.03763261064887047, 0.0876690223813057, 0.13...   \n",
       "28  [0.03821570798754692, 0.09497226029634476, 0.2...   \n",
       "29  [0.03736412152647972, 0.09466077387332916, 0.1...   \n",
       "30  [0.03824687749147415, 0.09801542013883591, 0.2...   \n",
       "31  [0.03816710412502289, 0.09355087578296661, 0.2...   \n",
       "32  [0.10881121456623077, 0.2775222063064575, 0.51...   \n",
       "33  [0.03842366486787796, 0.09558430314064026, 0.2...   \n",
       "34  [0.03992108628153801, 0.099322110414505, 0.362...   \n",
       "35  [0.038449306041002274, 0.09533718973398209, 0....   \n",
       "36  [0.038015078753232956, 0.09202282875776291, 0....   \n",
       "37  [0.03815960884094238, 0.09626717865467072, 0.3...   \n",
       "38  [0.03853874281048775, 0.09384673088788986, 0.2...   \n",
       "39  [0.03793982416391373, 0.09217415750026703, 0.1...   \n",
       "40  [0.10831275582313538, 0.27274632453918457, 0.4...   \n",
       "41  [0.03921667858958244, 0.09728263318538666, 0.3...   \n",
       "42  [0.039168890565633774, 0.10017517954111099, 0....   \n",
       "\n",
       "                                predictiveness_scores intervention_strengths  \\\n",
       "0   [-12.488578796386719, -13.774748802185059, -15...  [3, 10, 32, 100, 320]   \n",
       "1   [-12.4965181350708, -13.78377628326416, -15.51...  [3, 10, 32, 100, 320]   \n",
       "2   [-9.63817024230957, -9.544039726257324, -8.534...  [3, 10, 32, 100, 320]   \n",
       "3   [-9.352375984191895, -7.171655178070068, -2.17...  [3, 10, 32, 100, 320]   \n",
       "4   [-13.297449111938477, -14.434858322143555, -15...  [3, 10, 32, 100, 320]   \n",
       "5   [-12.49765396118164, -13.786548614501953, -15....  [3, 10, 32, 100, 320]   \n",
       "6   [-12.498421669006348, -13.787280082702637, -15...  [3, 10, 32, 100, 320]   \n",
       "7   [-12.697884559631348, -13.941573143005371, -16...  [3, 10, 32, 100, 320]   \n",
       "8   [-12.493782043457031, -13.779804229736328, -15...  [3, 10, 32, 100, 320]   \n",
       "9   [-10.045668601989746, -10.225157737731934, -9....  [3, 10, 32, 100, 320]   \n",
       "10  [-12.498202323913574, -13.787192344665527, -15...  [3, 10, 32, 100, 320]   \n",
       "11  [-12.496079444885254, -13.782938957214355, -15...  [3, 10, 32, 100, 320]   \n",
       "12  [-12.49836254119873, -13.78726863861084, -15.4...  [3, 10, 32, 100, 320]   \n",
       "13  [-12.498703956604004, -13.788411140441895, -15...  [3, 10, 32, 100, 320]   \n",
       "14  [-12.498709678649902, -13.788159370422363, -15...  [3, 10, 32, 100, 320]   \n",
       "15  [-12.507652282714844, -13.804536819458008, -15...  [3, 10, 32, 100, 320]   \n",
       "16  [-12.498562812805176, -13.78759765625, -15.392...  [3, 10, 32, 100, 320]   \n",
       "17  [-12.499435424804688, -13.789433479309082, -15...  [3, 10, 32, 100, 320]   \n",
       "18  [-12.842306137084961, -14.023512840270996, -14...  [3, 10, 32, 100, 320]   \n",
       "19  [-12.49584674835205, -13.782621383666992, -15....  [3, 10, 32, 100, 320]   \n",
       "20  [-12.49891471862793, -13.790125846862793, -15....  [3, 10, 32, 100, 320]   \n",
       "21  [-11.733702659606934, -13.06739616394043, -14....  [3, 10, 32, 100, 320]   \n",
       "22  [-12.692943572998047, -13.95312213897705, -16....  [3, 10, 32, 100, 320]   \n",
       "23  [-12.661088943481445, -13.908308982849121, -16...  [3, 10, 32, 100, 320]   \n",
       "24  [-12.877379417419434, -14.37542724609375, -16....  [3, 10, 32, 100, 320]   \n",
       "25  [-12.499165534973145, -13.789440155029297, -15...  [3, 10, 32, 100, 320]   \n",
       "26  [-12.497870445251465, -13.78623104095459, -15....  [3, 10, 32, 100, 320]   \n",
       "27  [-12.880313873291016, -14.390179634094238, -16...  [3, 10, 32, 100, 320]   \n",
       "28  [-12.499345779418945, -13.785490989685059, -15...  [3, 10, 32, 100, 320]   \n",
       "29  [-12.884515762329102, -14.394420623779297, -16...  [3, 10, 32, 100, 320]   \n",
       "30  [-12.502922058105469, -13.802064895629883, -15...  [3, 10, 32, 100, 320]   \n",
       "31  [-12.49895191192627, -13.788128852844238, -15....  [3, 10, 32, 100, 320]   \n",
       "32  [-9.654411315917969, -9.676199913024902, -9.14...  [3, 10, 32, 100, 320]   \n",
       "33  [-12.495956420898438, -13.782286643981934, -15...  [3, 10, 32, 100, 320]   \n",
       "34  [-12.472635269165039, -13.771761894226074, -16...  [3, 10, 32, 100, 320]   \n",
       "35  [-12.495715141296387, -13.78416633605957, -15....  [3, 10, 32, 100, 320]   \n",
       "36  [-12.502655029296875, -13.800891876220703, -15...  [3, 10, 32, 100, 320]   \n",
       "37  [-13.298213958740234, -14.427967071533203, -15...  [3, 10, 32, 100, 320]   \n",
       "38  [-12.495795249938965, -13.78832721710205, -15....  [3, 10, 32, 100, 320]   \n",
       "39  [-12.501554489135742, -13.795710563659668, -15...  [3, 10, 32, 100, 320]   \n",
       "40  [-9.639199256896973, -9.552501678466797, -8.57...  [3, 10, 32, 100, 320]   \n",
       "41  [-12.482281684875488, -13.762645721435547, -15...  [3, 10, 32, 100, 320]   \n",
       "42  [-12.696868896484375, -13.9252347946167, -15.7...  [3, 10, 32, 100, 320]   \n",
       "\n",
       "    ev_correlation_score  layer  feature  \n",
       "0               0.970093    2.0      0.0  \n",
       "1               0.966378    2.0     19.0  \n",
       "2               0.952401    0.0      0.0  \n",
       "3               0.952061    4.0      4.0  \n",
       "4               0.949993    0.0      5.0  \n",
       "5               0.941871    2.0      4.0  \n",
       "6               0.930066    2.0     11.0  \n",
       "7               0.918787    4.0     19.0  \n",
       "8               0.906342    0.0     14.0  \n",
       "9               0.897080    0.0      3.0  \n",
       "10              0.883083    0.0     11.0  \n",
       "11              0.868529    0.0      9.0  \n",
       "12              0.810241    2.0      3.0  \n",
       "13              0.805457    2.0     18.0  \n",
       "14              0.782019    0.0     16.0  \n",
       "15              0.779133    4.0     11.0  \n",
       "16              0.772255    0.0     10.0  \n",
       "17              0.764754    0.0      7.0  \n",
       "18              0.712047    2.0      7.0  \n",
       "19              0.711850    0.0     12.0  \n",
       "20              0.698656    4.0      0.0  \n",
       "21              0.678797    4.0     12.0  \n",
       "22              0.668821    4.0     15.0  \n",
       "23              0.668621    4.0     13.0  \n",
       "24              0.662348    2.0      1.0  \n",
       "25              0.659443    2.0      2.0  \n",
       "26              0.649270    0.0     13.0  \n",
       "27              0.645487    4.0      3.0  \n",
       "28              0.643440    2.0      6.0  \n",
       "29              0.627347    4.0     17.0  \n",
       "30              0.583940    4.0      8.0  \n",
       "31              0.556673    2.0     15.0  \n",
       "32              0.470279    0.0      4.0  \n",
       "33              0.447680    0.0      6.0  \n",
       "34              0.390438    4.0      7.0  \n",
       "35              0.378467    0.0     18.0  \n",
       "36              0.316718    4.0     14.0  \n",
       "37              0.272719    4.0      9.0  \n",
       "38              0.228020    4.0     10.0  \n",
       "39              0.167355    2.0     13.0  \n",
       "40              0.095751    2.0      5.0  \n",
       "41              0.023048    4.0      2.0  \n",
       "42             -0.060541    0.0     15.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(results)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ' Bed' (+5), 'VAL' (+4), ' Portable' (+4), '`.' (+4), ' Tablet' (+4), 'NPR' (+4), ' OWN' (+4), 'mod' (+4), ' Ban' (+4), 'BY' (+4)\n",
      "Explanation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' showc' (+10), ' inval' (+10), ' privile' (+10), 'SPONSORED' (+9), ' contrace' (+9), ' mathemat' (+9), ' misunder' (+9), 'paralle' (+9), 'accompan' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'govtrack' (+5), ' epit' (+5), ' ASA' (+5), 'gart' (+5), 'ilo' (+5), 'fighter' (+4), 'assetsadobe' (+4), 'rodu' (+4), 'bard' (+4), 'shi' (+4)\n",
      "Explanation:\n",
      "Tokens: 'ulla' (+5), 'eland' (+4), 'OTH' (+4), 'lier' (+4), 'lled' (+4), 'erk' (+4), ' omit' (+4), 'ERS' (+4), 'arth' (+4), 'ESCO' (+4)\n",
      "Explanation:\n",
      "Tokens: ' the' (+8), ' a' (+7), ',' (+7), 'The' (+7), '\n",
      "' (+6), ' and' (+6), '.' (+6), '-' (+6), ' The' (+6), ' I' (+6)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), ' indo' (+10), 'iscons' (+10), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' Azerb' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' showc' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'algia' (+6), ' Glac' (+6), '覚醒' (+6), ' lockout' (+5), 'alg' (+5), 'intend' (+5), ' Notting' (+5), 'ockey' (+5), 'fw' (+5), ' ecology' (+5)\n",
      "Explanation:\n",
      "Tokens: 'dx' (+4), 'MR' (+4), 'Else' (+4), 'GM' (+4), 'WD' (+3), ' Taste' (+3), 'xx' (+3), 'NR' (+3), 'gd' (+3), '̶' (+3)\n",
      "Explanation:\n",
      "Tokens: ' the' (+8), ',' (+8), '-' (+8), ' a' (+8), '.' (+8), ' in' (+7), '\n",
      "' (+7), ' \"' (+7), ' and' (+7), ' to' (+7)\n",
      "Explanation:\n",
      "Tokens: 'gow' (+5), 'lf' (+5), 'vo' (+5), 'bin' (+5), 'orld' (+5), 'biz' (+4), ' Department' (+4), 'best' (+4), 'res' (+4), 'ch' (+4)\n",
      "Explanation:\n",
      "Tokens: 'lehem' (+9), ' acknow' (+9), 'iscons' (+9), 'BuyableInstoreAndOnline' (+9), ' misunder' (+9), 'Buyable' (+9), ' Azerb' (+9), ' Yanuk' (+9), ' indo' (+9), 'FontSize' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: 'lehem' (+9), ' showc' (+8), 'FontSize' (+8), ' ado' (+8), 'cised' (+8), 'ashtra' (+8), ' misunder' (+8), ' Yanuk' (+8), ' guiActiveUn' (+8), ' Rohing' (+7)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), ' indo' (+10), 'iscons' (+10), 'BuyableInstoreAndOnline' (+10), ' misunder' (+10), ' Azerb' (+10), 'ngth' (+9), 'Buyable' (+9), ' Vaugh' (+9), ' condem' (+9)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), ' a' (+8), '.' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' a' (+8), ' and' (+8), '.' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' to' (+7)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'DOS' (+7), 'bian' (+6), ' pid' (+6), 'aceae' (+6), 'sonian' (+6), 'falls' (+6), 'runs' (+6), ' sack' (+6), ' laun' (+6), 'abase' (+6)\n",
      "Explanation:\n",
      "Tokens: 'Acknowled' (+4), 'enne' (+4), 'imon' (+4), ' Flavoring' (+4), 'IDENT' (+4), ' Burton' (+4), ' Berman' (+4), 'erry' (+4), 'cil' (+4), 'avering' (+4)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'selves' (+5), 'ayne' (+5), 'FontSize' (+5), 'ility' (+5), 'apult' (+4), '◼' (+4), 'itled' (+4), 'yright' (+4), 'paralle' (+4), 'lehem' (+4)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' misunder' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+8), '-' (+8), ' and' (+8), '.' (+8), ' in' (+7), '\n",
      "' (+7), ' a' (+7), ' \"' (+7), ' to' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'entimes' (+7), 'imaru' (+6), 'anooga' (+6), 'lie' (+6), 'itaire' (+6), ' subsequ' (+6), 'etheless' (+5), ' Swordsman' (+5), ' PLA' (+5), 'orem' (+5)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'oons' (+6), '��極' (+6), ' agre' (+6), ' describ' (+6), ''t' (+6), ' concess' (+6), ' unanim' (+6), ' fortun' (+6), ' pse' (+6), ' burns' (+6)\n",
      "Explanation:\n",
      "Tokens: ' Salvation' (+4), ' Kali' (+4), 'Resources' (+4), 'anism' (+4), 'gp' (+4), 'Supported' (+4), 'aments' (+4), 'rimination' (+4), 'oxide' (+4), 'anyl' (+4)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ',' (+8), ' and' (+8), '-' (+8), ' the' (+8), '.' (+7), '\n",
      "' (+7), ' in' (+7), ' to' (+7), ' a' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' ré' (+5), ' Editors' (+5), ' pes' (+5), ' Dos' (+5), 'Pg' (+5), ' ply' (+5), ' draws' (+5), ' draw' (+5), ' Cad' (+5), ' drew' (+5)\n",
      "Explanation:\n",
      "Tokens: 'rongh' (+8), 'Buyable' (+8), 'aeper' (+8), 'mbuds' (+8), 'iscons' (+8), 'morrow' (+8), 'odder' (+8), 'cffff' (+8), 'CRIP' (+8), ' acknow' (+8)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: 'sic' (+4), 'intend' (+3), 'lc' (+3), 'portation' (+3), 'ICLE' (+3), 'quart' (+3), '�' (+3), 'leg' (+3), 'fact' (+3), 'hern' (+3)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'terday' (+6), ' adolesc' (+6), ' taxp' (+6), 'cker' (+6), ' Rica' (+6), 'ymes' (+5), 'vous' (+5), ' independ' (+5), 'velt' (+5), 'enson' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+10), ' indo' (+10), 'BuyableInstoreAndOnline' (+10), ' Azerb' (+10), 'Buyable' (+9), 'ngth' (+9), 'MpServer' (+9), ' Yanuk' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: '*/(' (+7), 'aird' (+7), 'estern' (+6), ' scrut' (+6), 'lopp' (+6), 'ippi' (+6), 'itars' (+6), 'olitan' (+6), 'aston' (+6), 'edia' (+6)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+8), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' to' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+7), ' a' (+6), ' \"' (+6), ' more' (+6), ' other' (+5), ',' (+5), ' new' (+5), ' I' (+5), ' an' (+5), ' in' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: 'igi' (+5), 'soever' (+5), 'Parent' (+5), '��' (+5), '\"]=>' (+5), 'Topics' (+5), '*/(' (+5), 'Published' (+4), ' Speedway' (+4), ' POLITICO' (+4)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), ' indo' (+10), ' misunder' (+10), 'BuyableInstoreAndOnline' (+10), ' Azerb' (+10), 'iscons' (+9), 'ngth' (+9), ' showc' (+9), 'Buyable' (+9), 'lehem' (+9)\n",
      "Explanation:\n",
      "Tokens: 'PO' (+5), 'ESS' (+4), 'TABLE' (+4), 'MIC' (+4), 'ONY' (+4), 'dx' (+4), 'odder' (+4), 'aving' (+4), 'pse' (+4), 'sem' (+4)\n",
      "Explanation:\n",
      "Tokens: ',' (+8), '.' (+7), ' and' (+7), '-' (+7), ' the' (+7), ' in' (+6), '\n",
      "' (+6), ' to' (+6), ' (' (+6), ' \"' (+6)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: 'cause' (+5), 'iatus' (+5), 'ENTS' (+5), 'endi' (+5), 'ema' (+5), 'iband' (+5), ' signature' (+4), 'ribution' (+4), 'llan' (+4), 'cit' (+4)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'ngth' (+9), ' Azerb' (+9), ' showc' (+9), ' misunder' (+9), ' Vaugh' (+9), ' Yanuk' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' Reloaded' (+5), 'rea' (+5), 'vation' (+5), 'arching' (+5), 'amaru' (+5), 'ript' (+5), 'uden' (+5), 'ITED' (+5), 'Adapt' (+5), 'drive' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: 'interstitial' (+7), 'rio' (+7), 'enta' (+6), 'ranged' (+6), ' targ' (+6), 'planes' (+6), 'drops' (+6), 'rab' (+6), 'eton' (+6), 'resh' (+6)\n",
      "Explanation:\n",
      "Tokens: 'vity' (+5), 'soDeliveryDate' (+4), 'ILE' (+4), 'ENA' (+4), 'itle' (+4), 'vation' (+4), 'oma' (+4), 'lings' (+4), 'inks' (+4), 'ica' (+4)\n",
      "Explanation:\n",
      "Tokens: 'lehem' (+4), 'Versions' (+4), 'aroo' (+4), 'folios' (+4), ' guiActiveUn' (+4), 'lore' (+3), 'eers' (+3), 'ashtra' (+3), 'Upload' (+3), 'fuck' (+3)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '.' (+8), '-' (+8), ' and' (+8), ' a' (+8), ' in' (+8), ' \"' (+8), '\n",
      "' (+8), ' to' (+7)\n",
      "Explanation:\n",
      "Tokens: ' the' (+6), '-' (+5), '\n",
      "' (+5), ',' (+5), ' \"' (+5), ' a' (+5), '.' (+5), ' and' (+5), ' The' (+5), ' C' (+4)\n",
      "Explanation:\n",
      "Tokens: 'enes' (+5), 'usercontent' (+5), 'ene' (+5), 'ape' (+4), 'án' (+4), 'eon' (+4), 'auc' (+4), 'ivan' (+4), 'ENE' (+4), 'oho' (+4)\n",
      "Explanation:\n",
      "Tokens: 'llah' (+6), ' recognised' (+5), 'League' (+5), ' gall' (+5), 'lde' (+5), 'actionDate' (+5), 'jong' (+5), 'Chief' (+5), ' lapse' (+5), 'ourn' (+5)\n",
      "Explanation:\n",
      "Tokens: ' Narc' (+4), 'alist' (+4), 'iferation' (+4), 'elist' (+4), ' Immortal' (+4), ' Rite' (+4), 'chell' (+4), 'SPONSORED' (+4), 'gie' (+3), ' Paraly' (+3)\n",
      "Explanation:\n",
      "Tokens: 'soDeliveryDate' (+7), ' Chero' (+6), 'NJ' (+6), 'ires' (+5), 'soon' (+5), 'jas' (+5), 'nin' (+5), 'iciary' (+5), 'ayson' (+5), 'zon' (+5)\n",
      "Explanation:\n",
      "Tokens: ' altru' (+7), ' Rober' (+6), 'ersed' (+6), '�' (+6), ' tsun' (+6), ' Venezuel' (+5), ' Kimmel' (+5), ' Syri' (+5), ' nationalism' (+5), ' populist' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+10), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+7), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' horizon' (+6), 'OLOGY' (+6), ' deck' (+5), 'ouse' (+5), ' altar' (+5), ' crypto' (+5), 'enture' (+5), ' omega' (+5), ' docking' (+5), ' abl' (+5)\n",
      "Explanation:\n",
      "Tokens: ' confir' (+11), ' destro' (+11), ' answ' (+11), ' horizont' (+10), ' ingred' (+10), ' resil' (+10), ' embr' (+10), ' proble' (+10), ' seiz' (+10), ' arrang' (+10)\n",
      "Explanation:\n",
      "Tokens: 'deen' (+8), 'imposed' (+7), ' submar' (+7), ' behavi' (+7), 'heit' (+6), ' timet' (+6), 'arantine' (+6), 'paying' (+6), ' Rowe' (+6), ' punishable' (+6)\n",
      "Explanation:\n",
      "Tokens: 'roc' (+6), 'atel' (+5), 'ounters' (+5), 'ombat' (+5), 'ross' (+5), 'icz' (+5), 'terson' (+5), ' occasion' (+5), ' attrition' (+5), 'ADA' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' Azerb' (+9), ' indo' (+9), 'Buyable' (+9), ' Yanuk' (+9), 'lehem' (+9), 'MpServer' (+9)\n",
      "Explanation:\n",
      "Tokens: '/-' (+3), 'points' (+3), 'se' (+3), 'The' (+3), '\n",
      "' (+3), 'A' (+3), ' a' (+2), 'new' (+2), 'ohydrate' (+2), 'point' (+2)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+9), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), 'Buyable' (+9), 'lehem' (+9), ' indo' (+9), ' Yanuk' (+9), 'MpServer' (+9)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' misunder' (+9), ' Azerb' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' defic' (+13), ' Canaver' (+12), 'BuyableInstoreAndOnline' (+12), ' Vaugh' (+12), ' surpr' (+12), ' advoc' (+11), 'hovah' (+11), ' blat' (+11), ' answ' (+11), ' confir' (+11)\n",
      "Explanation:\n",
      "Tokens: 'ously' (+5), 'lessly' (+4), 'wise' (+3), 'ibly' (+3), 'lis' (+3), 'oyer' (+3), 'lu' (+3), 'achine' (+3), 'uses' (+3), ':' (+3)\n",
      "Explanation:\n",
      "Tokens: ',' (+8), ' the' (+7), '-' (+7), '.' (+6), ' a' (+6), '\n",
      "' (+6), ' in' (+6), ' (' (+6), ' \"' (+5), ':' (+5)\n",
      "Explanation:\n",
      "Tokens: 'ushi' (+4), 'leep' (+3), ' gladly' (+3), 'scill' (+3), 'eca' (+3), 'seys' (+3), 'aurus' (+3), 'chers' (+3), 'RON' (+3), ' retaliate' (+3)\n",
      "Explanation:\n",
      "Tokens: 'iscons' (+10), ' acknow' (+10), ' Azerb' (+9), ' indo' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'MpServer' (+9)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), ' a' (+8), '.' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ',' (+9), ' the' (+9), '-' (+8), ' and' (+8), '.' (+8), ' a' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' Azerb' (+9), ' misunder' (+9), 'BuyableInstoreAndOnline' (+9), ' indo' (+9), 'Buyable' (+9), 'lehem' (+9), ' Yanuk' (+9), 'ngth' (+9)\n",
      "Explanation:\n",
      "Tokens: ' the' (+7), ',' (+7), '-' (+7), ' and' (+7), '\n",
      "' (+7), ' a' (+6), '.' (+6), ' in' (+6), ' \"' (+6), ' (' (+6)\n",
      "Explanation:\n",
      "Tokens: ' Exodus' (+6), 'ulo' (+6), ' multiplication' (+6), 'trump' (+5), ' Decay' (+5), ' Slide' (+5), 'ド' (+5), 'oqu' (+5), 'bows' (+5), ' fluoride' (+5)\n",
      "Explanation:\n",
      "Tokens: ' acknow' (+10), 'iscons' (+9), ' indo' (+9), 'BuyableInstoreAndOnline' (+9), ' misunder' (+9), ' Azerb' (+9), 'ngth' (+9), 'Buyable' (+9), ' showc' (+9), ' Vaugh' (+9)\n",
      "Explanation:\n",
      "Tokens: 'iscons' (+9), 'lehem' (+9), ' Azerb' (+9), 'Buyable' (+9), ' Yanuk' (+9), ' acknow' (+9), 'BuyableInstoreAndOnline' (+9), ' misunder' (+9), 'rongh' (+9), 'MpServer' (+9)\n",
      "Explanation:\n",
      "Tokens: 'inventoryQuantity' (+6), 'LA' (+5), 'Pacific' (+5), 'aring' (+5), 'arta' (+5), 'hra' (+5), 'apters' (+5), ' tides' (+5), 'lat' (+5), ' Seasons' (+5)\n",
      "Explanation:\n",
      "Tokens: ' Tu' (+4), 'xit' (+4), ' now' (+4), ' Minutes' (+4), 'eur' (+4), ' palp' (+3), ' swat' (+3), '…]' (+3), ' Now' (+3), 'hem' (+3)\n",
      "Explanation:\n",
      "Tokens: ' the' (+9), ',' (+9), '-' (+8), ' and' (+8), ' a' (+8), '.' (+8), '\n",
      "' (+8), ' in' (+7), ' \"' (+7), ' (' (+7)\n",
      "Explanation:\n",
      "Tokens: ' awarding' (+5), 'krit' (+5), 'ativity' (+5), ' prest' (+5), ' Perception' (+5), 'tips' (+4), 'otine' (+4), 'gins' (+4), 'ator' (+4), ' course' (+4)\n",
      "Explanation:\n",
      "Tokens: 'pots' (+7), 'near' (+6), 'Materials' (+6), 'ARS' (+6), 'attach' (+6), ' Gloves' (+6), 'tips' (+6), ' replay' (+6), ' seiz' (+6), ' Compass' (+6)\n",
      "Explanation:\n",
      "lo: -43.35588359832764, hi: -23.750704193115237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>simplicity_score</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "      <th>top_tokens</th>\n",
       "      <th>top_logits</th>\n",
       "      <th>max_intervened_probs</th>\n",
       "      <th>predictiveness_scores</th>\n",
       "      <th>intervention_strengths</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.520758</td>\n",
       "      <td>-21.459478</td>\n",
       "      <td>-29.980236</td>\n",
       "      <td>bed</td>\n",
       "      <td>[ Bed, VAL,  Portable, `.,  Tablet, NPR,  OWN,...</td>\n",
       "      <td>[0.46401381492614746, 0.4439283609390259, 0.41...</td>\n",
       "      <td>[0.06392515450716019, 0.08862316608428955, 0.3...</td>\n",
       "      <td>[-10.361958503723145, -10.54781436920166, -10....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.318796</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-26.334055</td>\n",
       "      <td>the</td>\n",
       "      <td>[ the, ,, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8690711259841919, 0.8670687079429626, 0.798...</td>\n",
       "      <td>[0.10812719166278839, 0.27144700288772583, 0.4...</td>\n",
       "      <td>[-9.635549545288086, -9.525876998901367, -8.43...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.909198</td>\n",
       "      <td>-27.506376</td>\n",
       "      <td>-41.415573</td>\n",
       "      <td>mathematically</td>\n",
       "      <td>[ showc,  inval,  privile, SPONSORED,  contrac...</td>\n",
       "      <td>[0.9644421339035034, 0.9620494842529297, 0.953...</td>\n",
       "      <td>[0.053864531219005585, 0.03992891684174538, 0....</td>\n",
       "      <td>[-11.600071907043457, -13.232684135437012, -13...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.641014</td>\n",
       "      <td>-24.358652</td>\n",
       "      <td>-32.999664</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>[ the, ,, -,  and, .,  a, \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8719004392623901, 0.8665164709091187, 0.798...</td>\n",
       "      <td>[0.1077631413936615, 0.2683714032173157, 0.469...</td>\n",
       "      <td>[-10.033263206481934, -10.138259887695312, -8....</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-12.924649</td>\n",
       "      <td>-29.786480</td>\n",
       "      <td>-42.711128</td>\n",
       "      <td>govtrack</td>\n",
       "      <td>[govtrack,  epit,  ASA, gart, ilo, fighter, as...</td>\n",
       "      <td>[0.5115638375282288, 0.4868133068084717, 0.475...</td>\n",
       "      <td>[0.09865700453519821, 0.13692544400691986, 0.4...</td>\n",
       "      <td>[-10.611960411071777, -11.066946983337402, -13...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-12.280267</td>\n",
       "      <td>-30.001110</td>\n",
       "      <td>-42.281376</td>\n",
       "      <td>inventoryQuantity</td>\n",
       "      <td>[inventoryQuantity, LA, Pacific, aring, arta, ...</td>\n",
       "      <td>[0.5587134957313538, 0.545307993888855, 0.5137...</td>\n",
       "      <td>[0.05580572038888931, 0.0214955173432827, 0.38...</td>\n",
       "      <td>[-11.223621368408203, -11.544873237609863, -13...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-8.712907</td>\n",
       "      <td>-20.369608</td>\n",
       "      <td>-29.082516</td>\n",
       "      <td>Tu</td>\n",
       "      <td>[ Tu, xit,  now,  Minutes, eur,  palp,  swat, ...</td>\n",
       "      <td>[0.42431673407554626, 0.41883209347724915, 0.3...</td>\n",
       "      <td>[0.0740426629781723, 0.07886198908090591, 0.45...</td>\n",
       "      <td>[-11.076044082641602, -10.548381805419922, -10...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-6.602767</td>\n",
       "      <td>-18.015259</td>\n",
       "      <td>-24.618027</td>\n",
       "      <td>the</td>\n",
       "      <td>[ the, ,, -,  and,  a, ., \\n,  in,  \",  (]</td>\n",
       "      <td>[0.8759855628013611, 0.8640232682228088, 0.797...</td>\n",
       "      <td>[0.10659076273441315, 0.2607036530971527, 0.51...</td>\n",
       "      <td>[-9.611333847045898, -9.322669982910156, -7.46...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-6.810682</td>\n",
       "      <td>-27.261948</td>\n",
       "      <td>-34.072628</td>\n",
       "      <td>awarding</td>\n",
       "      <td>[ awarding, krit, ativity,  prest,  Perception...</td>\n",
       "      <td>[0.5239784717559814, 0.48778170347213745, 0.47...</td>\n",
       "      <td>[0.05849626660346985, 0.1298695057630539, 0.32...</td>\n",
       "      <td>[-10.17409610748291, -9.664491653442383, -9.09...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-7.455132</td>\n",
       "      <td>-26.523245</td>\n",
       "      <td>-33.978378</td>\n",
       "      <td>pots</td>\n",
       "      <td>[pots, near, Materials, ARS, attach,  Gloves, ...</td>\n",
       "      <td>[0.6794697642326355, 0.5856932997703552, 0.577...</td>\n",
       "      <td>[0.11339840292930603, 0.2301955670118332, 0.20...</td>\n",
       "      <td>[-10.550583839416504, -10.1970796585083, -10.5...</td>\n",
       "      <td>[3, 10, 32, 100, 320]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predictiveness_score  simplicity_score      score        explanation  \\\n",
       "0              -8.520758        -21.459478 -29.980236                bed   \n",
       "1              -8.318796        -18.015259 -26.334055                the   \n",
       "2             -13.909198        -27.506376 -41.415573     mathematically   \n",
       "3              -8.641014        -24.358652 -32.999664        punctuation   \n",
       "4             -12.924649        -29.786480 -42.711128           govtrack   \n",
       "..                   ...               ...        ...                ...   \n",
       "95            -12.280267        -30.001110 -42.281376  inventoryQuantity   \n",
       "96             -8.712907        -20.369608 -29.082516                 Tu   \n",
       "97             -6.602767        -18.015259 -24.618027                the   \n",
       "98             -6.810682        -27.261948 -34.072628           awarding   \n",
       "99             -7.455132        -26.523245 -33.978378               pots   \n",
       "\n",
       "                                           top_tokens  \\\n",
       "0   [ Bed, VAL,  Portable, `.,  Tablet, NPR,  OWN,...   \n",
       "1          [ the, ,, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "2   [ showc,  inval,  privile, SPONSORED,  contrac...   \n",
       "3          [ the, ,, -,  and, .,  a, \\n,  in,  \",  (]   \n",
       "4   [govtrack,  epit,  ASA, gart, ilo, fighter, as...   \n",
       "..                                                ...   \n",
       "95  [inventoryQuantity, LA, Pacific, aring, arta, ...   \n",
       "96  [ Tu, xit,  now,  Minutes, eur,  palp,  swat, ...   \n",
       "97         [ the, ,, -,  and,  a, ., \\n,  in,  \",  (]   \n",
       "98  [ awarding, krit, ativity,  prest,  Perception...   \n",
       "99  [pots, near, Materials, ARS, attach,  Gloves, ...   \n",
       "\n",
       "                                           top_logits  \\\n",
       "0   [0.46401381492614746, 0.4439283609390259, 0.41...   \n",
       "1   [0.8690711259841919, 0.8670687079429626, 0.798...   \n",
       "2   [0.9644421339035034, 0.9620494842529297, 0.953...   \n",
       "3   [0.8719004392623901, 0.8665164709091187, 0.798...   \n",
       "4   [0.5115638375282288, 0.4868133068084717, 0.475...   \n",
       "..                                                ...   \n",
       "95  [0.5587134957313538, 0.545307993888855, 0.5137...   \n",
       "96  [0.42431673407554626, 0.41883209347724915, 0.3...   \n",
       "97  [0.8759855628013611, 0.8640232682228088, 0.797...   \n",
       "98  [0.5239784717559814, 0.48778170347213745, 0.47...   \n",
       "99  [0.6794697642326355, 0.5856932997703552, 0.577...   \n",
       "\n",
       "                                 max_intervened_probs  \\\n",
       "0   [0.06392515450716019, 0.08862316608428955, 0.3...   \n",
       "1   [0.10812719166278839, 0.27144700288772583, 0.4...   \n",
       "2   [0.053864531219005585, 0.03992891684174538, 0....   \n",
       "3   [0.1077631413936615, 0.2683714032173157, 0.469...   \n",
       "4   [0.09865700453519821, 0.13692544400691986, 0.4...   \n",
       "..                                                ...   \n",
       "95  [0.05580572038888931, 0.0214955173432827, 0.38...   \n",
       "96  [0.0740426629781723, 0.07886198908090591, 0.45...   \n",
       "97  [0.10659076273441315, 0.2607036530971527, 0.51...   \n",
       "98  [0.05849626660346985, 0.1298695057630539, 0.32...   \n",
       "99  [0.11339840292930603, 0.2301955670118332, 0.20...   \n",
       "\n",
       "                                predictiveness_scores intervention_strengths  \\\n",
       "0   [-10.361958503723145, -10.54781436920166, -10....  [3, 10, 32, 100, 320]   \n",
       "1   [-9.635549545288086, -9.525876998901367, -8.43...  [3, 10, 32, 100, 320]   \n",
       "2   [-11.600071907043457, -13.232684135437012, -13...  [3, 10, 32, 100, 320]   \n",
       "3   [-10.033263206481934, -10.138259887695312, -8....  [3, 10, 32, 100, 320]   \n",
       "4   [-10.611960411071777, -11.066946983337402, -13...  [3, 10, 32, 100, 320]   \n",
       "..                                                ...                    ...   \n",
       "95  [-11.223621368408203, -11.544873237609863, -13...  [3, 10, 32, 100, 320]   \n",
       "96  [-11.076044082641602, -10.548381805419922, -10...  [3, 10, 32, 100, 320]   \n",
       "97  [-9.611333847045898, -9.322669982910156, -7.46...  [3, 10, 32, 100, 320]   \n",
       "98  [-10.17409610748291, -9.664491653442383, -9.09...  [3, 10, 32, 100, 320]   \n",
       "99  [-10.550583839416504, -10.1970796585083, -10.5...  [3, 10, 32, 100, 320]   \n",
       "\n",
       "    seed  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  \n",
       "..   ...  \n",
       "95    95  \n",
       "96    96  \n",
       "97    97  \n",
       "98    98  \n",
       "99    99  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_results = []\n",
    "for seed in range(100):\n",
    "    torch.manual_seed(seed)\n",
    "    feat = torch.randn(model.config.n_embd, device=\"cuda:0\")\n",
    "    feat /= feat.norm()  # NOTE: you want to make sure that the distribution of norms matches the distribution of norms in the dataset\n",
    "    result = get_scores(feat, random.randint(0, model.config.n_layer - 1))\n",
    "    random_results.append({\n",
    "        **result,\n",
    "        \"seed\": seed\n",
    "    })\n",
    "\n",
    "random_score_df = pd.DataFrame(random_results)\n",
    "lo, hi = random_score_df[\"score\"].quantile([0.025, 0.975])\n",
    "print(f\"lo: {lo}, hi: {hi}\")\n",
    "random_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mhist(random_score_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictiveness_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictiveness score for random direction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(random_score_df[\"predictiveness_score\"].tolist(), bins=40)\n",
    "plt.xlabel(\"Predictiveness score for random direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intervened_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mintervened_probs\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(), bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intervened_probs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='ev_correlation_score', ylabel='predictiveness_score'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=score_df[\"ev_correlation_score\"], y=score_df[\"predictiveness_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04755272716283798, 0.052709002047777176)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 1', ' 2', '1', ' 3', ' 4', ' 8', ' 7', ' 6', ' 5', ' 9']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 25\u001b[0m\n\u001b[1;32m      3\u001b[0m toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m 1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m1.12\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m 9\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(toks)\n\u001b[0;32m---> 25\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(toks, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     27\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embed(encodings)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "toks = \"\"\"\n",
    " 1\n",
    "1.12\n",
    " 2\n",
    "1.00\n",
    "1\n",
    "0.97\n",
    " 3\n",
    "0.95\n",
    " 4\n",
    "0.92\n",
    " 8\n",
    "0.91\n",
    " 7\n",
    "0.90\n",
    " 6\n",
    "0.89\n",
    " 5\n",
    "0.89\n",
    " 9\n",
    "\"\"\".split(\"\\n\")[1:-1:2]\n",
    "print(toks)\n",
    "encodings = tokenizer(toks, return_tensors=\"pt\").input_ids\n",
    "with torch.inference_mode():\n",
    "    embeddings = embed(encodings).squeeze(1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5440), tensor(0.2241))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([6.7853, 5.5921, 5.1799, 5.1528, 4.8437, 4.7982, 4.6626, 4.4385, 4.3578,\n",
       "         4.1862], grad_fn=<TopkBackward0>),\n",
       " indices=tensor([352, 362, 513,  16, 657, 604, 642, 718, 767, 807])),\n",
       " [' 1', ' 2', ' 3', '1', ' 0', ' 4', ' 5', ' 6', ' 7', ' 8'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 10\n",
    "fake_dictionary_weight = embed(tokenizer(\" 1\", return_tensors=\"pt\").input_ids).squeeze() # torch.randn(768)\n",
    "\n",
    "v = fake_dictionary_weight  # TODO account for layer norm\n",
    "# for l in range(layer, len(model.transformer.h)):\n",
    "#     v = model.transformer.h[l].ln_1(v)\n",
    "#     v = model.transformer.h[l].ln_2(v)\n",
    "    \n",
    "assert model.lm_head.bias is None\n",
    "logit_contributions = model.lm_head.weight.data @ v\n",
    "topk = logit_contributions.topk(10)\n",
    "topk, [tokenizer.decode(tok) for tok in topk.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   2.,\n",
       "          1.,   1.,   0.,   0.,   2.,   0.,   1.,   2.,   1.,   5.,   3.,\n",
       "         11.,  34.,  90., 170., 170., 127.,  87.,  25.,  13.,   5.,   4.,\n",
       "          4.,   2.,   0.,   0.,   0.,   2.,   0.,   1.,   0.,   1.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.]),\n",
       " array([-0.33035743, -0.31701479, -0.30367215, -0.29032951, -0.27698687,\n",
       "        -0.26364422, -0.25030158, -0.23695894, -0.2236163 , -0.21027366,\n",
       "        -0.19693102, -0.18358837, -0.17024573, -0.15690309, -0.14356045,\n",
       "        -0.13021781, -0.11687517, -0.10353253, -0.09018988, -0.07684724,\n",
       "        -0.0635046 , -0.05016196, -0.03681932, -0.02347668, -0.01013403,\n",
       "         0.00320861,  0.01655125,  0.02989389,  0.04323653,  0.05657917,\n",
       "         0.06992182,  0.08326446,  0.0966071 ,  0.10994974,  0.12329238,\n",
       "         0.13663502,  0.14997766,  0.16332031,  0.17666295,  0.19000559,\n",
       "         0.20334823,  0.21669087,  0.23003351,  0.24337616,  0.2567188 ,\n",
       "         0.27006144,  0.28340408,  0.29674672,  0.31008936,  0.32343201,\n",
       "         0.33677465,  0.35011729,  0.36345993,  0.37680257,  0.39014521,\n",
       "         0.40348786,  0.4168305 ,  0.43017314,  0.44351578,  0.45685842,\n",
       "         0.47020106,  0.4835437 ,  0.49688635,  0.51022899,  0.52357163,\n",
       "         0.53691427,  0.55025691,  0.56359955,  0.5769422 ,  0.59028484,\n",
       "         0.60362748,  0.61697012,  0.63031276,  0.6436554 ,  0.65699805,\n",
       "         0.67034069,  0.68368333,  0.69702597,  0.71036861,  0.72371125,\n",
       "         0.73705389,  0.75039654,  0.76373918,  0.77708182,  0.79042446,\n",
       "         0.8037671 ,  0.81710974,  0.83045239,  0.84379503,  0.85713767,\n",
       "         0.87048031,  0.88382295,  0.89716559,  0.91050824,  0.92385088,\n",
       "         0.93719352,  0.95053616,  0.9638788 ,  0.97722144,  0.99056409,\n",
       "         1.00390673]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(model.transformer.h[l].ln_1.bias.data.tolist(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_dictionary_weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output explanation scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "scorer_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "scorer = AutoModelForCausalLM.from_pretrained(scorer_name)\n",
    "scorer_tokenizer = AutoTokenizer.from_pretrained(scorer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9996)\n",
      "torch.Size([128256]) [3005, 6385]\n",
      "tensor(1.9996)\n",
      "torch.Size([128256]) [3005, 6385]\n",
      "tensor(1.9996)\n",
      "torch.Size([128256]) [3005, 6385]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2770), tensor(-3.0543), tensor(-3.0925))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation = \"I intervened on a feature that causes the model to use she/her pronouns more.\"\n",
    "random_explanation = \"I intervened on a feature that causes the model think of things related to mountains more.\"\n",
    "null_explanation = \"Here is a sentence.\"\n",
    "example = \"Sally went to the beach.\"\n",
    "\n",
    "# scorer_tokenizer.apply_chat_template([{\"role\": \"assistant\", \"content\": f\"Explanation:\\n{explanation}\\n\\nExample:\\n{example}\"}], tokenize=False, add_generation_prompt=False)\n",
    "def get_scorer_prompt(explanation):\n",
    "    return f\"Explanation:\\n{explanation}\\n\\nExample:\\n{example}\"\n",
    "\n",
    "def get_logprobs(text):\n",
    "    with torch.inference_mode():\n",
    "        logits = scorer(scorer_tokenizer(text, return_tensors=\"pt\").input_ids).logits[0, -1, :]\n",
    "    return logits.log_softmax(dim=-1)\n",
    "\n",
    "top_diff_toks = [\" She\", \" Her\"]\n",
    "top_logodds_diffs = torch.tensor([10.0, 8.0])\n",
    "top_diff_ids = [scorer_tokenizer.encode(tok, add_special_tokens=False)[0] for tok in top_diff_toks]\n",
    "\n",
    "def get_explanation_score(explanation):\n",
    "    logprobs = get_logprobs(get_scorer_prompt(explanation))\n",
    "    diff_distr = torch.sigmoid(top_logodds_diffs)\n",
    "    print(diff_distr.sum())\n",
    "    diff_distr /= diff_distr.sum()\n",
    "    # get the scorer's logP(next_tok | explanation, prompt) averaged over \n",
    "    # the next_toks that were most affected by the intervention \n",
    "    # (weighted by the sigmoid(bayes factor) distribution)\n",
    "    print(logprobs.shape, top_diff_ids)\n",
    "    logposterior = diff_distr * logprobs[top_diff_ids]\n",
    "    return logposterior.sum()\n",
    "\n",
    "get_explanation_score(explanation), get_explanation_score(random_explanation), get_explanation_score(null_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
