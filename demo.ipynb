{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Features Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by pip installing the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd sae_auto_interp && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ - Loading your own autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `nnsight` library to attach autoencoders to the module tree. \n",
    "\n",
    "At the time of writing (8/8/24), this feature isn't yet available on the main version of `nnsight`. Please install the `0.3` branch.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/ndif-team/nnsight.git@0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we'll load, cache, and evaluate some layer zero features from the recent OpenAI topk autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/autointerp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from sae_auto_interp.autoencoders.wrapper import AutoencoderLatents\n",
    "from sae_auto_interp.autoencoders.OpenAI import Autoencoder\n",
    "\n",
    "path = \"weights/gpt2_128k/0.pt\" # Change this line to your weights location.\n",
    "state_dict = torch.load(path)\n",
    "ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "ae.to(\"cuda:0\")\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helpful wrapper for collecting autoencoder latents. The wrapper is a `torch.nn.Module` which calls a given `forward` method at every forward pass. We'll use `partial` here so we don't run into late binding issues. \n",
    "\n",
    "If we use a lambda like `lambda x: ae.encode(x)[0]`, our wrappers will get only get a refrence to the last autoencoder's `encode` method in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(ae, x):\n",
    "    latents, _ = ae.encode(x)\n",
    "    return latents\n",
    "\n",
    "# We can simply add the new module as an attribute to an existing\n",
    "# submodule on GPT-2's module tree.\n",
    "submodule = model.transformer.h[0]\n",
    "submodule.ae = AutoencoderLatents(\n",
    "    ae, \n",
    "    partial(_forward, ae),\n",
    "    width=131_072\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `nnsight`'s `edit` context to set default interventions on the model's forward pass. \n",
    "\n",
    "Check out the [official demo](https://github.com/ndif-team/nnsight/blob/main/NNsight_v0_2.ipynb) to learn more about `nnsight` (which will be updated to 0.3 soon).\n",
    "\n",
    "As a quick refresher, `nnsight` allows users to execute PyTorch models, with interventions, lazily. A context manager collects operations, then compiles and executes them on completion. The `.edit` context defines default nodes in the intervention graph to be compiled on execution of the real model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.edit(\" \"):\n",
    "    acts = submodule.output[0]\n",
    "    submodule.ae(acts, hook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now collecting latents is as simple as saving the output of the submodule within the trace. This is uniquely helpful because (a) we can just handle references to submodules and access their `.ae` property which (b) removes the complexity of having to store a dictionary of submodules and their respective autoencoders, then passing the submodule's activations through the autoencoder every forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "       grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model.trace(\"hello, my name is\"):\n",
    "    latents = submodule.ae.output.save()\n",
    "\n",
    "latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above is a quite a bit of boilerplate, so we provide some starter code within the `.autoencoders` module. See the available options in the `__init__.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2️⃣ - Caching Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an edited model, lets cache activations for the first one hundred features in the autoencoder across 100k tokens. Ideally, you'll want to cache on as many tokens as necessary to get a wide distribution of activations for your autoencoder's rarer features.\n",
    "\n",
    "Let's define a couple of constants for our cache and load tokens. Again, we provide utils for loading a `torch.utils.data.Dataset` of tokens, but feel free to load and tokenize however you want. Note that our tokenizer appends padding to the start of every sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.features import FeatureCache\n",
    "from sae_auto_interp.utils import load_tokenized_data\n",
    "\n",
    "CTX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "N_TOKENS = 500_000\n",
    "N_SPLITS = 2\n",
    "\n",
    "tokens = load_tokenized_data(\n",
    "    CTX_LEN,\n",
    "    model.tokenizer,\n",
    "    \"kh4dien/fineweb-100m-sample\",\n",
    "    \"train[:15%]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cache accepts two dictionaries. \n",
    "\n",
    "`submodule_dict` is a `Dict[str, nnsight.Envoy]` which is iterated through during caching. \n",
    "\n",
    "`module_filter` is an optional filter for which we mask feature_ids found from caching. Note that this process is a slower, especially for larger numbers of tokens. However, it's very helpful for conserving CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['.transformer.h.0'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching features: 100%|██████████| 244/244 [00:16<00:00, 14.80it/s, Total Tokens=499,712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens processed: 499,712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "module_path = submodule._module_path\n",
    "\n",
    "submodule_dict = {module_path : submodule}\n",
    "module_filter = {module_path : torch.arange(100).to(\"cuda:0\")}\n",
    "\n",
    "cache = FeatureCache(\n",
    "    model, \n",
    "    submodule_dict, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    filters=module_filter\n",
    ")\n",
    "\n",
    "cache.run(N_TOKENS, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw features are saved as `safetensors` with the structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"location\" : torch.Tensor[\"n_activations\", 3],\n",
    "    \"activations\" : torch.Tensor[\"n_activations\"],\n",
    "}\n",
    "```\n",
    "\n",
    "Where each row of locations points to an activation, with the data `[batch_idx, seq_pos, feature_id]`. We also provide a splits parameter to save splits of the features into different `safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = \"raw_features/gpt2_128k\" # Change this line to your save location.\n",
    "cache.save_splits(\n",
    "    n_splits=N_SPLITS,\n",
    "    save_dir=raw_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ - Loading Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a data loader for reconstructing features from their locations and activations. \n",
    "\n",
    "The loader requires a `FeatureConfig` which details how features were saved and how to reconstruct examples. \n",
    "\n",
    "The `ExperimentConfig` configures how train and test examples are sampled for explanation and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.features import FeatureDataset, pool_max_activation_windows, sample\n",
    "from sae_auto_interp.config import FeatureConfig, ExperimentConfig\n",
    "\n",
    "cfg = FeatureConfig(\n",
    "    width = 131_072,\n",
    "    min_examples = 200,\n",
    "    max_examples = 10_000,\n",
    "    example_ctx_len = 64,\n",
    "    n_splits = 2\n",
    ")\n",
    "\n",
    "sample_cfg = ExperimentConfig()\n",
    "\n",
    "dataset = FeatureDataset(\n",
    "    raw_dir=raw_dir,\n",
    "    cfg=cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.load` method of dataset accepts functions to reconstruct and sample activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "    pool_max_activation_windows,\n",
    "    tokens=tokens,\n",
    "    ctx_len=cfg.example_ctx_len,\n",
    "    max_examples=cfg.max_examples,\n",
    ")\n",
    "\n",
    "sampler = partial(\n",
    "    sample,\n",
    "    cfg=sample_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a batch of records! The `.load` method is an iterator that just returns all records in a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading .transformer.h.0: 70it [00:00, 2660.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for records in dataset.load(constructor=constructor, sampler=sampler):\n",
    "    break\n",
    "\n",
    "record = records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display method in `.utils` just renders examples as html with their activating tokens highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".transformer.h.0_feature26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<|endoftext|> July 16.\n",
       "Whilst Xbox 360 users will be able to get a hold of Plants vs. Zombies and<mark> Gears</mark> of War 3. The releases will be staggered, with Plants vs. Zombies available from July 1, and<mark> Gears</mark> Of War 3 available from July 16.\n",
       "Although Games With Gold has been running for two<br><br><|endoftext|>iprocity. Ghost stands strong after one of the greatest matches in<mark> Gears</mark> of War 5 History. A team is only as hype and the coach who leads them. EU Finest is poised and prepared to take on the powerhouse that is TOX Gaming. All eyes are on how EU finest will perform after a successful<br><br><|endoftext|>s ultra-resilient Gorilla Glass, named after a gorilla��s inherent toughness and beauty.\n",
       "As with any glass, the process of making Gorilla Glass begins by melting down a mixture of sand and other chemicals in a furnace. After that, however, automated<mark> robotic</mark> arms form the molten mixture into<br><br><|endoftext|> undervalued effectively. Dustin strings give it you archduchies smudged exclusively. Sunny glister their deprecator excogitates bicephalous and spasmodically fluking headlines. Parabolic Federico wawl, his pronominally lionizing. Udall<mark> robot</mark>ized descarg<br><br><|endoftext|> changing diapers... hehe\n",
       "Some months ago I started with some quick concept stuff for the comic... some of it you can see here:\n",
       "First an alternate filthier version of the<mark> robot</mark>design from above:\n",
       "And here are some of the first character designs and sketches:\n",
       "Some fighter designs:\n",
       "A"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_auto_interp.utils import display\n",
    "\n",
    "print(record.feature)\n",
    "display(record, model.tokenizer, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ - Explaining Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define several clients for querying completion APIs such as vLLM and OpenRouter. For this example, we'll just use the OpenRouter client with `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import key\n",
    "from sae_auto_interp.clients import OpenRouter\n",
    "\n",
    "client = OpenRouter('openai/gpt-4o-mini', api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just load an explainer and pass the client, a tokenizer, and generation configs as optional keyword arguments. The explainer outputs an `ExplainerResult` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens \"Gears,\" \"robot,\" and \"Titan\" indicate specific references to popular video games and technology, suggesting the neuron's activation is related to gaming and robotics contexts.\n"
     ]
    }
   ],
   "source": [
    "from sae_auto_interp.explainers import SimpleExplainer\n",
    "\n",
    "explainer = SimpleExplainer(\n",
    "    client,\n",
    "    model.tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "explainer_result = await explainer(record)\n",
    "\n",
    "print(explainer_result.explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4️⃣ - Explaining Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can score explanations by loading a scorer and passing an feature record. The record should be updated to contain the `.explanation` attribute. \n",
    "\n",
    "In this example, we use the `RecallScorer` which requires random, non-activating examples to measure precision. For simplicity, we didn't sample those earlier so we'll just set those to train examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_auto_interp.scorers import RecallScorer\n",
    "\n",
    "scorer = RecallScorer(\n",
    "    client,\n",
    "    model.tokenizer,\n",
    "    max_tokens=25,\n",
    "    temperature=0.0,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "record.explanation = explainer_result.explanation\n",
    "record.random_examples = record.train\n",
    "\n",
    "score = await scorer(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got a score. The `.score` attribute contains a list of `ClassifierOutput`s. For each `ClassifierOutput`, we have the following attributes:\n",
    "\n",
    "- `distance` : The quantile of the sample.\n",
    "- `ground_truth` : Whether the sample actually activated or not.\n",
    "- `prediction` : The model's prediction for whether the example activated. \n",
    "- `highlighted` : Whether the example was \"highlighted\" or not. Only True for the `FuzzScorer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
