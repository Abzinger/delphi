{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject model\n",
    "# load SAEs without attaching them to the model\n",
    "# for now just use the Islam feature and explanation\n",
    "# load a scorer. The prompt should have the input as well this time\n",
    "# (for now) on random pretraining data, evaluate gpt2 with a hook that \n",
    "# adds a multiple of the Islam feature to the appropriate residual stream layer and position\n",
    "# Get the pre- and post-intervention output distributions of gpt2\n",
    "# (TODO: check if all the Islam features just have similar embeddings)\n",
    "# Show this to the scorer and get a score (scorer should be able to have a good prior without being given the clean output distribution)\n",
    "# Also get a simplicity score for the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_correlation_score</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature0</th>\n",
       "      <td>0.970093</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature19</th>\n",
       "      <td>0.966378</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature0</th>\n",
       "      <td>0.952401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature4</th>\n",
       "      <td>0.952061</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature5</th>\n",
       "      <td>0.949993</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature4</th>\n",
       "      <td>0.941871</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature11</th>\n",
       "      <td>0.930066</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature19</th>\n",
       "      <td>0.918787</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature14</th>\n",
       "      <td>0.906342</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature3</th>\n",
       "      <td>0.897080</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature11</th>\n",
       "      <td>0.883083</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature9</th>\n",
       "      <td>0.868529</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature3</th>\n",
       "      <td>0.810241</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature18</th>\n",
       "      <td>0.805457</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature16</th>\n",
       "      <td>0.782019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature11</th>\n",
       "      <td>0.779133</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature10</th>\n",
       "      <td>0.772255</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature7</th>\n",
       "      <td>0.764754</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature7</th>\n",
       "      <td>0.712047</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature12</th>\n",
       "      <td>0.711850</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature0</th>\n",
       "      <td>0.698656</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature12</th>\n",
       "      <td>0.678797</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature15</th>\n",
       "      <td>0.668821</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature13</th>\n",
       "      <td>0.668621</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature1</th>\n",
       "      <td>0.662348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature2</th>\n",
       "      <td>0.659443</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature13</th>\n",
       "      <td>0.649270</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature3</th>\n",
       "      <td>0.645487</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature6</th>\n",
       "      <td>0.643440</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature17</th>\n",
       "      <td>0.627347</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature8</th>\n",
       "      <td>0.583940</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature15</th>\n",
       "      <td>0.556673</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature4</th>\n",
       "      <td>0.470279</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature6</th>\n",
       "      <td>0.447680</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature7</th>\n",
       "      <td>0.390438</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature18</th>\n",
       "      <td>0.378467</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature14</th>\n",
       "      <td>0.316718</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature9</th>\n",
       "      <td>0.272719</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature10</th>\n",
       "      <td>0.228020</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature13</th>\n",
       "      <td>0.167355</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature5</th>\n",
       "      <td>0.095751</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature2</th>\n",
       "      <td>0.023048</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature15</th>\n",
       "      <td>-0.060541</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ev_correlation_score  layer  feature\n",
       ".transformer.h.2_feature0               0.970093      2        0\n",
       ".transformer.h.2_feature19              0.966378      2       19\n",
       ".transformer.h.0_feature0               0.952401      0        0\n",
       ".transformer.h.4_feature4               0.952061      4        4\n",
       ".transformer.h.0_feature5               0.949993      0        5\n",
       ".transformer.h.2_feature4               0.941871      2        4\n",
       ".transformer.h.2_feature11              0.930066      2       11\n",
       ".transformer.h.4_feature19              0.918787      4       19\n",
       ".transformer.h.0_feature14              0.906342      0       14\n",
       ".transformer.h.0_feature3               0.897080      0        3\n",
       ".transformer.h.0_feature11              0.883083      0       11\n",
       ".transformer.h.0_feature9               0.868529      0        9\n",
       ".transformer.h.2_feature3               0.810241      2        3\n",
       ".transformer.h.2_feature18              0.805457      2       18\n",
       ".transformer.h.0_feature16              0.782019      0       16\n",
       ".transformer.h.4_feature11              0.779133      4       11\n",
       ".transformer.h.0_feature10              0.772255      0       10\n",
       ".transformer.h.0_feature7               0.764754      0        7\n",
       ".transformer.h.2_feature7               0.712047      2        7\n",
       ".transformer.h.0_feature12              0.711850      0       12\n",
       ".transformer.h.4_feature0               0.698656      4        0\n",
       ".transformer.h.4_feature12              0.678797      4       12\n",
       ".transformer.h.4_feature15              0.668821      4       15\n",
       ".transformer.h.4_feature13              0.668621      4       13\n",
       ".transformer.h.2_feature1               0.662348      2        1\n",
       ".transformer.h.2_feature2               0.659443      2        2\n",
       ".transformer.h.0_feature13              0.649270      0       13\n",
       ".transformer.h.4_feature3               0.645487      4        3\n",
       ".transformer.h.2_feature6               0.643440      2        6\n",
       ".transformer.h.4_feature17              0.627347      4       17\n",
       ".transformer.h.4_feature8               0.583940      4        8\n",
       ".transformer.h.2_feature15              0.556673      2       15\n",
       ".transformer.h.0_feature4               0.470279      0        4\n",
       ".transformer.h.0_feature6               0.447680      0        6\n",
       ".transformer.h.4_feature7               0.390438      4        7\n",
       ".transformer.h.0_feature18              0.378467      0       18\n",
       ".transformer.h.4_feature14              0.316718      4       14\n",
       ".transformer.h.4_feature9               0.272719      4        9\n",
       ".transformer.h.4_feature10              0.228020      4       10\n",
       ".transformer.h.2_feature13              0.167355      2       13\n",
       ".transformer.h.2_feature5               0.095751      2        5\n",
       ".transformer.h.4_feature2               0.023048      4        2\n",
       ".transformer.h.0_feature15             -0.060541      0       15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "results_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/results/gpt2_simulation/all_at_once\"\n",
    "results = dict()\n",
    "for fname in Path(results_dir).iterdir():\n",
    "    with open(fname, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "    last = fname.stem.split(\".\")[-1]\n",
    "    layer = int(last.split(\"_\")[0])\n",
    "    feat = int(last[last.index(\"_feature\") + len(\"_feature\"):])\n",
    "    results[fname.stem] = {\"ev_correlation_score\": r[\"ev_correlation_score\"], \"layer\": layer, \"feature\": feat}\n",
    "input_scores_df = pd.DataFrame(results).T\n",
    "input_scores_df[\"layer\"] = input_scores_df[\"layer\"].astype(int)\n",
    "input_scores_df[\"feature\"] = input_scores_df[\"feature\"].astype(int)\n",
    "input_scores_df = input_scores_df.sort_values(\"ev_correlation_score\", ascending=False)\n",
    "unq_layers = input_scores_df[\"layer\"].unique()\n",
    "input_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"pile.jsonl\", \"r\") as f:\n",
    "    pile = random.sample([json.loads(line) for line in f.readlines()], 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/autointerp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alex/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "subject_device = \"cuda:0\"\n",
    "\n",
    "subject_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "subject = AutoModelForCausalLM.from_pretrained(subject_name).to(subject_device)\n",
    "subject_tokenizer = AutoTokenizer.from_pretrained(subject_name)\n",
    "subject_tokenizer.pad_token = subject_tokenizer.eos_token\n",
    "subject.config.pad_token_id = subject_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "scorer_device = \"cuda:1\"\n",
    "scorer_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "scorer = AutoModelForCausalLM.from_pretrained(\n",
    "    scorer_name,\n",
    "    device_map={\"\": scorer_device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    ")\n",
    "scorer_tokenizer = AutoTokenizer.from_pretrained(scorer_name)\n",
    "scorer_tokenizer.pad_token = scorer_tokenizer.eos_token\n",
    "scorer.config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "scorer.generation_config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "\n",
    "# explainer is the same model as the scorer\n",
    "explainer_device = scorer_device\n",
    "explainer = scorer\n",
    "explainer_tokenizer = scorer_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\n",
      "\n",
      "For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\n",
      "\n",
      "The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\n",
      "\n",
      "Neuron 1\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation: fruits\n",
      "\n",
      "Neuron 2\n",
      "<PROMPT>Once</PROMPT>\n",
      "Most increased tokens: ' upon' (+0.22), ' in' (+0.2), ' a' (+0.05), ' long' (+0.04)\n",
      "\n",
      "<PROMPT>Ryan Quarles\\n\\nRyan Francis Quarles (born October 20, 1983)</PROMPT>\n",
      "Most increased tokens: ' once' (+0.03), ' happily' (+0.31), ' for' (+0.01)\n",
      "\n",
      "<PROMPT>MSI Going Full Throttle @ CeBIT</PROMPT>\n",
      "Most increased tokens: ' Once' (+0.02), ' once' (+0.01), ' in' (+0.01), ' the' (+0.01), ' a' (+0.01), ' The' (+0.01)\n",
      "\n",
      "Explanation: storytelling\n",
      "\n",
      "Neuron 3\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation:\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "@dataclass\n",
    "class ExplainerInterventionExample:\n",
    "    prompt: str\n",
    "    top_tokens: list[str]\n",
    "    top_p_increases: list[float]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.prompt = self.prompt.replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "    def text(self) -> str:\n",
    "        tokens_str = \", \".join(f\"'{tok}' (+{round(p, 3)})\" for tok, p in zip(self.top_tokens, self.top_p_increases))\n",
    "        return f\"<PROMPT>{self.prompt}</PROMPT>\\nMost increased tokens: {tokens_str}\"\n",
    "    \n",
    "@dataclass\n",
    "class ExplainerNeuronFormatter:\n",
    "    intervention_examples: list[ExplainerInterventionExample]\n",
    "    explanation: str | None = None\n",
    "\n",
    "    def text(self) -> str:\n",
    "        text = \"\\n\\n\".join(example.text() for example in self.intervention_examples)\n",
    "        text += \"\\n\\nExplanation:\"\n",
    "        if self.explanation is not None:\n",
    "            text += \" \" + self.explanation\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_explainer_prompt(neuron_prompter: ExplainerNeuronFormatter, few_shot_examples: list[ExplainerNeuronFormatter] | None = None) -> str:\n",
    "    prompt = \"We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\\n\\n\" \\\n",
    "        \"For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\\n\\n\" \\\n",
    "        \"The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\\n\\n\"\n",
    "    \n",
    "    i = 1\n",
    "    for few_shot_example in few_shot_examples or []:\n",
    "        assert few_shot_example.explanation is not None\n",
    "        prompt += f\"Neuron {i}\\n\" + few_shot_example.text() + \"\\n\\n\"\n",
    "        i += 1\n",
    "\n",
    "    prompt += f\"Neuron {i}\\n\"\n",
    "    prompt += neuron_prompter.text()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "fs_examples = [\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"My favorite food is\",\n",
    "                top_tokens=[\" oranges\", \" bananas\", \" apples\"],\n",
    "                top_p_increases=[0.81, 0.09, 0.02]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Whenever I would see\",\n",
    "                top_tokens=[\" fruit\", \" a\", \" apples\", \" red\"],\n",
    "                top_p_increases=[0.09, 0.06, 0.06, 0.05]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"I like to eat\",\n",
    "                top_tokens=[\" fro\", \" fruit\", \" oranges\", \" bananas\", \" strawberries\"],\n",
    "                top_p_increases=[0.14, 0.13, 0.11, 0.10, 0.03]\n",
    "            )\n",
    "        ],\n",
    "        explanation=\"fruits\"\n",
    "    ),\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Once\",\n",
    "                top_tokens=[\" upon\", \" in\", \" a\", \" long\"],\n",
    "                top_p_increases=[0.22, 0.2, 0.05, 0.04]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Ryan Quarles\\\\n\\\\nRyan Francis Quarles (born October 20, 1983)\",\n",
    "                top_tokens=[\" once\", \" happily\", \" for\"],\n",
    "                top_p_increases=[0.03, 0.31, 0.01]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"MSI Going Full Throttle @ CeBIT\",\n",
    "                top_tokens=[\" Once\", \" once\", \" in\", \" the\", \" a\", \" The\"],\n",
    "                top_p_increases=[0.02, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "            ),\n",
    "        ],\n",
    "        explanation=\"storytelling\"\n",
    "    ),\n",
    "    # ExplainerNeuronFormatter(\n",
    "    #     intervention_examples=[\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"He owned the watch for a long time. While he never said it was\",\n",
    "    #             top_tokens=[\" hers\", \" hers\", \" hers\"],\n",
    "    #             top_p_increases=[0.09, 0.06, 0.06, 0.5]\n",
    "    #         ),\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"For some reason\",\n",
    "    #             top_tokens=[\" she\", \" her\", \" hers\"],\n",
    "    #             top_p_increases=[0.14, 0.01, 0.01]\n",
    "    #         ),\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"insurance does not cover\",\n",
    "    #             top_tokens=[\" her\", \" women\", \" her's\"],\n",
    "    #             top_p_increases=[0.10, 0.02, 0.01]\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     explanation=\"she/her pronouns\"\n",
    "    # )\n",
    "]\n",
    "\n",
    "neuron_prompter = copy.deepcopy(fs_examples[0])\n",
    "neuron_prompter.explanation = None\n",
    "print(get_explainer_prompt(neuron_prompter, fs_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT> oranges\n",
      "\n",
      "Explanation: ateg\n",
      "<PROMPT>From west to east, the westmost of the seven</PROMPT>WAY\n",
      "\n",
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT>\n"
     ]
    }
   ],
   "source": [
    "def get_scorer_simplicity_prompt(explanation):\n",
    "    prefix = \"Explanation\\n\\n\"\n",
    "    return f\"{prefix}{explanation}{scorer_tokenizer.eos_token}\", prefix\n",
    "\n",
    "def get_scorer_predictiveness_prompt(prompt, explanation, few_shot_prompts=None, few_shot_explanations=None, few_shot_tokens=None):\n",
    "    if few_shot_explanations is not None:\n",
    "        assert few_shot_tokens is not None and few_shot_prompts is not None\n",
    "        assert len(few_shot_explanations) == len(few_shot_tokens) == len(few_shot_prompts)\n",
    "        few_shot_prompt = \"\\n\\n\".join(get_scorer_predictiveness_prompt(pr, expl) + token for pr, expl, token in zip(few_shot_prompts, few_shot_explanations, few_shot_tokens)) + \"\\n\\n\"\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    return few_shot_prompt + f\"Explanation: {explanation}\\n<PROMPT>{prompt}</PROMPT>\"\n",
    "\n",
    "few_shot_prompts = [\"My favorite food is\", \"From west to east, the westmost of the seven\"]\n",
    "few_shot_explanations = [\"fruits and vegetables\", \"ateg\"]\n",
    "few_shot_tokens = [\" oranges\", \"WAY\"]\n",
    "# few_shot_prompts = [\"My favorite food is\", \"From west to east, the westmost of the seven\", \"He owned the watch for a long time. While he never said it was\"]\n",
    "# few_shot_explanations = [\"fruits and vegetables\", \"ateg\", \"she/her pronouns\"]\n",
    "# few_shot_tokens = [\" oranges\", \"WAY\", \" hers\"]\n",
    "print(get_scorer_predictiveness_prompt(few_shot_prompts[0], few_shot_explanations[0], few_shot_prompts, few_shot_explanations, few_shot_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def intervene(module, input, output, intervention_strength=10.0, position=-1, feat=None):\n",
    "    hiddens = output[0]  # the later elements of the tuple are the key value cache\n",
    "    hiddens[:, position, :] += intervention_strength * feat.to(hiddens.device)\n",
    "\n",
    "def get_texts(n, seed=42, randomize_length=True):\n",
    "    random.seed(seed)\n",
    "    texts = []\n",
    "    for _ in range(n):\n",
    "        \n",
    "        # sample a random text from the pile, and stop it at a random token position, less than 64 tokens\n",
    "        text = random.choice(pile)[\"text\"]\n",
    "        tokenized_text = subject_tokenizer.encode(text, add_special_tokens=False, max_length=64, truncation=True)\n",
    "        if len(tokenized_text) < 1:\n",
    "            continue\n",
    "        if randomize_length:\n",
    "            stop_pos = random.randint(1, min(len(tokenized_text), 63))\n",
    "        else:\n",
    "            stop_pos = 63\n",
    "        text = subject_tokenizer.decode(tokenized_text[:stop_pos])\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "n_explainer_texts = 3\n",
    "n_scorer_texts = 3\n",
    "n_explanations = 5\n",
    "# explainer_texts = get_texts(n_explainer_texts)\n",
    "# explainer_texts = [\"Current religion:\", \"A country that is\", \"Many people believe that\"]\n",
    "# scorer_texts = get_texts(n_scorer_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No scorer token found for '   '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '   '\n",
      "No scorer token found for '     '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '     '\n",
      "No scorer token found for '                     '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                     '\n",
      "No scorer token found for '             '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '             '\n",
      "No scorer token found for '                      '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                      '\n",
      "No scorer token found for '                '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                '\n",
      "No scorer token found for '                       '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                       '\n",
      "No scorer token found for '       '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '       '\n",
      "No scorer token found for '      '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '      '\n",
      "No scorer token found for '              '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '              '\n",
      "No scorer token found for '                 '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                 '\n",
      "No scorer token found for '  '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '  '\n",
      "No scorer token found for '           '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '           '\n",
      "No scorer token found for '          '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '          '\n",
      "No scorer token found for '                        '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                        '\n",
      "No scorer token found for '            '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '            '\n",
      "No scorer token found for '                  '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                  '\n",
      "No scorer token found for '               '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '               '\n",
      "No scorer token found for '         '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '         '\n",
      "No scorer token found for '                    '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                    '\n",
      "No scorer token found for '    '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '    '\n",
      "No scorer token found for '        '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '        '\n",
      "No scorer token found for '                   '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                   '\n"
     ]
    }
   ],
   "source": [
    "scorer_vocab = scorer_tokenizer.get_vocab()\n",
    "subject_vocab = subject_tokenizer.get_vocab()\n",
    "\n",
    "# Pre-compute the mapping of subject tokens to scorer tokens\n",
    "subject_to_scorer = {}\n",
    "text_subject_to_scorer = {}\n",
    "for subj_tok, subj_id in subject_vocab.items():\n",
    "    if subj_tok in scorer_vocab:\n",
    "        subject_to_scorer[subj_id] = scorer_vocab[subj_tok]\n",
    "        text_subject_to_scorer[subj_tok] = subj_tok\n",
    "    else:\n",
    "        for i in range(len(subj_tok) - 1, 0, -1):\n",
    "            if subj_tok[:i] in scorer_vocab:\n",
    "                subject_to_scorer[subj_id] = scorer_vocab[subj_tok[:i]]\n",
    "                text_subject_to_scorer[subj_tok] = subj_tok[:i]\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No scorer token found for '{subj_tok}'\")\n",
    "            subject_to_scorer[subj_id] = len(scorer_vocab) - 3  # some very rare token\n",
    "            print(f\"Using '{scorer_tokenizer.decode([len(scorer_vocab) - 3])}' as a placeholder for '{subj_tok}'\")\n",
    "subject_ids = torch.tensor(list(subject_to_scorer.keys()), device=scorer_device)\n",
    "scorer_ids = torch.tensor(list(subject_to_scorer.values()), device=scorer_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_intervention_examples = 5\n",
    "n_candidate_texts = 500\n",
    "candidate_texts = get_texts(n_candidate_texts, randomize_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 91.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 5.49 seconds\n",
      "Explainer took 1.59 seconds\n",
      "[' her', ' she', ' herself', ' She', ' hers']\n",
      "[0.9169365763664246, 0.06864659488201141, 0.014236417599022388, 8.034688653424382e-05, 2.3430850148997706e-08]\n",
      "[' her', ' she', ' herself', '<|padding|>', '<|endoftext|>']\n",
      "[0.9081762433052063, 0.0903496965765953, 0.001298226648941636, 0.0, 0.0]\n",
      "[' her', ' she', ' herself', ' She', ' hers']\n",
      "[0.8244187235832214, 0.15637685358524323, 0.01874895766377449, 0.0002570784417912364, 3.008909033042073e-08]\n",
      "Counter({'female pronouns': 3, 'female': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.12it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.49it/s]\n",
      "  0%|          | 1/202 [00:09<33:19,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 2.80 seconds\n",
      "predictiveness_score=-6.606029033660889\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 129.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 3.86 seconds\n",
      "Explainer took 1.39 seconds\n",
      "['ing', 'lying', 's', 'urch', 'fully']\n",
      "[0.12137379497289658, 0.09610537439584732, 0.07358910143375397, 0.030431075021624565, 0.026000313460826874]\n",
      "['s', 'rote', 'f', 'n', 'urch']\n",
      "[0.11145196110010147, 0.09851589053869247, 0.07393316924571991, 0.04224688559770584, 0.023582376539707184]\n",
      "['s', 'und', 'ing', 'telling', ' Frost']\n",
      "[0.14011235535144806, 0.030827190726995468, 0.026327718049287796, 0.023694975301623344, 0.02085311897099018]\n",
      "Counter({'storytelling': 2, 'telling': 1, 'adjectives': 1, 'ending': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.54it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.53it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "  1%|          | 2/202 [00:22<37:34, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.81 seconds\n",
      "predictiveness_score=-11.82196922302246\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 85.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 5.87 seconds\n",
      "Explainer took 1.13 seconds\n",
      "['@', 'ibrary', '�', '<|padding|>', '<|endoftext|>']\n",
      "[0.9998772144317627, 1.4673590120750646e-10, 0.0, 0.0, 0.0]\n",
      "['@', '�', '<|endoftext|>', '�', '<|padding|>']\n",
      "[0.9986705183982849, 0.0, 0.0, 0.0, 0.0]\n",
      "['@', '�', '<|endoftext|>', '�', '<|padding|>']\n",
      "[0.9998978972434998, 0.0, 0.0, 0.0, 0.0]\n",
      "Counter({'email': 2, 'emails': 1, 'Twitter handles': 1, 'twitter': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.93it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.49it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.55it/s]\n",
      "  1%|▏         | 3/202 [00:35<41:07, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.66 seconds\n",
      "predictiveness_score=-9.237468910217284\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 132.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 3.79 seconds\n",
      "Explainer took 1.57 seconds\n",
      "['essee', 'king', ' time', 'ns', ' settlement']\n",
      "[0.6908113360404968, 0.14241111278533936, 0.038456305861473083, 0.01946074701845646, 0.012874310836195946]\n",
      "['king', 'ding', 'ky', 'ks', 's']\n",
      "[0.8632607460021973, 0.04527141526341438, 0.007642056792974472, 0.007184164598584175, 0.0057197813875973225]\n",
      "['king', ' time', 'name', ' name', ' Malaysia']\n",
      "[0.8412436842918396, 0.01505723875015974, 0.014293982647359371, 0.010964670218527317, 0.006286932621151209]\n",
      "Counter({'time': 2, 'names': 1, 'king': 1, 'the number 10': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.59it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.20it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.95it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.03it/s]\n",
      "  2%|▏         | 4/202 [00:47<39:48, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.12 seconds\n",
      "predictiveness_score=-10.92839698791504\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "from sae_auto_interp.autoencoders.OpenAI.model import Autoencoder\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    subject_layers = subject.transformer.h\n",
    "except:\n",
    "    subject_layers = subject.gpt_neox.layers\n",
    "\n",
    "def get_encoder_decoder_weights(feat_idx, feat_layer, device):\n",
    "    # weight_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/weights/gpt2_128k\"\n",
    "    # path = f\"{weight_dir}/{feat_layer}.pt\"\n",
    "    # state_dict = torch.load(path)\n",
    "    # ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "    # decoder_feat = ae.decoder.weight[:, feat_idx].to(device)\n",
    "    # encoder_feat = ae.encoder.weight[feat_idx, :].to(device)\n",
    "    weight_dir = f\"/mnt/ssd-1/alexm/dictionary_learning/dictionaries/pythia-70m-deduped/resid_out_layer{feat_layer}/10_32768/ae.pt\"\n",
    "    state_dict = torch.load(weight_dir)\n",
    "    encoder_feat = state_dict['encoder.weight'][feat_idx, :]\n",
    "    decoder_feat = state_dict['decoder.weight'][:, feat_idx]\n",
    "    return encoder_feat, decoder_feat\n",
    "\n",
    "all_results = []\n",
    "\n",
    "feat_idxs = [12420] + list(range(100))\n",
    "feat_layers = [4, 2]\n",
    "total_iterations = len(feat_idxs) * len(feat_layers)\n",
    "for feat_idx, feat_layer in tqdm(product(feat_idxs, feat_layers), total=total_iterations):\n",
    "    scorer_intervention_strengths = [0, 10, 32, 100, 320, 1000]\n",
    "    explainer_intervention_strength = 32\n",
    "\n",
    "    print(\"Loading autoencoder...\", end=\"\")\n",
    "    encoder_feat, decoder_feat = get_encoder_decoder_weights(feat_idx, feat_layer, subject_device)\n",
    "\n",
    "    ### Find examples where the feature activates\n",
    "    # Remove any hooks\n",
    "    for l in range(len(subject_layers)):\n",
    "        subject_layers[l]._forward_hooks.clear()\n",
    "    print(\"done\")\n",
    "\n",
    "    selection_time = time.time()\n",
    "    subtexts = []\n",
    "    subtext_acts = []\n",
    "    for text in tqdm(candidate_texts, total=len(candidate_texts)):\n",
    "        input_ids = subject_tokenizer(text, return_tensors=\"pt\").input_ids.to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            out = subject(input_ids, output_hidden_states=True)\n",
    "            # hidden_states is actually one longer than the number of layers, because it includes the input embeddings\n",
    "            h = out.hidden_states[feat_layer + 1].squeeze(0)\n",
    "            # feat_acts = ae.activation(ae.encoder(h))[:, feat_idx]\n",
    "            feat_acts = h @ encoder_feat\n",
    "            # the first token position just has way higher norm all the time for some reason\n",
    "            feat_acts[0] = 0\n",
    "\n",
    "        for i in range(1, len(feat_acts) + 1):\n",
    "            reassembled_text = subject_tokenizer.decode(input_ids[0, :i])\n",
    "            subtexts.append(reassembled_text)\n",
    "            subtext_acts.append(feat_acts[i - 1].item())\n",
    "\n",
    "    # get a random sample of activating contexts\n",
    "    subtext_acts = torch.tensor(subtext_acts)\n",
    "    n_candidates = 200\n",
    "    candidate_indices = subtext_acts.topk(n_candidates).indices\n",
    "    sampled_indices = np.random.choice(candidate_indices.numpy(), n_scorer_texts + n_explainer_texts, replace=False)\n",
    "    \n",
    "    # Get top k subtexts and their activations\n",
    "    sampled_subtexts = [subtexts[i] for i in sampled_indices]\n",
    "    sampled_activations = subtext_acts[sampled_indices]\n",
    "\n",
    "    random.shuffle(sampled_subtexts)  # just as assurance\n",
    "    scorer_texts = sampled_subtexts[:n_scorer_texts]\n",
    "    explainer_texts = sampled_subtexts[n_scorer_texts:]\n",
    "    selection_time = time.time() - selection_time\n",
    "    print(f\"Selection took {selection_time:.2f} seconds\")\n",
    "\n",
    "    # get explanation\n",
    "    def get_subject_logits(text, layer, intervention_strength=0.0, position=-1, feat=None):\n",
    "        for l in range(len(subject_layers)):\n",
    "            subject_layers[l]._forward_hooks.clear()\n",
    "        subject_layers[layer].register_forward_hook(partial(intervene, intervention_strength=intervention_strength, position=-1, feat=feat))\n",
    "\n",
    "        inputs = subject_tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = subject(**inputs)\n",
    "\n",
    "        return outputs.logits[0, -1, :]\n",
    "\n",
    "    explainer_time = time.time()\n",
    "    intervention_examples = []\n",
    "    for text in explainer_texts:\n",
    "        clean_logits = get_subject_logits(text, feat_layer, intervention_strength=0.0, feat=decoder_feat)\n",
    "        intervened_logits = get_subject_logits(text, feat_layer, intervention_strength=explainer_intervention_strength, feat=decoder_feat)\n",
    "        top_probs = (intervened_logits.softmax(dim=-1) - clean_logits.softmax(dim=-1)).topk(n_intervention_examples)\n",
    "        \n",
    "        top_tokens = [subject_tokenizer.decode(i) for i in top_probs.indices]\n",
    "        top_p_increases = top_probs.values.tolist()\n",
    "        intervention_examples.append(\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=text,\n",
    "                top_tokens=top_tokens,\n",
    "                top_p_increases=top_p_increases\n",
    "            )\n",
    "        )\n",
    "\n",
    "    neuron_prompter = ExplainerNeuronFormatter(\n",
    "        intervention_examples=intervention_examples\n",
    "    )\n",
    "\n",
    "    # TODO: improve the few-shot examples\n",
    "    explainer_prompt = get_explainer_prompt(neuron_prompter, fs_examples)\n",
    "    explainer_input_ids = explainer_tokenizer(explainer_prompt, return_tensors=\"pt\").input_ids.to(explainer_device)\n",
    "    with torch.inference_mode():\n",
    "        samples = explainer.generate(explainer_input_ids, max_new_tokens=100, eos_token_id=explainer_tokenizer.encode(\"\\n\")[-1], num_return_sequences=n_explanations)[:, explainer_input_ids.shape[1]:]\n",
    "    explanations = Counter([explainer_tokenizer.decode(sample).split(\"\\n\")[0].strip() for sample in samples])\n",
    "    explainer_time = time.time() - explainer_time\n",
    "    print(f\"Explainer took {explainer_time:.2f} seconds\")\n",
    "\n",
    "    for ie in intervention_examples:\n",
    "        print(ie.top_tokens)\n",
    "        print(ie.top_p_increases)\n",
    "    print(explanations)\n",
    "\n",
    "    scoring_time = time.time()\n",
    "    predictiveness_score_by_explanation = dict()\n",
    "    normalized_predictiveness_score_by_explanation = dict()\n",
    "    all_pred_scores = []\n",
    "    scoring_interventions = dict()\n",
    "    for explanation in explanations:\n",
    "        expl_predictiveness_scores = []\n",
    "        scoring_interventions[explanation] = dict()\n",
    "        for scorer_intervention_strength in tqdm(scorer_intervention_strengths):\n",
    "            \n",
    "            current_pred_scores = []\n",
    "            max_intervened_prob = 0.0\n",
    "            scoring_interventions[scorer_intervention_strength] = []\n",
    "            for text in scorer_texts:\n",
    "                \n",
    "                intervened_probs = get_subject_logits(text, feat_layer, intervention_strength=scorer_intervention_strength, feat=decoder_feat).softmax(dim=-1).to(scorer_device)\n",
    "\n",
    "                # get the explanation predictiveness\n",
    "                scorer_predictiveness_prompt = get_scorer_predictiveness_prompt(text, explanation, few_shot_prompts, few_shot_explanations, few_shot_tokens)\n",
    "                scorer_input_ids = scorer_tokenizer(scorer_predictiveness_prompt, return_tensors=\"pt\").input_ids.to(scorer_device)\n",
    "                with torch.inference_mode():\n",
    "                    scorer_logits = scorer(scorer_input_ids).logits[0, -1, :]\n",
    "                    scorer_logp = scorer_logits.log_softmax(dim=-1)\n",
    "                \n",
    "                current_pred_scores.append((intervened_probs[subject_ids] * scorer_logp[scorer_ids]).sum())\n",
    "\n",
    "                topk = intervened_probs.topk(n_intervention_examples).indices\n",
    "                top_tokens = [subject_tokenizer.decode(i) for i in topk]\n",
    "                scoring_interventions[scorer_intervention_strength].append({\n",
    "                    \"prompt\": text,\n",
    "                    \"top_tokens\": top_tokens,\n",
    "                    \"top_token_probs\": intervened_probs[topk].tolist()\n",
    "                })\n",
    "\n",
    "            expl_predictiveness_scores.append(torch.tensor(current_pred_scores).mean().item())\n",
    "            all_pred_scores.extend(current_pred_scores * explanations[explanation])  # as if we did the inference on the scorer multiple times\n",
    "    \n",
    "        assert scorer_intervention_strengths[0] == 0\n",
    "        null_predictiveness_score = expl_predictiveness_scores[0]\n",
    "        normalized_predictiveness_scores = [score - null_predictiveness_score for score in expl_predictiveness_scores[1:]]\n",
    "        normalized_predictiveness_score = sum(normalized_predictiveness_scores) / len(normalized_predictiveness_scores)\n",
    "        predictiveness_score = normalized_predictiveness_score + null_predictiveness_score\n",
    "        \n",
    "        predictiveness_score_by_explanation[explanation] = predictiveness_score\n",
    "        normalized_predictiveness_score_by_explanation[explanation] = normalized_predictiveness_score\n",
    "    \n",
    "    # note that this computes stderr over explanations, pile samples, *and* intervention strengths (which is kind of weird)\n",
    "    pred_score_stderr = torch.std(torch.tensor(all_pred_scores)).item() / len(all_pred_scores) ** 0.5\n",
    "    pred_score = torch.mean(torch.tensor(all_pred_scores)).item()\n",
    "    normalized_predictiveness_score = sum([normalized_predictiveness_score_by_explanation[explanation] * count for explanation, count in explanations.items()]) / sum(explanations.values())\n",
    "\n",
    "    scoring_time = time.time() - scoring_time\n",
    "    print(f\"Scoring took {scoring_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"{normalized_predictiveness_score=}\")\n",
    "    print()\n",
    "    print()\n",
    "    all_results.append({\n",
    "        \"feat_idx\": feat_idx,\n",
    "        \"feat_layer\": feat_layer,\n",
    "        \"explanations\": dict(explanations),\n",
    "        \"predictiveness_score\": pred_score,\n",
    "        \"normalized_predictiveness_score\": normalized_predictiveness_score,\n",
    "        \"predictiveness_score_stderr\": pred_score_stderr,\n",
    "        \"max_predictiveness_score\": max(predictiveness_score_by_explanation.values()),\n",
    "        \"max_normalized_predictiveness_score\": max(normalized_predictiveness_score_by_explanation.values()),\n",
    "        \"explainer_prompts\": [example.prompt for example in intervention_examples],\n",
    "        \"explainer_top_tokens\": [example.top_tokens for example in intervention_examples],\n",
    "        \"explainer_top_p_increases\": [example.top_p_increases for example in intervention_examples],\n",
    "        \"scorer_intervention_strengths\": scorer_intervention_strengths,\n",
    "        \"explainer_intervention_strength\": explainer_intervention_strength,\n",
    "        \"scorer_texts\": scorer_texts,\n",
    "        \"explainer_texts\": explainer_texts,\n",
    "        \"predictiveness_score_by_explanation\": predictiveness_score_by_explanation,\n",
    "        \"normalized_predictiveness_score_by_explanation\": normalized_predictiveness_score_by_explanation,\n",
    "        \"scoring_interventions\": scoring_interventions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanations</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>normalized_predictiveness_score</th>\n",
       "      <th>predictiveness_score_stderr</th>\n",
       "      <th>max_predictiveness_score</th>\n",
       "      <th>max_normalized_predictiveness_score</th>\n",
       "      <th>explainer_prompts</th>\n",
       "      <th>explainer_top_tokens</th>\n",
       "      <th>explainer_top_p_increases</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_score_by_explanation</th>\n",
       "      <th>normalized_predictiveness_score_by_explanation</th>\n",
       "      <th>scoring_interventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12420</td>\n",
       "      <td>4</td>\n",
       "      <td>{'first person': 1, 'pronouns': 3, 'female pro...</td>\n",
       "      <td>-7.926619</td>\n",
       "      <td>2.511932</td>\n",
       "      <td>0.243935</td>\n",
       "      <td>-6.117489</td>\n",
       "      <td>3.846179</td>\n",
       "      <td>[A San Diego runner and cancer survivor says s...</td>\n",
       "      <td>[[ her,  she,  herself, &lt;|padding|&gt;, &lt;|endofte...</td>\n",
       "      <td>[[0.9901813268661499, 0.008692975156009197, 0....</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[A group of Bitcoin miners that bought into th...</td>\n",
       "      <td>[A San Diego runner and cancer survivor says s...</td>\n",
       "      <td>{'first person': -9.072517585754394, 'pronouns...</td>\n",
       "      <td>{'first person': 0.6047761917114258, 'pronouns...</td>\n",
       "      <td>{'first person': {}, 0: [{'prompt': 'A group o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_idx  feat_layer                                       explanations  \\\n",
       "0     12420           4  {'first person': 1, 'pronouns': 3, 'female pro...   \n",
       "\n",
       "   predictiveness_score  normalized_predictiveness_score  \\\n",
       "0             -7.926619                         2.511932   \n",
       "\n",
       "   predictiveness_score_stderr  max_predictiveness_score  \\\n",
       "0                     0.243935                 -6.117489   \n",
       "\n",
       "   max_normalized_predictiveness_score  \\\n",
       "0                             3.846179   \n",
       "\n",
       "                                   explainer_prompts  \\\n",
       "0  [A San Diego runner and cancer survivor says s...   \n",
       "\n",
       "                                explainer_top_tokens  \\\n",
       "0  [[ her,  she,  herself, <|padding|>, <|endofte...   \n",
       "\n",
       "                           explainer_top_p_increases  \\\n",
       "0  [[0.9901813268661499, 0.008692975156009197, 0....   \n",
       "\n",
       "  scorer_intervention_strengths  explainer_intervention_strength  \\\n",
       "0   [0, 10, 32, 100, 320, 1000]                               32   \n",
       "\n",
       "                                        scorer_texts  \\\n",
       "0  [A group of Bitcoin miners that bought into th...   \n",
       "\n",
       "                                     explainer_texts  \\\n",
       "0  [A San Diego runner and cancer survivor says s...   \n",
       "\n",
       "                 predictiveness_score_by_explanation  \\\n",
       "0  {'first person': -9.072517585754394, 'pronouns...   \n",
       "\n",
       "      normalized_predictiveness_score_by_explanation  \\\n",
       "0  {'first person': 0.6047761917114258, 'pronouns...   \n",
       "\n",
       "                               scoring_interventions  \n",
       "0  {'first person': {}, 0: [{'prompt': 'A group o...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "all_df.to_json(f\"counterfactual_results/{subject_name.split('/')[-1]}_{len(feat_layers)}layers_{len(feat_idxs)}feats.json\")\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mscoring_interventions\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "all_df.to_pickle(f\"counterfactual_results/{len(feat_layers)}layers_{len(feat_idxs)}feats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanations</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>normalized_predictiveness_score</th>\n",
       "      <th>predictiveness_score_stderr</th>\n",
       "      <th>max_predictiveness_score</th>\n",
       "      <th>max_normalized_predictiveness_score</th>\n",
       "      <th>explainer_prompts</th>\n",
       "      <th>explainer_top_tokens</th>\n",
       "      <th>explainer_top_p_increases</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_score_by_explanation</th>\n",
       "      <th>normalized_predictiveness_score_by_explanation</th>\n",
       "      <th>scoring_interventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>{'time': 1, 'year': 1, 'dates': 2, 'years': 1}</td>\n",
       "      <td>-4.859853</td>\n",
       "      <td>7.479265</td>\n",
       "      <td>0.372199</td>\n",
       "      <td>-3.140593</td>\n",
       "      <td>8.166472</td>\n",
       "      <td>[\\n501 F.Supp. 158 (1980)\\nWilliam L. PALOMBI\\...</td>\n",
       "      <td>[[ 1978,  1974,  1976,  1971,  1984], [ 1998, ...</td>\n",
       "      <td>[[0.0273308121, 0.0123613551, 0.00622998600000...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Live, Thursday, The]</td>\n",
       "      <td>[\\n501 F.Supp. 158 (1980)\\nWilliam L. PALOMBI\\...</td>\n",
       "      <td>{'time': -3.6941101551, 'year': -3.2754569769,...</td>\n",
       "      <td>{'time': 7.3446919917999995, 'year': 7.9918350...</td>\n",
       "      <td>{'time': {}, '0': [{'prompt': 'Live', 'top_tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>625</td>\n",
       "      <td>4</td>\n",
       "      <td>{'numbers': 5}</td>\n",
       "      <td>-5.918956</td>\n",
       "      <td>6.191844</td>\n",
       "      <td>0.268001</td>\n",
       "      <td>-4.886982</td>\n",
       "      <td>6.191844</td>\n",
       "      <td>[\", �, /]</td>\n",
       "      <td>[[6, 7, 8, 9, 3], [7, 6, 8, 9, 4], [6, 7, 8, 9...</td>\n",
       "      <td>[[0.232171759, 0.2214426398, 0.1743511707, 0.1...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[[, Theme, A]</td>\n",
       "      <td>[\", �, /]</td>\n",
       "      <td>{'numbers': -4.8869820595}</td>\n",
       "      <td>{'numbers': 6.1918438910999996}</td>\n",
       "      <td>{'numbers': {}, '0': [{'prompt': '[', 'top_tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>{'time, effort, energy, space, money': 1, 'tim...</td>\n",
       "      <td>-6.782176</td>\n",
       "      <td>5.213839</td>\n",
       "      <td>0.256991</td>\n",
       "      <td>-5.243265</td>\n",
       "      <td>5.839702</td>\n",
       "      <td>[What people say about working at Apple\\n\\nGoo...</td>\n",
       "      <td>[[ time,  money,  effort,  space,  energy], [ ...</td>\n",
       "      <td>[[0.7181766033, 0.1450483948, 0.0541959144, 0....</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[The, error, Long]</td>\n",
       "      <td>[What people say about working at Apple\\n\\nGoo...</td>\n",
       "      <td>{'time, effort, energy, space, money': -5.2540...</td>\n",
       "      <td>{'time, effort, energy, space, money': 5.83970...</td>\n",
       "      <td>{'time, effort, energy, space, money': {}, '0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>471</td>\n",
       "      <td>4</td>\n",
       "      <td>{'code': 2, 'or': 2, 'programming': 1}</td>\n",
       "      <td>-6.678521</td>\n",
       "      <td>4.752080</td>\n",
       "      <td>0.198007</td>\n",
       "      <td>-5.159261</td>\n",
       "      <td>5.517492</td>\n",
       "      <td>[Q, Q:\\n\\nC++ while loop with 2 conditions, VS...</td>\n",
       "      <td>[[ or, �, &lt;|endoftext|&gt;, �, &lt;|padding|&gt;], [ or...</td>\n",
       "      <td>[[0.9998978376000001, 0.0, 0.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Long, \", British]</td>\n",
       "      <td>[Q, Q:\\n\\nC++ while loop with 2 conditions, VS...</td>\n",
       "      <td>{'code': -6.3253965378, 'or': -5.1592611313, '...</td>\n",
       "      <td>{'code': 4.250957489, 'or': 5.5174919128, 'pro...</td>\n",
       "      <td>{'code': {}, '0': [{'prompt': 'Long', 'top_tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>{'results': 1, 'including': 2, 'questions': 1,...</td>\n",
       "      <td>-8.950260</td>\n",
       "      <td>3.030504</td>\n",
       "      <td>0.336055</td>\n",
       "      <td>-6.245436</td>\n",
       "      <td>5.287371</td>\n",
       "      <td>[Q, Results, N]</td>\n",
       "      <td>[[ including, including,  assuming,  address, ...</td>\n",
       "      <td>[[0.9487411976000001, 0.0254448727, 0.00905056...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Modern, Pop, Posts]</td>\n",
       "      <td>[Q, Results, N]</td>\n",
       "      <td>{'results': -10.8864421844, 'including': -6.24...</td>\n",
       "      <td>{'results': 0.4372577667, 'including': 5.28737...</td>\n",
       "      <td>{'results': {}, '0': [{'prompt': 'Modern', 'to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>572</td>\n",
       "      <td>2</td>\n",
       "      <td>{'punctuation': 2, 'writing': 1, 'comma': 1, '...</td>\n",
       "      <td>-10.720153</td>\n",
       "      <td>-8.044349</td>\n",
       "      <td>0.421816</td>\n",
       "      <td>-11.897296</td>\n",
       "      <td>-8.014710</td>\n",
       "      <td>[Philtrum\\n\\nThe philtrum (,  philtron, lit. \"...</td>\n",
       "      <td>[[,,  it,  I,  we,  there], [,,  it,  however,...</td>\n",
       "      <td>[[0.8786817193, 0.0091542406, 0.0050156475, 0....</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[4,75 millions de barils de diesel par année, ...</td>\n",
       "      <td>[Philtrum\\n\\nThe philtrum (,  philtron, lit. \"...</td>\n",
       "      <td>{'punctuation': -11.9374860764, 'writing': -12...</td>\n",
       "      <td>{'punctuation': -8.0158463955, 'writing': -8.0...</td>\n",
       "      <td>{'punctuation': {}, '0': [{'prompt': '4,75 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>427</td>\n",
       "      <td>4</td>\n",
       "      <td>{'nanofilms': 1, 'nanotechnology': 1, 'chemist...</td>\n",
       "      <td>-12.814263</td>\n",
       "      <td>-8.293813</td>\n",
       "      <td>0.524284</td>\n",
       "      <td>-14.102210</td>\n",
       "      <td>-8.203892</td>\n",
       "      <td>[Layer-by-layer films made from extracellular ...</td>\n",
       "      <td>[[\\n,  chemical,  a,  single,  operator], [ a,...</td>\n",
       "      <td>[[0.020971164100000002, 0.0185869616, 0.018565...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Q:\\n\\nhow to regex across line breaks\\n\\nI am...</td>\n",
       "      <td>[Layer-by-layer films made from extracellular ...</td>\n",
       "      <td>{'nanofilms': -14.1022098541, 'nanotechnology'...</td>\n",
       "      <td>{'nanofilms': -8.2038920403, 'nanotechnology':...</td>\n",
       "      <td>{'nanofilms': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "how to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>835</td>\n",
       "      <td>2</td>\n",
       "      <td>{'punctuation': 1, 'music': 3, 'supergroup': 1}</td>\n",
       "      <td>-11.067059</td>\n",
       "      <td>-8.317599</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>-12.421036</td>\n",
       "      <td>-8.270683</td>\n",
       "      <td>[Atsuko Enomoto\\n\\nis a Japanese voice actress...</td>\n",
       "      <td>[[ B,  and,  the,  K, ?\"], [ B, ?, ,,  and,  B...</td>\n",
       "      <td>[[0.0584790595, 0.031087832500000002, 0.023454...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[---\\nabstract: |\\n    Given a flat injective ...</td>\n",
       "      <td>[Atsuko Enomoto\\n\\nis a Japanese voice actress...</td>\n",
       "      <td>{'punctuation': -12.421035862, 'music': -12.43...</td>\n",
       "      <td>{'punctuation': -8.441075658799999, 'music': -...</td>\n",
       "      <td>{'punctuation': {}, '0': [{'prompt': '---\n",
       "abst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>348</td>\n",
       "      <td>2</td>\n",
       "      <td>{'math': 3, 'statistics': 1, 'numbers': 1}</td>\n",
       "      <td>-10.709068</td>\n",
       "      <td>-9.232220</td>\n",
       "      <td>0.559356</td>\n",
       "      <td>-12.138460</td>\n",
       "      <td>-8.751772</td>\n",
       "      <td>[Announcing Reports\\n\\n, letters picked withou...</td>\n",
       "      <td>[[import,  seriously,  offers, 's,  -], [-,  -...</td>\n",
       "      <td>[[0.3821032345, 0.1341270208, 0.05702698980000...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Q:\\n\\nsymfony credentials issue with sfDoctri...</td>\n",
       "      <td>[Announcing Reports\\n\\n, letters picked withou...</td>\n",
       "      <td>{'math': -12.2441618919, 'statistics': -12.367...</td>\n",
       "      <td>{'math': -9.318301487, 'statistics': -9.454425...</td>\n",
       "      <td>{'math': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "symfony cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>{'translation': 4, 'promoting': 1}</td>\n",
       "      <td>-11.824643</td>\n",
       "      <td>-8.885698</td>\n",
       "      <td>0.491427</td>\n",
       "      <td>-13.254419</td>\n",
       "      <td>-8.863644</td>\n",
       "      <td>[Purpose:to contribute to the transfer of know...</td>\n",
       "      <td>[[\\n,  objects,  and,  velocity,  toward], [ a...</td>\n",
       "      <td>[[0.0779624656, 0.06311330200000001, 0.0266567...</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Q:\\n\\nPassing a touch gesture\\n\\nI'm looking ...</td>\n",
       "      <td>[Purpose:to contribute to the transfer of know...</td>\n",
       "      <td>{'translation': -13.2544187546, 'promoting': -...</td>\n",
       "      <td>{'translation': -8.863643550900001, 'promoting...</td>\n",
       "      <td>{'translation': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "Pass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2002 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat_idx  feat_layer                                       explanations  \\\n",
       "230        114           4     {'time': 1, 'year': 1, 'dates': 2, 'years': 1}   \n",
       "1252       625           4                                     {'numbers': 5}   \n",
       "124         61           4  {'time, effort, energy, space, money': 1, 'tim...   \n",
       "944        471           4             {'code': 2, 'or': 2, 'programming': 1}   \n",
       "222        110           4  {'results': 1, 'including': 2, 'questions': 1,...   \n",
       "...        ...         ...                                                ...   \n",
       "1147       572           2  {'punctuation': 2, 'writing': 1, 'comma': 1, '...   \n",
       "856        427           4  {'nanofilms': 1, 'nanotechnology': 1, 'chemist...   \n",
       "1673       835           2    {'punctuation': 1, 'music': 3, 'supergroup': 1}   \n",
       "699        348           2         {'math': 3, 'statistics': 1, 'numbers': 1}   \n",
       "29          13           2                 {'translation': 4, 'promoting': 1}   \n",
       "\n",
       "      predictiveness_score  normalized_predictiveness_score  \\\n",
       "230              -4.859853                         7.479265   \n",
       "1252             -5.918956                         6.191844   \n",
       "124              -6.782176                         5.213839   \n",
       "944              -6.678521                         4.752080   \n",
       "222              -8.950260                         3.030504   \n",
       "...                    ...                              ...   \n",
       "1147            -10.720153                        -8.044349   \n",
       "856             -12.814263                        -8.293813   \n",
       "1673            -11.067059                        -8.317599   \n",
       "699             -10.709068                        -9.232220   \n",
       "29              -11.824643                        -8.885698   \n",
       "\n",
       "      predictiveness_score_stderr  max_predictiveness_score  \\\n",
       "230                      0.372199                 -3.140593   \n",
       "1252                     0.268001                 -4.886982   \n",
       "124                      0.256991                 -5.243265   \n",
       "944                      0.198007                 -5.159261   \n",
       "222                      0.336055                 -6.245436   \n",
       "...                           ...                       ...   \n",
       "1147                     0.421816                -11.897296   \n",
       "856                      0.524284                -14.102210   \n",
       "1673                     0.517542                -12.421036   \n",
       "699                      0.559356                -12.138460   \n",
       "29                       0.491427                -13.254419   \n",
       "\n",
       "      max_normalized_predictiveness_score  \\\n",
       "230                              8.166472   \n",
       "1252                             6.191844   \n",
       "124                              5.839702   \n",
       "944                              5.517492   \n",
       "222                              5.287371   \n",
       "...                                   ...   \n",
       "1147                            -8.014710   \n",
       "856                             -8.203892   \n",
       "1673                            -8.270683   \n",
       "699                             -8.751772   \n",
       "29                              -8.863644   \n",
       "\n",
       "                                      explainer_prompts  \\\n",
       "230   [\\n501 F.Supp. 158 (1980)\\nWilliam L. PALOMBI\\...   \n",
       "1252                                          [\", �, /]   \n",
       "124   [What people say about working at Apple\\n\\nGoo...   \n",
       "944   [Q, Q:\\n\\nC++ while loop with 2 conditions, VS...   \n",
       "222                                     [Q, Results, N]   \n",
       "...                                                 ...   \n",
       "1147  [Philtrum\\n\\nThe philtrum (,  philtron, lit. \"...   \n",
       "856   [Layer-by-layer films made from extracellular ...   \n",
       "1673  [Atsuko Enomoto\\n\\nis a Japanese voice actress...   \n",
       "699   [Announcing Reports\\n\\n, letters picked withou...   \n",
       "29    [Purpose:to contribute to the transfer of know...   \n",
       "\n",
       "                                   explainer_top_tokens  \\\n",
       "230   [[ 1978,  1974,  1976,  1971,  1984], [ 1998, ...   \n",
       "1252  [[6, 7, 8, 9, 3], [7, 6, 8, 9, 4], [6, 7, 8, 9...   \n",
       "124   [[ time,  money,  effort,  space,  energy], [ ...   \n",
       "944   [[ or, �, <|endoftext|>, �, <|padding|>], [ or...   \n",
       "222   [[ including, including,  assuming,  address, ...   \n",
       "...                                                 ...   \n",
       "1147  [[,,  it,  I,  we,  there], [,,  it,  however,...   \n",
       "856   [[\\n,  chemical,  a,  single,  operator], [ a,...   \n",
       "1673  [[ B,  and,  the,  K, ?\"], [ B, ?, ,,  and,  B...   \n",
       "699   [[import,  seriously,  offers, 's,  -], [-,  -...   \n",
       "29    [[\\n,  objects,  and,  velocity,  toward], [ a...   \n",
       "\n",
       "                              explainer_top_p_increases  \\\n",
       "230   [[0.0273308121, 0.0123613551, 0.00622998600000...   \n",
       "1252  [[0.232171759, 0.2214426398, 0.1743511707, 0.1...   \n",
       "124   [[0.7181766033, 0.1450483948, 0.0541959144, 0....   \n",
       "944   [[0.9998978376000001, 0.0, 0.0, 0.0, 0.0], [0....   \n",
       "222   [[0.9487411976000001, 0.0254448727, 0.00905056...   \n",
       "...                                                 ...   \n",
       "1147  [[0.8786817193, 0.0091542406, 0.0050156475, 0....   \n",
       "856   [[0.020971164100000002, 0.0185869616, 0.018565...   \n",
       "1673  [[0.0584790595, 0.031087832500000002, 0.023454...   \n",
       "699   [[0.3821032345, 0.1341270208, 0.05702698980000...   \n",
       "29    [[0.0779624656, 0.06311330200000001, 0.0266567...   \n",
       "\n",
       "     scorer_intervention_strengths  explainer_intervention_strength  \\\n",
       "230    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "1252   [0, 10, 32, 100, 320, 1000]                               32   \n",
       "124    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "944    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "222    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "...                            ...                              ...   \n",
       "1147   [0, 10, 32, 100, 320, 1000]                               32   \n",
       "856    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "1673   [0, 10, 32, 100, 320, 1000]                               32   \n",
       "699    [0, 10, 32, 100, 320, 1000]                               32   \n",
       "29     [0, 10, 32, 100, 320, 1000]                               32   \n",
       "\n",
       "                                           scorer_texts  \\\n",
       "230                               [Live, Thursday, The]   \n",
       "1252                                      [[, Theme, A]   \n",
       "124                                  [The, error, Long]   \n",
       "944                                  [Long, \", British]   \n",
       "222                                [Modern, Pop, Posts]   \n",
       "...                                                 ...   \n",
       "1147  [4,75 millions de barils de diesel par année, ...   \n",
       "856   [Q:\\n\\nhow to regex across line breaks\\n\\nI am...   \n",
       "1673  [---\\nabstract: |\\n    Given a flat injective ...   \n",
       "699   [Q:\\n\\nsymfony credentials issue with sfDoctri...   \n",
       "29    [Q:\\n\\nPassing a touch gesture\\n\\nI'm looking ...   \n",
       "\n",
       "                                        explainer_texts  \\\n",
       "230   [\\n501 F.Supp. 158 (1980)\\nWilliam L. PALOMBI\\...   \n",
       "1252                                          [\", �, /]   \n",
       "124   [What people say about working at Apple\\n\\nGoo...   \n",
       "944   [Q, Q:\\n\\nC++ while loop with 2 conditions, VS...   \n",
       "222                                     [Q, Results, N]   \n",
       "...                                                 ...   \n",
       "1147  [Philtrum\\n\\nThe philtrum (,  philtron, lit. \"...   \n",
       "856   [Layer-by-layer films made from extracellular ...   \n",
       "1673  [Atsuko Enomoto\\n\\nis a Japanese voice actress...   \n",
       "699   [Announcing Reports\\n\\n, letters picked withou...   \n",
       "29    [Purpose:to contribute to the transfer of know...   \n",
       "\n",
       "                    predictiveness_score_by_explanation  \\\n",
       "230   {'time': -3.6941101551, 'year': -3.2754569769,...   \n",
       "1252                         {'numbers': -4.8869820595}   \n",
       "124   {'time, effort, energy, space, money': -5.2540...   \n",
       "944   {'code': -6.3253965378, 'or': -5.1592611313, '...   \n",
       "222   {'results': -10.8864421844, 'including': -6.24...   \n",
       "...                                                 ...   \n",
       "1147  {'punctuation': -11.9374860764, 'writing': -12...   \n",
       "856   {'nanofilms': -14.1022098541, 'nanotechnology'...   \n",
       "1673  {'punctuation': -12.421035862, 'music': -12.43...   \n",
       "699   {'math': -12.2441618919, 'statistics': -12.367...   \n",
       "29    {'translation': -13.2544187546, 'promoting': -...   \n",
       "\n",
       "         normalized_predictiveness_score_by_explanation  \\\n",
       "230   {'time': 7.3446919917999995, 'year': 7.9918350...   \n",
       "1252                    {'numbers': 6.1918438910999996}   \n",
       "124   {'time, effort, energy, space, money': 5.83970...   \n",
       "944   {'code': 4.250957489, 'or': 5.5174919128, 'pro...   \n",
       "222   {'results': 0.4372577667, 'including': 5.28737...   \n",
       "...                                                 ...   \n",
       "1147  {'punctuation': -8.0158463955, 'writing': -8.0...   \n",
       "856   {'nanofilms': -8.2038920403, 'nanotechnology':...   \n",
       "1673  {'punctuation': -8.441075658799999, 'music': -...   \n",
       "699   {'math': -9.318301487, 'statistics': -9.454425...   \n",
       "29    {'translation': -8.863643550900001, 'promoting...   \n",
       "\n",
       "                                  scoring_interventions  \n",
       "230   {'time': {}, '0': [{'prompt': 'Live', 'top_tok...  \n",
       "1252  {'numbers': {}, '0': [{'prompt': '[', 'top_tok...  \n",
       "124   {'time, effort, energy, space, money': {}, '0'...  \n",
       "944   {'code': {}, '0': [{'prompt': 'Long', 'top_tok...  \n",
       "222   {'results': {}, '0': [{'prompt': 'Modern', 'to...  \n",
       "...                                                 ...  \n",
       "1147  {'punctuation': {}, '0': [{'prompt': '4,75 mil...  \n",
       "856   {'nanofilms': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "how to...  \n",
       "1673  {'punctuation': {}, '0': [{'prompt': '---\n",
       "abst...  \n",
       "699   {'math': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "symfony cre...  \n",
       "29    {'translation': {}, '0': [{'prompt': 'Q:\n",
       "\n",
       "Pass...  \n",
       "\n",
       "[2002 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.read_json(f\"counterfactual_results/pythia-70m-deduped_2layers_1001feats.json\", orient=\"records\")\n",
    "x = all_df.sort_values(\"max_normalized_predictiveness_score\", ascending=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/2002) \"years\" | layer 4, idx 114\n",
      "\texplanations: {'time': 1, 'year': 1, 'dates': 2, 'years': 1}\n",
      "\tnormalized predictiveness score: 7.5 ± 0.4\n",
      "\tmax normalized predictiveness score: 8.2 (expl=years)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>\\n501 F.Supp. 158 (1980)\\nWilliam L. PALOMBI\\nv.\\nGETTY OIL COMPANY t/a Getty.\\nCiv. A. No. 79-1103.\\nUnited States District Court, E. D. Pennsylvania.\\nOctober 27,</PROMPT>\n",
      "\t\tMost increased tokens: ' 1978' (+0.027), ' 1974' (+0.012), ' 1976' (+0.006), ' 1971' (+0.005), ' 1984' (+0.005)\n",
      "\n",
      "\t<PROMPT>error</PROMPT>\n",
      "\t\tMost increased tokens: ' 1998' (+0.196), ' 1995' (+0.113), ' 2005' (+0.099), ' 2009' (+0.064), ' 1968' (+0.062)\n",
      "\n",
      "\t<PROMPT>Posts</PROMPT>\n",
      "\t\tMost increased tokens: ' 2016' (+0.292), ' 2017' (+0.121), ' 2011' (+0.12), ' 2018' (+0.077), ' 2010' (+0.052)\n",
      "\n",
      "(2/2002) \"numbers\" | layer 4, idx 625\n",
      "\texplanations: {'numbers': 5}\n",
      "\tnormalized predictiveness score: 6.2 ± 0.3\n",
      "\tmax normalized predictiveness score: 6.2 (expl=numbers)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>\"</PROMPT>\n",
      "\t\tMost increased tokens: '6' (+0.232), '7' (+0.221), '8' (+0.174), '9' (+0.101), '3' (+0.073)\n",
      "\n",
      "\t<PROMPT>�</PROMPT>\n",
      "\t\tMost increased tokens: '7' (+0.377), '6' (+0.342), '8' (+0.142), '9' (+0.044), '4' (+0.034)\n",
      "\n",
      "\t<PROMPT>/</PROMPT>\n",
      "\t\tMost increased tokens: '6' (+0.286), '7' (+0.231), '8' (+0.162), '9' (+0.085), '3' (+0.067)\n",
      "\n",
      "(3/2002) \"time, effort, energy, space, money\" | layer 4, idx 61\n",
      "\texplanations: {'time, effort, energy, space, money': 1, 'time, money, effort, space, energy, information': 1, 'time, money, space, effort, energy': 1, 'time': 2}\n",
      "\tnormalized predictiveness score: 5.2 ± 0.3\n",
      "\tmax normalized predictiveness score: 5.8 (expl=time, effort, energy, space, money)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>What people say about working at Apple\\n\\nGood benefits, great environment\\n\\n5.0\\n\\nSomewhat halted progress, in career development, cloudy updates. Very high pressure on numbers, even when rationale is invalid.\\nCork is not big enough, but Apple makes it enough surprisingly</PROMPT>\n",
      "\t\tMost increased tokens: ' time' (+0.718), ' money' (+0.145), ' effort' (+0.054), ' space' (+0.047), ' energy' (+0.012)\n",
      "\n",
      "\t<PROMPT>Introduction {#Sec1}\\n============\\n\\nBirth weight (BW) has been associated with several adverse outcomes, both early and late in life. High BW has been associated with caesarean section, obesity and diabetes later in life^[@CR1],\\ [@CR2]^. BW is influenced by maternal dietary</PROMPT>\n",
      "\t\tMost increased tokens: ' time' (+0.582), ' energy' (+0.167), ' information' (+0.062), ' space' (+0.056), ' effort' (+0.053)\n",
      "\n",
      "\t<PROMPT>/*</PROMPT>\n",
      "\t\tMost increased tokens: ' time' (+0.872), ' money' (+0.086), ' space' (+0.025), ' information' (+0.008), ' attention' (+0.002)\n",
      "\n",
      "(4/2002) \"or\" | layer 4, idx 471\n",
      "\texplanations: {'code': 2, 'or': 2, 'programming': 1}\n",
      "\tnormalized predictiveness score: 4.8 ± 0.2\n",
      "\tmax normalized predictiveness score: 5.5 (expl=or)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Q</PROMPT>\n",
      "\t\tMost increased tokens: ' or' (+1.0), '�' (+0.0), '<|endoftext|>' (+0.0), '�' (+0.0), '<|padding|>' (+0.0)\n",
      "\n",
      "\t<PROMPT>Q:\\n\\nC++ while loop with 2 conditions, VS. for loop with 2 conditions?\\n\\nIf I wanted to iterate through a string, and stop once I found either the letter</PROMPT>\n",
      "\t\tMost increased tokens: ' or' (+0.85), '�' (+0.0), '<|endoftext|>' (+0.0), '�' (+0.0), '<|padding|>' (+0.0)\n",
      "\n",
      "\t<PROMPT>Q:\\n\\nRuby on Rails: trying to create a new object that belongs to a user... not sure how to</PROMPT>\n",
      "\t\tMost increased tokens: ' or' (+1.0), ' nor' (+0.0), '�' (+0.0), '<|padding|>' (+0.0), '<|endoftext|>' (+0.0)\n",
      "\n",
      "(5/2002) \"including\" | layer 4, idx 110\n",
      "\texplanations: {'results': 1, 'including': 2, 'questions': 1, 'include': 1}\n",
      "\tnormalized predictiveness score: 3.0 ± 0.3\n",
      "\tmax normalized predictiveness score: 5.3 (expl=including)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Q</PROMPT>\n",
      "\t\tMost increased tokens: ' including' (+0.949), 'including' (+0.025), ' assuming' (+0.009), ' address' (+0.005), 'INCLUDING' (+0.002)\n",
      "\n",
      "\t<PROMPT>Results</PROMPT>\n",
      "\t\tMost increased tokens: ' including' (+0.999), ' address' (+0.0), ' assuming' (+0.0), 'including' (+0.0), ' courtesy' (+0.0)\n",
      "\n",
      "\t<PROMPT>N</PROMPT>\n",
      "\t\tMost increased tokens: ' including' (+0.961), 'including' (+0.026), ' assuming' (+0.005), 'INCLUDING' (+0.002), ' address' (+0.001)\n",
      "\n",
      "(10/2002) \"government\" | layer 4, idx 363\n",
      "\texplanations: {'law enforcement': 1, 'law': 1, 'government': 3}\n",
      "\tnormalized predictiveness score: 4.1 ± 0.4\n",
      "\tmax normalized predictiveness score: 4.9 (expl=government)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Local</PROMPT>\n",
      "\t\tMost increased tokens: ' government' (+0.49), ' law' (+0.25), 'ized' (+0.121), 'ist' (+0.054), ' authorities' (+0.022)\n",
      "\n",
      "\t<PROMPT>In</PROMPT>\n",
      "\t\tMost increased tokens: ' law' (+0.705), ' government' (+0.278), ' agencies' (+0.008), ' agency' (+0.001), ' courts' (+0.001)\n",
      "\n",
      "\t<PROMPT>100</PROMPT>\n",
      "\t\tMost increased tokens: ' law' (+0.5), ' government' (+0.414), ' agencies' (+0.019), ' authorities' (+0.014), 'ist' (+0.01)\n",
      "\n",
      "(50/2002) \"morning\" | layer 2, idx 435\n",
      "\texplanations: {'mornings': 1, 'morning': 4}\n",
      "\tnormalized predictiveness score: 3.3 ± 0.3\n",
      "\tmax normalized predictiveness score: 3.4 (expl=morning)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>letters</PROMPT>\n",
      "\t\tMost increased tokens: ' morning' (+0.774), ' comments' (+0.009), ' space' (+0.008), 'ize' (+0.007), ' flags' (+0.006)\n",
      "\n",
      "\t<PROMPT>T</PROMPT>\n",
      "\t\tMost increased tokens: ' morning' (+0.495), 'UST' (+0.041), 'Y' (+0.034), 'BF' (+0.029), 'FU' (+0.023)\n",
      "\n",
      "\t<PROMPT>Pre</PROMPT>\n",
      "\t\tMost increased tokens: ' morning' (+0.947), 'AST' (+0.006), 'language' (+0.006), 'izers' (+0.004), 'izing' (+0.002)\n",
      "\n",
      "(100/2002) \"verbs\" | layer 4, idx 812\n",
      "\texplanations: {'language': 1, 'update': 2, 'editing': 1, 'verbs': 1}\n",
      "\tnormalized predictiveness score: 2.2 ± 0.2\n",
      "\tmax normalized predictiveness score: 2.6 (expl=verbs)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Update</PROMPT>\n",
      "\t\tMost increased tokens: 'ed' (+0.89), 'Jones' (+0.018), ' (@' (+0.006), ' :=' (+0.004), ' {' (+0.003)\n",
      "\n",
      "\t<PROMPT>Gen</PROMPT>\n",
      "\t\tMost increased tokens: 'ed' (+0.94), 'edic' (+0.008), 'Jones' (+0.008), 'ically' (+0.006), 'LY' (+0.004)\n",
      "\n",
      "\t<PROMPT>'</PROMPT>\n",
      "\t\tMost increased tokens: 'ed' (+0.941), 'Jones' (+0.017), 'LY' (+0.009), ' thought' (+0.003), 'mente' (+0.001)\n",
      "\n",
      "(500/2002) \"colorful\" | layer 2, idx 499\n",
      "\texplanations: {'not': 1, 'colors': 1, 'color': 2, 'colorful': 1}\n",
      "\tnormalized predictiveness score: -0.8 ± 0.2\n",
      "\tmax normalized predictiveness score: -0.0 (expl=colorful)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Spec</PROMPT>\n",
      "\t\tMost increased tokens: 'icolor' (+0.664), 'ially' (+0.107), 'aken' (+0.039), 'ien' (+0.038), 'odia' (+0.027)\n",
      "\n",
      "\t<PROMPT>Things</PROMPT>\n",
      "\t\tMost increased tokens: ' some' (+0.094), 'NOT' (+0.052), 'aken' (+0.037), ' NOT' (+0.033), ' lost' (+0.031)\n",
      "\n",
      "\t<PROMPT>{</PROMPT>\n",
      "\t\tMost increased tokens: 'NOT' (+0.305), 'aken' (+0.293), 'icolor' (+0.128), 'sc' (+0.043), 'DI' (+0.021)\n",
      "\n",
      "(1000/2002) \"rea\" | layer 4, idx 500\n",
      "\texplanations: {'reading': 1, 'rearing': 1, 'real': 1, 'reproduction': 1, 'rea': 1}\n",
      "\tnormalized predictiveness score: -1.7 ± 0.1\n",
      "\tmax normalized predictiveness score: -1.1 (expl=rea)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>One</PROMPT>\n",
      "\t\tMost increased tokens: 'rea' (+0.668), 'rei' (+0.144), 'rogen' (+0.076), 're' (+0.031), 'ers' (+0.024)\n",
      "\n",
      "\t<PROMPT>In</PROMPT>\n",
      "\t\tMost increased tokens: 'rea' (+0.594), 'rogen' (+0.183), 'rei' (+0.158), 're' (+0.03), 'rogenic' (+0.011)\n",
      "\n",
      "\t<PROMPT>Let</PROMPT>\n",
      "\t\tMost increased tokens: 'rea' (+0.711), 'rogen' (+0.106), 'rei' (+0.045), 'ro' (+0.026), 're' (+0.023)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for a given index i, print out\n",
    "\"\"\"\n",
    "feat idx and layer\n",
    "explanations\n",
    "normalized predictiveness score\n",
    "the best explanation\n",
    "max normalized predictiveness score\n",
    "the three explainer intervention examples\n",
    "\"\"\"\n",
    "for i in list(range(5)) + [9, 49, 99, 499, 999]:\n",
    "\n",
    "    row = x.iloc[i]\n",
    "    best_expl = max(row.normalized_predictiveness_score_by_explanation, key=row.normalized_predictiveness_score_by_explanation.get)\n",
    "    print(f\"({i + 1}/{len(x)}) \\\"{best_expl}\\\" | layer {row.feat_layer}, idx {row.feat_idx}\")\n",
    "    print(f\"\\texplanations: {row.explanations}\")\n",
    "    print(f\"\\tnormalized predictiveness score: {row.normalized_predictiveness_score:.1f} ± {row.predictiveness_score_stderr:.1f}\")\n",
    "    print(f\"\\tmax normalized predictiveness score: {row.max_normalized_predictiveness_score:.1f} (expl={best_expl})\")\n",
    "    print(\"\\n\\tExample counterfactuals (intervention strength = 32):\")\n",
    "    for pr, tts, tpis in zip(row[\"explainer_prompts\"], row[\"explainer_top_tokens\"], row[\"explainer_top_p_increases\"]):\n",
    "        ex = ExplainerInterventionExample(\n",
    "                    prompt=pr,\n",
    "                    top_tokens=tts,\n",
    "                    top_p_increases=tpis\n",
    "                )\n",
    "        print(\"\\t\" + ex.text().replace(\"\\nMost\", \"\\n\\t\\tMost\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What people say about working at Apple\\\\n\\\\nGood benefits, great environment\\\\n\\\\n5.0\\\\n\\\\nSomewhat halted progress, in career development, cloudy updates. Very high pressure on numbers, even when rationale is invalid.\\\\nCork is not big enough, but Apple makes it enough surprisingly',\n",
       " 'Introduction {#Sec1}\\\\n============\\\\n\\\\nBirth weight (BW) has been associated with several adverse outcomes, both early and late in life. High BW has been associated with caesarean section, obesity and diabetes later in life^[@CR1],\\\\ [@CR2]^. BW is influenced by maternal dietary',\n",
       " '/*']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.iloc[i].explainer_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' time', ' money', ' effort', ' space', ' energy'],\n",
       " [' time', ' energy', ' information', ' space', ' effort'],\n",
       " [' time', ' money', ' space', ' information', ' attention']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.iloc[i].explainer_top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time, effort, energy, space, money': 5.8397022247,\n",
       " 'time, money, effort, space, energy, information': 5.4518841743,\n",
       " 'time, money, space, effort, energy': 5.8395374298,\n",
       " 'time': 4.4690350533}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.iloc[i].normalized_predictiveness_score_by_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "key = \"max_normalized_predictiveness_score\"\n",
    "plt.hist(all_df[key].values)\n",
    "plt.xlabel(key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
