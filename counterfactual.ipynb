{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject model\n",
    "# load SAEs without attaching them to the model\n",
    "# for now just use the Islam feature and explanation\n",
    "# load a scorer. The prompt should have the input as well this time\n",
    "# (for now) on random pretraining data, evaluate gpt2 with a hook that \n",
    "# adds a multiple of the Islam feature to the appropriate residual stream layer and position\n",
    "# Get the pre- and post-intervention output distributions of gpt2\n",
    "# (TODO: check if all the Islam features just have similar embeddings)\n",
    "# Show this to the scorer and get a score (scorer should be able to have a good prior without being given the clean output distribution)\n",
    "# Also get a simplicity score for the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_correlation_score</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature0</th>\n",
       "      <td>0.970093</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature19</th>\n",
       "      <td>0.966378</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature0</th>\n",
       "      <td>0.952401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature4</th>\n",
       "      <td>0.952061</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature5</th>\n",
       "      <td>0.949993</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature4</th>\n",
       "      <td>0.941871</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature11</th>\n",
       "      <td>0.930066</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature19</th>\n",
       "      <td>0.918787</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature14</th>\n",
       "      <td>0.906342</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature3</th>\n",
       "      <td>0.897080</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature11</th>\n",
       "      <td>0.883083</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature9</th>\n",
       "      <td>0.868529</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature3</th>\n",
       "      <td>0.810241</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature18</th>\n",
       "      <td>0.805457</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature16</th>\n",
       "      <td>0.782019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature11</th>\n",
       "      <td>0.779133</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature10</th>\n",
       "      <td>0.772255</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature7</th>\n",
       "      <td>0.764754</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature7</th>\n",
       "      <td>0.712047</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature12</th>\n",
       "      <td>0.711850</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature0</th>\n",
       "      <td>0.698656</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature12</th>\n",
       "      <td>0.678797</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature15</th>\n",
       "      <td>0.668821</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature13</th>\n",
       "      <td>0.668621</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature1</th>\n",
       "      <td>0.662348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature2</th>\n",
       "      <td>0.659443</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature13</th>\n",
       "      <td>0.649270</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature3</th>\n",
       "      <td>0.645487</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature6</th>\n",
       "      <td>0.643440</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature17</th>\n",
       "      <td>0.627347</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature8</th>\n",
       "      <td>0.583940</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature15</th>\n",
       "      <td>0.556673</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature4</th>\n",
       "      <td>0.470279</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature6</th>\n",
       "      <td>0.447680</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature7</th>\n",
       "      <td>0.390438</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature18</th>\n",
       "      <td>0.378467</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature14</th>\n",
       "      <td>0.316718</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature9</th>\n",
       "      <td>0.272719</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature10</th>\n",
       "      <td>0.228020</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature13</th>\n",
       "      <td>0.167355</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature5</th>\n",
       "      <td>0.095751</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature2</th>\n",
       "      <td>0.023048</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature15</th>\n",
       "      <td>-0.060541</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ev_correlation_score  layer  feature\n",
       ".transformer.h.2_feature0               0.970093      2        0\n",
       ".transformer.h.2_feature19              0.966378      2       19\n",
       ".transformer.h.0_feature0               0.952401      0        0\n",
       ".transformer.h.4_feature4               0.952061      4        4\n",
       ".transformer.h.0_feature5               0.949993      0        5\n",
       ".transformer.h.2_feature4               0.941871      2        4\n",
       ".transformer.h.2_feature11              0.930066      2       11\n",
       ".transformer.h.4_feature19              0.918787      4       19\n",
       ".transformer.h.0_feature14              0.906342      0       14\n",
       ".transformer.h.0_feature3               0.897080      0        3\n",
       ".transformer.h.0_feature11              0.883083      0       11\n",
       ".transformer.h.0_feature9               0.868529      0        9\n",
       ".transformer.h.2_feature3               0.810241      2        3\n",
       ".transformer.h.2_feature18              0.805457      2       18\n",
       ".transformer.h.0_feature16              0.782019      0       16\n",
       ".transformer.h.4_feature11              0.779133      4       11\n",
       ".transformer.h.0_feature10              0.772255      0       10\n",
       ".transformer.h.0_feature7               0.764754      0        7\n",
       ".transformer.h.2_feature7               0.712047      2        7\n",
       ".transformer.h.0_feature12              0.711850      0       12\n",
       ".transformer.h.4_feature0               0.698656      4        0\n",
       ".transformer.h.4_feature12              0.678797      4       12\n",
       ".transformer.h.4_feature15              0.668821      4       15\n",
       ".transformer.h.4_feature13              0.668621      4       13\n",
       ".transformer.h.2_feature1               0.662348      2        1\n",
       ".transformer.h.2_feature2               0.659443      2        2\n",
       ".transformer.h.0_feature13              0.649270      0       13\n",
       ".transformer.h.4_feature3               0.645487      4        3\n",
       ".transformer.h.2_feature6               0.643440      2        6\n",
       ".transformer.h.4_feature17              0.627347      4       17\n",
       ".transformer.h.4_feature8               0.583940      4        8\n",
       ".transformer.h.2_feature15              0.556673      2       15\n",
       ".transformer.h.0_feature4               0.470279      0        4\n",
       ".transformer.h.0_feature6               0.447680      0        6\n",
       ".transformer.h.4_feature7               0.390438      4        7\n",
       ".transformer.h.0_feature18              0.378467      0       18\n",
       ".transformer.h.4_feature14              0.316718      4       14\n",
       ".transformer.h.4_feature9               0.272719      4        9\n",
       ".transformer.h.4_feature10              0.228020      4       10\n",
       ".transformer.h.2_feature13              0.167355      2       13\n",
       ".transformer.h.2_feature5               0.095751      2        5\n",
       ".transformer.h.4_feature2               0.023048      4        2\n",
       ".transformer.h.0_feature15             -0.060541      0       15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "results_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/results/gpt2_simulation/all_at_once\"\n",
    "results = dict()\n",
    "for fname in Path(results_dir).iterdir():\n",
    "    with open(fname, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "    last = fname.stem.split(\".\")[-1]\n",
    "    layer = int(last.split(\"_\")[0])\n",
    "    feat = int(last[last.index(\"_feature\") + len(\"_feature\"):])\n",
    "    results[fname.stem] = {\"ev_correlation_score\": r[\"ev_correlation_score\"], \"layer\": layer, \"feature\": feat}\n",
    "input_scores_df = pd.DataFrame(results).T\n",
    "input_scores_df[\"layer\"] = input_scores_df[\"layer\"].astype(int)\n",
    "input_scores_df[\"feature\"] = input_scores_df[\"feature\"].astype(int)\n",
    "input_scores_df = input_scores_df.sort_values(\"ev_correlation_score\", ascending=False)\n",
    "unq_layers = input_scores_df[\"layer\"].unique()\n",
    "input_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"pile.jsonl\", \"r\") as f:\n",
    "    pile = random.sample([json.loads(line) for line in f.readlines()], 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/autointerp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alex/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "subject_device = \"cuda:0\"\n",
    "\n",
    "subject_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "subject = AutoModelForCausalLM.from_pretrained(subject_name).to(subject_device)\n",
    "subject_tokenizer = AutoTokenizer.from_pretrained(subject_name)\n",
    "subject_tokenizer.pad_token = subject_tokenizer.eos_token\n",
    "subject.config.pad_token_id = subject_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "scorer_device = \"cuda:1\"\n",
    "scorer_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "scorer = AutoModelForCausalLM.from_pretrained(\n",
    "    scorer_name,\n",
    "    device_map={\"\": scorer_device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    ")\n",
    "scorer_tokenizer = AutoTokenizer.from_pretrained(scorer_name)\n",
    "scorer_tokenizer.pad_token = scorer_tokenizer.eos_token\n",
    "scorer.config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "scorer.generation_config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "\n",
    "# explainer is the same model as the scorer\n",
    "explainer_device = scorer_device\n",
    "explainer = scorer\n",
    "explainer_tokenizer = scorer_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\n",
      "\n",
      "For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\n",
      "\n",
      "The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\n",
      "\n",
      "Neuron 1\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation: fruits\n",
      "\n",
      "Neuron 2\n",
      "<PROMPT>Once</PROMPT>\n",
      "Most increased tokens: ' upon' (+0.22), ' in' (+0.2), ' a' (+0.05), ' long' (+0.04)\n",
      "\n",
      "<PROMPT>Ryan Quarles\\n\\nRyan Francis Quarles (born October 20, 1983)</PROMPT>\n",
      "Most increased tokens: ' once' (+0.03), ' happily' (+0.31), ' for' (+0.01)\n",
      "\n",
      "<PROMPT>MSI Going Full Throttle @ CeBIT</PROMPT>\n",
      "Most increased tokens: ' Once' (+0.02), ' once' (+0.01), ' in' (+0.01), ' the' (+0.01), ' a' (+0.01), ' The' (+0.01)\n",
      "\n",
      "Explanation: storytelling\n",
      "\n",
      "Neuron 3\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation:\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "@dataclass\n",
    "class ExplainerInterventionExample:\n",
    "    prompt: str\n",
    "    top_tokens: list[str]\n",
    "    top_p_increases: list[float]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.prompt = self.prompt.replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "    def text(self) -> str:\n",
    "        tokens_str = \", \".join(f\"'{tok}' (+{round(p, 3)})\" for tok, p in zip(self.top_tokens, self.top_p_increases))\n",
    "        return f\"<PROMPT>{self.prompt}</PROMPT>\\nMost increased tokens: {tokens_str}\"\n",
    "    \n",
    "@dataclass\n",
    "class ExplainerNeuronFormatter:\n",
    "    intervention_examples: list[ExplainerInterventionExample]\n",
    "    explanation: str | None = None\n",
    "\n",
    "    def text(self) -> str:\n",
    "        text = \"\\n\\n\".join(example.text() for example in self.intervention_examples)\n",
    "        text += \"\\n\\nExplanation:\"\n",
    "        if self.explanation is not None:\n",
    "            text += \" \" + self.explanation\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_explainer_prompt(neuron_prompter: ExplainerNeuronFormatter, few_shot_examples: list[ExplainerNeuronFormatter] | None = None) -> str:\n",
    "    prompt = \"We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\\n\\n\" \\\n",
    "        \"For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\\n\\n\" \\\n",
    "        \"The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\\n\\n\"\n",
    "    \n",
    "    i = 1\n",
    "    for few_shot_example in few_shot_examples or []:\n",
    "        assert few_shot_example.explanation is not None\n",
    "        prompt += f\"Neuron {i}\\n\" + few_shot_example.text() + \"\\n\\n\"\n",
    "        i += 1\n",
    "\n",
    "    prompt += f\"Neuron {i}\\n\"\n",
    "    prompt += neuron_prompter.text()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "fs_examples = [\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"My favorite food is\",\n",
    "                top_tokens=[\" oranges\", \" bananas\", \" apples\"],\n",
    "                top_p_increases=[0.81, 0.09, 0.02]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Whenever I would see\",\n",
    "                top_tokens=[\" fruit\", \" a\", \" apples\", \" red\"],\n",
    "                top_p_increases=[0.09, 0.06, 0.06, 0.05]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"I like to eat\",\n",
    "                top_tokens=[\" fro\", \" fruit\", \" oranges\", \" bananas\", \" strawberries\"],\n",
    "                top_p_increases=[0.14, 0.13, 0.11, 0.10, 0.03]\n",
    "            )\n",
    "        ],\n",
    "        explanation=\"fruits\"\n",
    "    ),\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Once\",\n",
    "                top_tokens=[\" upon\", \" in\", \" a\", \" long\"],\n",
    "                top_p_increases=[0.22, 0.2, 0.05, 0.04]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Ryan Quarles\\\\n\\\\nRyan Francis Quarles (born October 20, 1983)\",\n",
    "                top_tokens=[\" once\", \" happily\", \" for\"],\n",
    "                top_p_increases=[0.03, 0.31, 0.01]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"MSI Going Full Throttle @ CeBIT\",\n",
    "                top_tokens=[\" Once\", \" once\", \" in\", \" the\", \" a\", \" The\"],\n",
    "                top_p_increases=[0.02, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "            ),\n",
    "        ],\n",
    "        explanation=\"storytelling\"\n",
    "    ),\n",
    "    # ExplainerNeuronFormatter(\n",
    "    #     intervention_examples=[\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"He owned the watch for a long time. While he never said it was\",\n",
    "    #             top_tokens=[\" hers\", \" hers\", \" hers\"],\n",
    "    #             top_p_increases=[0.09, 0.06, 0.06, 0.5]\n",
    "    #         ),\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"For some reason\",\n",
    "    #             top_tokens=[\" she\", \" her\", \" hers\"],\n",
    "    #             top_p_increases=[0.14, 0.01, 0.01]\n",
    "    #         ),\n",
    "    #         ExplainerInterventionExample(\n",
    "    #             prompt=\"insurance does not cover\",\n",
    "    #             top_tokens=[\" her\", \" women\", \" her's\"],\n",
    "    #             top_p_increases=[0.10, 0.02, 0.01]\n",
    "    #         )\n",
    "    #     ],\n",
    "    #     explanation=\"she/her pronouns\"\n",
    "    # )\n",
    "]\n",
    "\n",
    "neuron_prompter = copy.deepcopy(fs_examples[0])\n",
    "neuron_prompter.explanation = None\n",
    "print(get_explainer_prompt(neuron_prompter, fs_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT> oranges\n",
      "\n",
      "Explanation: ateg\n",
      "<PROMPT>From west to east, the westmost of the seven</PROMPT>WAY\n",
      "\n",
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT>\n"
     ]
    }
   ],
   "source": [
    "def get_scorer_simplicity_prompt(explanation):\n",
    "    prefix = \"Explanation\\n\\n\"\n",
    "    return f\"{prefix}{explanation}{scorer_tokenizer.eos_token}\", prefix\n",
    "\n",
    "def get_scorer_predictiveness_prompt(prompt, explanation, few_shot_prompts=None, few_shot_explanations=None, few_shot_tokens=None):\n",
    "    if few_shot_explanations is not None:\n",
    "        assert few_shot_tokens is not None and few_shot_prompts is not None\n",
    "        assert len(few_shot_explanations) == len(few_shot_tokens) == len(few_shot_prompts)\n",
    "        few_shot_prompt = \"\\n\\n\".join(get_scorer_predictiveness_prompt(pr, expl) + token for pr, expl, token in zip(few_shot_prompts, few_shot_explanations, few_shot_tokens)) + \"\\n\\n\"\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    return few_shot_prompt + f\"Explanation: {explanation}\\n<PROMPT>{prompt}</PROMPT>\"\n",
    "\n",
    "few_shot_prompts = [\"My favorite food is\", \"From west to east, the westmost of the seven\"]\n",
    "few_shot_explanations = [\"fruits and vegetables\", \"ateg\"]\n",
    "few_shot_tokens = [\" oranges\", \"WAY\"]\n",
    "# few_shot_prompts = [\"My favorite food is\", \"From west to east, the westmost of the seven\", \"He owned the watch for a long time. While he never said it was\"]\n",
    "# few_shot_explanations = [\"fruits and vegetables\", \"ateg\", \"she/her pronouns\"]\n",
    "# few_shot_tokens = [\" oranges\", \"WAY\", \" hers\"]\n",
    "print(get_scorer_predictiveness_prompt(few_shot_prompts[0], few_shot_explanations[0], few_shot_prompts, few_shot_explanations, few_shot_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def intervene(module, input, output, intervention_strength=10.0, position=-1, feat=None):\n",
    "    hiddens = output[0]  # the later elements of the tuple are the key value cache\n",
    "    hiddens[:, position, :] += intervention_strength * feat.to(hiddens.device)\n",
    "\n",
    "def get_texts(n, seed=42, randomize_length=True):\n",
    "    random.seed(seed)\n",
    "    texts = []\n",
    "    for _ in range(n):\n",
    "        \n",
    "        # sample a random text from the pile, and stop it at a random token position, less than 64 tokens\n",
    "        text = random.choice(pile)[\"text\"]\n",
    "        tokenized_text = subject_tokenizer.encode(text, add_special_tokens=False, max_length=64, truncation=True)\n",
    "        if len(tokenized_text) < 1:\n",
    "            continue\n",
    "        if randomize_length:\n",
    "            stop_pos = random.randint(1, min(len(tokenized_text), 63))\n",
    "        else:\n",
    "            stop_pos = 63\n",
    "        text = subject_tokenizer.decode(tokenized_text[:stop_pos])\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "n_explainer_texts = 3\n",
    "n_scorer_texts = 3\n",
    "n_explanations = 5\n",
    "# explainer_texts = get_texts(n_explainer_texts)\n",
    "# explainer_texts = [\"Current religion:\", \"A country that is\", \"Many people believe that\"]\n",
    "# scorer_texts = get_texts(n_scorer_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No scorer token found for '   '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '   '\n",
      "No scorer token found for '     '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '     '\n",
      "No scorer token found for '                     '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                     '\n",
      "No scorer token found for '             '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '             '\n",
      "No scorer token found for '                      '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                      '\n",
      "No scorer token found for '                '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                '\n",
      "No scorer token found for '                       '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                       '\n",
      "No scorer token found for '       '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '       '\n",
      "No scorer token found for '      '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '      '\n",
      "No scorer token found for '              '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '              '\n",
      "No scorer token found for '                 '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                 '\n",
      "No scorer token found for '  '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '  '\n",
      "No scorer token found for '           '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '           '\n",
      "No scorer token found for '          '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '          '\n",
      "No scorer token found for '                        '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                        '\n",
      "No scorer token found for '            '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '            '\n",
      "No scorer token found for '                  '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                  '\n",
      "No scorer token found for '               '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '               '\n",
      "No scorer token found for '         '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '         '\n",
      "No scorer token found for '                    '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                    '\n",
      "No scorer token found for '    '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '    '\n",
      "No scorer token found for '        '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '        '\n",
      "No scorer token found for '                   '\n",
      "Using '<|reserved_special_token_245|>' as a placeholder for '                   '\n"
     ]
    }
   ],
   "source": [
    "scorer_vocab = scorer_tokenizer.get_vocab()\n",
    "subject_vocab = subject_tokenizer.get_vocab()\n",
    "\n",
    "# Pre-compute the mapping of subject tokens to scorer tokens\n",
    "subject_to_scorer = {}\n",
    "text_subject_to_scorer = {}\n",
    "for subj_tok, subj_id in subject_vocab.items():\n",
    "    if subj_tok in scorer_vocab:\n",
    "        subject_to_scorer[subj_id] = scorer_vocab[subj_tok]\n",
    "        text_subject_to_scorer[subj_tok] = subj_tok\n",
    "    else:\n",
    "        for i in range(len(subj_tok) - 1, 0, -1):\n",
    "            if subj_tok[:i] in scorer_vocab:\n",
    "                subject_to_scorer[subj_id] = scorer_vocab[subj_tok[:i]]\n",
    "                text_subject_to_scorer[subj_tok] = subj_tok[:i]\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No scorer token found for '{subj_tok}'\")\n",
    "            subject_to_scorer[subj_id] = len(scorer_vocab) - 3  # some very rare token\n",
    "            print(f\"Using '{scorer_tokenizer.decode([len(scorer_vocab) - 3])}' as a placeholder for '{subj_tok}'\")\n",
    "subject_ids = torch.tensor(list(subject_to_scorer.keys()), device=scorer_device)\n",
    "scorer_ids = torch.tensor(list(subject_to_scorer.values()), device=scorer_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_intervention_examples = 5\n",
    "n_candidate_texts = 500\n",
    "candidate_texts = get_texts(n_candidate_texts, randomize_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 91.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 5.49 seconds\n",
      "Explainer took 1.59 seconds\n",
      "[' her', ' she', ' herself', ' She', ' hers']\n",
      "[0.9169365763664246, 0.06864659488201141, 0.014236417599022388, 8.034688653424382e-05, 2.3430850148997706e-08]\n",
      "[' her', ' she', ' herself', '<|padding|>', '<|endoftext|>']\n",
      "[0.9081762433052063, 0.0903496965765953, 0.001298226648941636, 0.0, 0.0]\n",
      "[' her', ' she', ' herself', ' She', ' hers']\n",
      "[0.8244187235832214, 0.15637685358524323, 0.01874895766377449, 0.0002570784417912364, 3.008909033042073e-08]\n",
      "Counter({'female pronouns': 3, 'female': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.12it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.49it/s]\n",
      "  0%|          | 1/202 [00:09<33:19,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 2.80 seconds\n",
      "predictiveness_score=-6.606029033660889\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 129.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 3.86 seconds\n",
      "Explainer took 1.39 seconds\n",
      "['ing', 'lying', 's', 'urch', 'fully']\n",
      "[0.12137379497289658, 0.09610537439584732, 0.07358910143375397, 0.030431075021624565, 0.026000313460826874]\n",
      "['s', 'rote', 'f', 'n', 'urch']\n",
      "[0.11145196110010147, 0.09851589053869247, 0.07393316924571991, 0.04224688559770584, 0.023582376539707184]\n",
      "['s', 'und', 'ing', 'telling', ' Frost']\n",
      "[0.14011235535144806, 0.030827190726995468, 0.026327718049287796, 0.023694975301623344, 0.02085311897099018]\n",
      "Counter({'storytelling': 2, 'telling': 1, 'adjectives': 1, 'ending': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.54it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.53it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "  1%|          | 2/202 [00:22<37:34, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.81 seconds\n",
      "predictiveness_score=-11.82196922302246\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 85.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 5.87 seconds\n",
      "Explainer took 1.13 seconds\n",
      "['@', 'ibrary', '�', '<|padding|>', '<|endoftext|>']\n",
      "[0.9998772144317627, 1.4673590120750646e-10, 0.0, 0.0, 0.0]\n",
      "['@', '�', '<|endoftext|>', '�', '<|padding|>']\n",
      "[0.9986705183982849, 0.0, 0.0, 0.0, 0.0]\n",
      "['@', '�', '<|endoftext|>', '�', '<|padding|>']\n",
      "[0.9998978972434998, 0.0, 0.0, 0.0, 0.0]\n",
      "Counter({'email': 2, 'emails': 1, 'Twitter handles': 1, 'twitter': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.93it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.49it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.52it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.55it/s]\n",
      "  1%|▏         | 3/202 [00:35<41:07, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.66 seconds\n",
      "predictiveness_score=-9.237468910217284\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 132.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection took 3.79 seconds\n",
      "Explainer took 1.57 seconds\n",
      "['essee', 'king', ' time', 'ns', ' settlement']\n",
      "[0.6908113360404968, 0.14241111278533936, 0.038456305861473083, 0.01946074701845646, 0.012874310836195946]\n",
      "['king', 'ding', 'ky', 'ks', 's']\n",
      "[0.8632607460021973, 0.04527141526341438, 0.007642056792974472, 0.007184164598584175, 0.0057197813875973225]\n",
      "['king', ' time', 'name', ' name', ' Malaysia']\n",
      "[0.8412436842918396, 0.01505723875015974, 0.014293982647359371, 0.010964670218527317, 0.006286932621151209]\n",
      "Counter({'time': 2, 'names': 1, 'king': 1, 'the number 10': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.59it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.20it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.95it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.03it/s]\n",
      "  2%|▏         | 4/202 [00:47<39:48, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring took 6.12 seconds\n",
      "predictiveness_score=-10.92839698791504\n",
      "\n",
      "\n",
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "from sae_auto_interp.autoencoders.OpenAI.model import Autoencoder\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    subject_layers = subject.transformer.h\n",
    "except:\n",
    "    subject_layers = subject.gpt_neox.layers\n",
    "\n",
    "def get_encoder_decoder_weights(feat_idx, feat_layer, device):\n",
    "    # weight_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/weights/gpt2_128k\"\n",
    "    # path = f\"{weight_dir}/{feat_layer}.pt\"\n",
    "    # state_dict = torch.load(path)\n",
    "    # ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "    # decoder_feat = ae.decoder.weight[:, feat_idx].to(device)\n",
    "    # encoder_feat = ae.encoder.weight[feat_idx, :].to(device)\n",
    "    weight_dir = f\"/mnt/ssd-1/alexm/dictionary_learning/dictionaries/pythia-70m-deduped/resid_out_layer{feat_layer}/10_32768/ae.pt\"\n",
    "    state_dict = torch.load(weight_dir)\n",
    "    encoder_feat = state_dict['encoder.weight'][feat_idx, :]\n",
    "    decoder_feat = state_dict['decoder.weight'][:, feat_idx]\n",
    "    return encoder_feat, decoder_feat\n",
    "\n",
    "all_results = []\n",
    "\n",
    "feat_idxs = [12420] + list(range(100))\n",
    "feat_layers = [4, 2]\n",
    "total_iterations = len(feat_idxs) * len(feat_layers)\n",
    "for feat_idx, feat_layer in tqdm(product(feat_idxs, feat_layers), total=total_iterations):\n",
    "    scorer_intervention_strengths = [0, 10, 32, 100, 320, 1000]\n",
    "    explainer_intervention_strength = 32\n",
    "\n",
    "    print(\"Loading autoencoder...\", end=\"\")\n",
    "    encoder_feat, decoder_feat = get_encoder_decoder_weights(feat_idx, feat_layer, subject_device)\n",
    "\n",
    "    ### Find examples where the feature activates\n",
    "    # Remove any hooks\n",
    "    for l in range(len(subject_layers)):\n",
    "        subject_layers[l]._forward_hooks.clear()\n",
    "    print(\"done\")\n",
    "\n",
    "    selection_time = time.time()\n",
    "    subtexts = []\n",
    "    subtext_acts = []\n",
    "    for text in tqdm(candidate_texts, total=len(candidate_texts)):\n",
    "        input_ids = subject_tokenizer(text, return_tensors=\"pt\").input_ids.to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            out = subject(input_ids, output_hidden_states=True)\n",
    "            # hidden_states is actually one longer than the number of layers, because it includes the input embeddings\n",
    "            h = out.hidden_states[feat_layer + 1].squeeze(0)\n",
    "            # feat_acts = ae.activation(ae.encoder(h))[:, feat_idx]\n",
    "            feat_acts = h @ encoder_feat\n",
    "            # the first token position just has way higher norm all the time for some reason\n",
    "            feat_acts[0] = 0\n",
    "\n",
    "        for i in range(1, len(feat_acts) + 1):\n",
    "            reassembled_text = subject_tokenizer.decode(input_ids[0, :i])\n",
    "            subtexts.append(reassembled_text)\n",
    "            subtext_acts.append(feat_acts[i - 1].item())\n",
    "\n",
    "    # get a random sample of activating contexts\n",
    "    subtext_acts = torch.tensor(subtext_acts)\n",
    "    n_candidates = 200\n",
    "    candidate_indices = subtext_acts.topk(n_candidates).indices\n",
    "    sampled_indices = np.random.choice(candidate_indices.numpy(), n_scorer_texts + n_explainer_texts, replace=False)\n",
    "    \n",
    "    # Get top k subtexts and their activations\n",
    "    sampled_subtexts = [subtexts[i] for i in sampled_indices]\n",
    "    sampled_activations = subtext_acts[sampled_indices]\n",
    "\n",
    "    random.shuffle(sampled_subtexts)  # just as assurance\n",
    "    scorer_texts = sampled_subtexts[:n_scorer_texts]\n",
    "    explainer_texts = sampled_subtexts[n_scorer_texts:]\n",
    "    selection_time = time.time() - selection_time\n",
    "    print(f\"Selection took {selection_time:.2f} seconds\")\n",
    "\n",
    "    # get explanation\n",
    "    def get_subject_logits(text, layer, intervention_strength=0.0, position=-1, feat=None):\n",
    "        for l in range(len(subject_layers)):\n",
    "            subject_layers[l]._forward_hooks.clear()\n",
    "        subject_layers[layer].register_forward_hook(partial(intervene, intervention_strength=intervention_strength, position=-1, feat=feat))\n",
    "\n",
    "        inputs = subject_tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = subject(**inputs)\n",
    "\n",
    "        return outputs.logits[0, -1, :]\n",
    "\n",
    "    explainer_time = time.time()\n",
    "    intervention_examples = []\n",
    "    for text in explainer_texts:\n",
    "        clean_logits = get_subject_logits(text, feat_layer, intervention_strength=0.0, feat=decoder_feat)\n",
    "        intervened_logits = get_subject_logits(text, feat_layer, intervention_strength=explainer_intervention_strength, feat=decoder_feat)\n",
    "        top_probs = (intervened_logits.softmax(dim=-1) - clean_logits.softmax(dim=-1)).topk(n_intervention_examples)\n",
    "        \n",
    "        top_tokens = [subject_tokenizer.decode(i) for i in top_probs.indices]\n",
    "        top_p_increases = top_probs.values.tolist()\n",
    "        intervention_examples.append(\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=text,\n",
    "                top_tokens=top_tokens,\n",
    "                top_p_increases=top_p_increases\n",
    "            )\n",
    "        )\n",
    "\n",
    "    neuron_prompter = ExplainerNeuronFormatter(\n",
    "        intervention_examples=intervention_examples\n",
    "    )\n",
    "\n",
    "    # TODO: improve the few-shot examples\n",
    "    explainer_prompt = get_explainer_prompt(neuron_prompter, fs_examples)\n",
    "    explainer_input_ids = explainer_tokenizer(explainer_prompt, return_tensors=\"pt\").input_ids.to(explainer_device)\n",
    "    with torch.inference_mode():\n",
    "        samples = explainer.generate(explainer_input_ids, max_new_tokens=100, eos_token_id=explainer_tokenizer.encode(\"\\n\")[-1], num_return_sequences=n_explanations)[:, explainer_input_ids.shape[1]:]\n",
    "    explanations = Counter([explainer_tokenizer.decode(sample).split(\"\\n\")[0].strip() for sample in samples])\n",
    "    explainer_time = time.time() - explainer_time\n",
    "    print(f\"Explainer took {explainer_time:.2f} seconds\")\n",
    "\n",
    "    for ie in intervention_examples:\n",
    "        print(ie.top_tokens)\n",
    "        print(ie.top_p_increases)\n",
    "    print(explanations)\n",
    "\n",
    "    scoring_time = time.time()\n",
    "    predictiveness_score_by_explanation = dict()\n",
    "    normalized_predictiveness_score_by_explanation = dict()\n",
    "    all_pred_scores = []\n",
    "    scoring_interventions = dict()\n",
    "    for explanation in explanations:\n",
    "        expl_predictiveness_scores = []\n",
    "        scoring_interventions[explanation] = dict()\n",
    "        for scorer_intervention_strength in tqdm(scorer_intervention_strengths):\n",
    "            \n",
    "            current_pred_scores = []\n",
    "            max_intervened_prob = 0.0\n",
    "            scoring_interventions[scorer_intervention_strength] = []\n",
    "            for text in scorer_texts:\n",
    "                \n",
    "                intervened_probs = get_subject_logits(text, feat_layer, intervention_strength=scorer_intervention_strength, feat=decoder_feat).softmax(dim=-1).to(scorer_device)\n",
    "\n",
    "                # get the explanation predictiveness\n",
    "                scorer_predictiveness_prompt = get_scorer_predictiveness_prompt(text, explanation, few_shot_prompts, few_shot_explanations, few_shot_tokens)\n",
    "                scorer_input_ids = scorer_tokenizer(scorer_predictiveness_prompt, return_tensors=\"pt\").input_ids.to(scorer_device)\n",
    "                with torch.inference_mode():\n",
    "                    scorer_logits = scorer(scorer_input_ids).logits[0, -1, :]\n",
    "                    scorer_logp = scorer_logits.log_softmax(dim=-1)\n",
    "                \n",
    "                current_pred_scores.append((intervened_probs[subject_ids] * scorer_logp[scorer_ids]).sum())\n",
    "\n",
    "                topk = intervened_probs.topk(n_intervention_examples).indices\n",
    "                top_tokens = [subject_tokenizer.decode(i) for i in topk]\n",
    "                scoring_interventions[scorer_intervention_strength].append({\n",
    "                    \"prompt\": text,\n",
    "                    \"top_tokens\": top_tokens,\n",
    "                    \"top_token_probs\": intervened_probs[topk].tolist()\n",
    "                })\n",
    "\n",
    "            expl_predictiveness_scores.append(torch.tensor(current_pred_scores).mean().item())\n",
    "            all_pred_scores.extend(current_pred_scores * explanations[explanation])  # as if we did the inference on the scorer multiple times\n",
    "    \n",
    "        assert scorer_intervention_strengths[0] == 0\n",
    "        null_predictiveness_score = expl_predictiveness_scores[0]\n",
    "        normalized_predictiveness_scores = [score - null_predictiveness_score for score in expl_predictiveness_scores[1:]]\n",
    "        normalized_predictiveness_score = sum(normalized_predictiveness_scores) / len(normalized_predictiveness_scores)\n",
    "        predictiveness_score = normalized_predictiveness_score + null_predictiveness_score\n",
    "        \n",
    "        predictiveness_score_by_explanation[explanation] = predictiveness_score\n",
    "        normalized_predictiveness_score_by_explanation[explanation] = normalized_predictiveness_score\n",
    "    \n",
    "    # note that this computes stderr over explanations, pile samples, *and* intervention strengths (which is kind of weird)\n",
    "    pred_score_stderr = torch.std(torch.tensor(all_pred_scores)).item() / len(all_pred_scores) ** 0.5\n",
    "    pred_score = torch.mean(torch.tensor(all_pred_scores)).item()\n",
    "    normalized_predictiveness_score = sum([normalized_predictiveness_score_by_explanation[explanation] * count for explanation, count in explanations.items()]) / sum(explanations.values())\n",
    "\n",
    "    scoring_time = time.time() - scoring_time\n",
    "    print(f\"Scoring took {scoring_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"{normalized_predictiveness_score=}\")\n",
    "    print()\n",
    "    print()\n",
    "    all_results.append({\n",
    "        \"feat_idx\": feat_idx,\n",
    "        \"feat_layer\": feat_layer,\n",
    "        \"explanations\": dict(explanations),\n",
    "        \"predictiveness_score\": pred_score,\n",
    "        \"normalized_predictiveness_score\": normalized_predictiveness_score,\n",
    "        \"predictiveness_score_stderr\": pred_score_stderr,\n",
    "        \"max_predictiveness_score\": max(predictiveness_score_by_explanation.values()),\n",
    "        \"max_normalized_predictiveness_score\": max(normalized_predictiveness_score_by_explanation.values()),\n",
    "        \"explainer_prompts\": [example.prompt for example in intervention_examples],\n",
    "        \"explainer_top_tokens\": [example.top_tokens for example in intervention_examples],\n",
    "        \"explainer_top_p_increases\": [example.top_p_increases for example in intervention_examples],\n",
    "        \"scorer_intervention_strengths\": scorer_intervention_strengths,\n",
    "        \"explainer_intervention_strength\": explainer_intervention_strength,\n",
    "        \"scorer_texts\": scorer_texts,\n",
    "        \"explainer_texts\": explainer_texts,\n",
    "        \"predictiveness_score_by_explanation\": predictiveness_score_by_explanation,\n",
    "        \"normalized_predictiveness_score_by_explanation\": normalized_predictiveness_score_by_explanation,\n",
    "        \"scoring_interventions\": scoring_interventions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanations</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>normalized_predictiveness_score</th>\n",
       "      <th>predictiveness_score_stderr</th>\n",
       "      <th>max_predictiveness_score</th>\n",
       "      <th>max_normalized_predictiveness_score</th>\n",
       "      <th>explainer_prompts</th>\n",
       "      <th>explainer_top_tokens</th>\n",
       "      <th>explainer_top_p_increases</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_score_by_explanation</th>\n",
       "      <th>normalized_predictiveness_score_by_explanation</th>\n",
       "      <th>scoring_interventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12420</td>\n",
       "      <td>4</td>\n",
       "      <td>{'first person': 1, 'pronouns': 3, 'female pro...</td>\n",
       "      <td>-7.926619</td>\n",
       "      <td>2.511932</td>\n",
       "      <td>0.243935</td>\n",
       "      <td>-6.117489</td>\n",
       "      <td>3.846179</td>\n",
       "      <td>[A San Diego runner and cancer survivor says s...</td>\n",
       "      <td>[[ her,  she,  herself, &lt;|padding|&gt;, &lt;|endofte...</td>\n",
       "      <td>[[0.9901813268661499, 0.008692975156009197, 0....</td>\n",
       "      <td>[0, 10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[A group of Bitcoin miners that bought into th...</td>\n",
       "      <td>[A San Diego runner and cancer survivor says s...</td>\n",
       "      <td>{'first person': -9.072517585754394, 'pronouns...</td>\n",
       "      <td>{'first person': 0.6047761917114258, 'pronouns...</td>\n",
       "      <td>{'first person': {}, 0: [{'prompt': 'A group o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_idx  feat_layer                                       explanations  \\\n",
       "0     12420           4  {'first person': 1, 'pronouns': 3, 'female pro...   \n",
       "\n",
       "   predictiveness_score  normalized_predictiveness_score  \\\n",
       "0             -7.926619                         2.511932   \n",
       "\n",
       "   predictiveness_score_stderr  max_predictiveness_score  \\\n",
       "0                     0.243935                 -6.117489   \n",
       "\n",
       "   max_normalized_predictiveness_score  \\\n",
       "0                             3.846179   \n",
       "\n",
       "                                   explainer_prompts  \\\n",
       "0  [A San Diego runner and cancer survivor says s...   \n",
       "\n",
       "                                explainer_top_tokens  \\\n",
       "0  [[ her,  she,  herself, <|padding|>, <|endofte...   \n",
       "\n",
       "                           explainer_top_p_increases  \\\n",
       "0  [[0.9901813268661499, 0.008692975156009197, 0....   \n",
       "\n",
       "  scorer_intervention_strengths  explainer_intervention_strength  \\\n",
       "0   [0, 10, 32, 100, 320, 1000]                               32   \n",
       "\n",
       "                                        scorer_texts  \\\n",
       "0  [A group of Bitcoin miners that bought into th...   \n",
       "\n",
       "                                     explainer_texts  \\\n",
       "0  [A San Diego runner and cancer survivor says s...   \n",
       "\n",
       "                 predictiveness_score_by_explanation  \\\n",
       "0  {'first person': -9.072517585754394, 'pronouns...   \n",
       "\n",
       "      normalized_predictiveness_score_by_explanation  \\\n",
       "0  {'first person': 0.6047761917114258, 'pronouns...   \n",
       "\n",
       "                               scoring_interventions  \n",
       "0  {'first person': {}, 0: [{'prompt': 'A group o...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "all_df.to_json(f\"counterfactual_results/{subject_name.split('/')[-1]}_{len(feat_layers)}layers_{len(feat_idxs)}feats.json\")\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mscoring_interventions\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "all_df.to_pickle(f\"counterfactual_results/{len(feat_layers)}layers_{len(feat_idxs)}feats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanations</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>normalized_predictiveness_score</th>\n",
       "      <th>predictiveness_score_stderr</th>\n",
       "      <th>max_predictiveness_score</th>\n",
       "      <th>max_normalized_predictiveness_score</th>\n",
       "      <th>explainer_prompts</th>\n",
       "      <th>explainer_top_tokens</th>\n",
       "      <th>explainer_top_p_increases</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_score_by_explanation</th>\n",
       "      <th>normalized_predictiveness_score_by_explanation</th>\n",
       "      <th>scoring_interventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "      <td>{'abstract': 1, 'quotes': 2, 'quoting': 1, 'fo...</td>\n",
       "      <td>-10.830346</td>\n",
       "      <td>3.054792</td>\n",
       "      <td>0.321318</td>\n",
       "      <td>-9.701030</td>\n",
       "      <td>3.656476</td>\n",
       "      <td>[Abstract, S, \"]</td>\n",
       "      <td>[[’, ”, &lt;/, ]{}, Fig], [’, ]{}, Fig, ”, \\'], [...</td>\n",
       "      <td>[[0.2037494779, 0.0732237473, 0.0497660711, 0....</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[The, Description,                         ]</td>\n",
       "      <td>[Abstract, S, \"]</td>\n",
       "      <td>{'abstract': -10.4960212708, 'quotes': -10.482...</td>\n",
       "      <td>{'abstract': 2.8763084412, 'quotes': 2.8934225...</td>\n",
       "      <td>{'abstract': {'0': [{'prompt': 'The', 'top_tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>678</td>\n",
       "      <td>2</td>\n",
       "      <td>{'invention': 4, 'inventions': 1}</td>\n",
       "      <td>-13.566348</td>\n",
       "      <td>3.316314</td>\n",
       "      <td>0.309899</td>\n",
       "      <td>-12.978553</td>\n",
       "      <td>3.332780</td>\n",
       "      <td>[All, “, Ma]</td>\n",
       "      <td>[[ invention,  facts,  first,  Invention,  Gar...</td>\n",
       "      <td>[[0.9784275889, 0.0025607622, 0.0021682424, 0....</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[                        , Professional,      ...</td>\n",
       "      <td>[All, “, Ma]</td>\n",
       "      <td>{'invention': -12.9785531998, 'inventions': -1...</td>\n",
       "      <td>{'invention': 3.3327795029000002, 'inventions'...</td>\n",
       "      <td>{'invention': {'0': [{'prompt': '             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>859</td>\n",
       "      <td>2</td>\n",
       "      <td>{'a': 1, 'factuality': 1, 'fact': 2, 'crime': 1}</td>\n",
       "      <td>-11.020069</td>\n",
       "      <td>2.880541</td>\n",
       "      <td>0.284297</td>\n",
       "      <td>-10.285944</td>\n",
       "      <td>3.062009</td>\n",
       "      <td>[The, \\n, IN]</td>\n",
       "      <td>[[able,  fact,  we, \\r,  so], [able, crime, \\r...</td>\n",
       "      <td>[[0.08951304110000001, 0.06269221750000001, 0....</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[All, My,                         ]</td>\n",
       "      <td>[The, \\n, IN]</td>\n",
       "      <td>{'a': -10.4409889221, 'factuality': -10.671296...</td>\n",
       "      <td>{'a': 2.9604803085, 'factuality': 2.7379817963...</td>\n",
       "      <td>{'a': {'0': [{'prompt': 'All', 'top_tokens': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>{'professional': 1, 'tech': 1, 'technology': 3}</td>\n",
       "      <td>-10.699896</td>\n",
       "      <td>3.017359</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>-10.025401</td>\n",
       "      <td>3.052778</td>\n",
       "      <td>[Professional, S, T]</td>\n",
       "      <td>[[ technologies, :,  methods,  technology,  te...</td>\n",
       "      <td>[[0.5680180192000001, 0.0678640008, 0.06189619...</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Q, Ev,                         ]</td>\n",
       "      <td>[Professional, S, T]</td>\n",
       "      <td>{'professional': -10.0600194931, 'tech': -10.0...</td>\n",
       "      <td>{'professional': 3.015958786, 'tech': 3.052777...</td>\n",
       "      <td>{'professional': {'0': [{'prompt': 'Q', 'top_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>{'punctuation': 3, 'code': 2}</td>\n",
       "      <td>-10.189189</td>\n",
       "      <td>2.928971</td>\n",
       "      <td>0.414358</td>\n",
       "      <td>-9.659527</td>\n",
       "      <td>3.038082</td>\n",
       "      <td>[Q, //////////////////////////////////////////...</td>\n",
       "      <td>[[ \\\",  , ]{}, \\_, D], [*, \\], _, ]{},  \\\"], [...</td>\n",
       "      <td>[[0.0946576148, 0.059231139700000004, 0.052751...</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[I, By,                         ]</td>\n",
       "      <td>[Q, //////////////////////////////////////////...</td>\n",
       "      <td>{'punctuation': -9.6595269203, 'code': -9.7632...</td>\n",
       "      <td>{'punctuation': 3.0380820274, 'code': 2.765303...</td>\n",
       "      <td>{'punctuation': {'0': [{'prompt': 'I', 'top_to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>224</td>\n",
       "      <td>4</td>\n",
       "      <td>{'pronouns': 1, 'by': 2, 'language': 2}</td>\n",
       "      <td>-10.703680</td>\n",
       "      <td>-8.602939</td>\n",
       "      <td>0.465696</td>\n",
       "      <td>-11.957182</td>\n",
       "      <td>-8.497859</td>\n",
       "      <td>[Archive for the ‘body image’ Category\\n\\nI wa...</td>\n",
       "      <td>[[ by, �, &lt;|endoftext|&gt;, �, &lt;|padding|&gt;], [ by...</td>\n",
       "      <td>[[0.9656823277000001, 0.0, 0.0, 0.0, 0.0], [0....</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Hummus is a popular Middle Eastern appetizer....</td>\n",
       "      <td>[Archive for the ‘body image’ Category\\n\\nI wa...</td>\n",
       "      <td>{'pronouns': -12.3778980255, 'by': -12.1976281...</td>\n",
       "      <td>{'pronouns': -8.6482129574, 'by': -8.685382938...</td>\n",
       "      <td>{'pronouns': {'0': [{'prompt': 'Hummus is a po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>750</td>\n",
       "      <td>2</td>\n",
       "      <td>{'kindness': 4, 'kind': 1}</td>\n",
       "      <td>-12.893206</td>\n",
       "      <td>-8.797585</td>\n",
       "      <td>0.465511</td>\n",
       "      <td>-14.321601</td>\n",
       "      <td>-8.643282</td>\n",
       "      <td>[So, while I’m looking forward to tonight’s ev...</td>\n",
       "      <td>[[ kind,  really,  sort,  got,  had], [ kind, ...</td>\n",
       "      <td>[[0.499720484, 0.0873650163, 0.032158237, 0.03...</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Victory Castle Conspirer\\n\\nMichael Moore zu ...</td>\n",
       "      <td>[So, while I’m looking forward to tonight’s ev...</td>\n",
       "      <td>{'kindness': -14.3216005325, 'kind': -14.51095...</td>\n",
       "      <td>{'kindness': -8.8361607552, 'kind': -8.6432816...</td>\n",
       "      <td>{'kindness': {'0': [{'prompt': 'Victory Castle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>{'code': 2, 'computer': 1, 'programming': 1, '...</td>\n",
       "      <td>-10.575034</td>\n",
       "      <td>-9.589483</td>\n",
       "      <td>0.479447</td>\n",
       "      <td>-12.081158</td>\n",
       "      <td>-9.334050</td>\n",
       "      <td>[Q:\\n\\nProGuard java.lang.NoSuchMethodExceptio...</td>\n",
       "      <td>[[g, f, _, cl, il], [get, _, comp, gen, cl], [...</td>\n",
       "      <td>[[0.18580000100000002, 0.15455044810000002, 0....</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[\\n25 B.R. 410 (1982)\\nIn re Alan CAMERON, Deb...</td>\n",
       "      <td>[Q:\\n\\nProGuard java.lang.NoSuchMethodExceptio...</td>\n",
       "      <td>{'code': -12.2648371696, 'computer': -12.08115...</td>\n",
       "      <td>{'code': -9.7298735619, 'computer': -9.3340501...</td>\n",
       "      <td>{'code': {'0': [{'prompt': '\\n25 B.R. 410 (198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>975</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Poland': 2, 'Polish': 2, 'polish': 1}</td>\n",
       "      <td>-13.072577</td>\n",
       "      <td>-12.105904</td>\n",
       "      <td>0.631727</td>\n",
       "      <td>-14.961908</td>\n",
       "      <td>-11.911187</td>\n",
       "      <td>[Hepatozoon\\n, Podgornoye, Republic of Bury, {...</td>\n",
       "      <td>[[ Poland,  Polish,  Warsaw, ł, &lt;|endoftext|&gt;]...</td>\n",
       "      <td>[[0.6104973555000001, 0.36323639750000003, 0.0...</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Podgornoye, Republic of Buryatia\\n\\nPodgornoy...</td>\n",
       "      <td>[Hepatozoon\\n, Podgornoye, Republic of Bury, {...</td>\n",
       "      <td>{'Poland': -14.961907959, 'Polish': -15.170232...</td>\n",
       "      <td>{'Poland': -11.9111870289, 'Polish': -12.24814...</td>\n",
       "      <td>{'Poland': {'0': [{'prompt': 'Podgornoye, Repu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>939</td>\n",
       "      <td>2</td>\n",
       "      <td>{'programming': 1, 'framework': 1, 'software':...</td>\n",
       "      <td>-13.136003</td>\n",
       "      <td>-12.926760</td>\n",
       "      <td>0.625540</td>\n",
       "      <td>-15.244898</td>\n",
       "      <td>-12.821550</td>\n",
       "      <td>[This is an archived article and the informati...</td>\n",
       "      <td>[[ Core,  Framework,  MVC,  framework,  C], [ ...</td>\n",
       "      <td>[[0.8490654826, 0.0435771905, 0.02567716500000...</td>\n",
       "      <td>[0, -10, -32, -100, -320, -1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Sequential Temporal Dependencies in Associati...</td>\n",
       "      <td>[This is an archived article and the informati...</td>\n",
       "      <td>{'programming': -15.2482095718, 'framework': -...</td>\n",
       "      <td>{'programming': -12.8527520657, 'framework': -...</td>\n",
       "      <td>{'programming': {'0': [{'prompt': 'Sequential ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat_idx  feat_layer                                       explanations  \\\n",
       "924        462           4  {'abstract': 1, 'quotes': 2, 'quoting': 1, 'fo...   \n",
       "1357       678           2                  {'invention': 4, 'inventions': 1}   \n",
       "1719       859           2   {'a': 1, 'factuality': 1, 'fact': 2, 'crime': 1}   \n",
       "1783       891           2    {'professional': 1, 'tech': 1, 'technology': 3}   \n",
       "288        144           4                      {'punctuation': 3, 'code': 2}   \n",
       "...        ...         ...                                                ...   \n",
       "448        224           4            {'pronouns': 1, 'by': 2, 'language': 2}   \n",
       "1501       750           2                         {'kindness': 4, 'kind': 1}   \n",
       "1387       693           2  {'code': 2, 'computer': 1, 'programming': 1, '...   \n",
       "1950       975           4            {'Poland': 2, 'Polish': 2, 'polish': 1}   \n",
       "1879       939           2  {'programming': 1, 'framework': 1, 'software':...   \n",
       "\n",
       "      predictiveness_score  normalized_predictiveness_score  \\\n",
       "924             -10.830346                         3.054792   \n",
       "1357            -13.566348                         3.316314   \n",
       "1719            -11.020069                         2.880541   \n",
       "1783            -10.699896                         3.017359   \n",
       "288             -10.189189                         2.928971   \n",
       "...                    ...                              ...   \n",
       "448             -10.703680                        -8.602939   \n",
       "1501            -12.893206                        -8.797585   \n",
       "1387            -10.575034                        -9.589483   \n",
       "1950            -13.072577                       -12.105904   \n",
       "1879            -13.136003                       -12.926760   \n",
       "\n",
       "      predictiveness_score_stderr  max_predictiveness_score  \\\n",
       "924                      0.321318                 -9.701030   \n",
       "1357                     0.309899                -12.978553   \n",
       "1719                     0.284297                -10.285944   \n",
       "1783                     0.317637                -10.025401   \n",
       "288                      0.414358                 -9.659527   \n",
       "...                           ...                       ...   \n",
       "448                      0.465696                -11.957182   \n",
       "1501                     0.465511                -14.321601   \n",
       "1387                     0.479447                -12.081158   \n",
       "1950                     0.631727                -14.961908   \n",
       "1879                     0.625540                -15.244898   \n",
       "\n",
       "      max_normalized_predictiveness_score  \\\n",
       "924                              3.656476   \n",
       "1357                             3.332780   \n",
       "1719                             3.062009   \n",
       "1783                             3.052778   \n",
       "288                              3.038082   \n",
       "...                                   ...   \n",
       "448                             -8.497859   \n",
       "1501                            -8.643282   \n",
       "1387                            -9.334050   \n",
       "1950                           -11.911187   \n",
       "1879                           -12.821550   \n",
       "\n",
       "                                      explainer_prompts  \\\n",
       "924                                    [Abstract, S, \"]   \n",
       "1357                                       [All, “, Ma]   \n",
       "1719                                      [The, \\n, IN]   \n",
       "1783                               [Professional, S, T]   \n",
       "288   [Q, //////////////////////////////////////////...   \n",
       "...                                                 ...   \n",
       "448   [Archive for the ‘body image’ Category\\n\\nI wa...   \n",
       "1501  [So, while I’m looking forward to tonight’s ev...   \n",
       "1387  [Q:\\n\\nProGuard java.lang.NoSuchMethodExceptio...   \n",
       "1950  [Hepatozoon\\n, Podgornoye, Republic of Bury, {...   \n",
       "1879  [This is an archived article and the informati...   \n",
       "\n",
       "                                   explainer_top_tokens  \\\n",
       "924   [[’, ”, </, ]{}, Fig], [’, ]{}, Fig, ”, \\'], [...   \n",
       "1357  [[ invention,  facts,  first,  Invention,  Gar...   \n",
       "1719  [[able,  fact,  we, \\r,  so], [able, crime, \\r...   \n",
       "1783  [[ technologies, :,  methods,  technology,  te...   \n",
       "288   [[ \\\",  , ]{}, \\_, D], [*, \\], _, ]{},  \\\"], [...   \n",
       "...                                                 ...   \n",
       "448   [[ by, �, <|endoftext|>, �, <|padding|>], [ by...   \n",
       "1501  [[ kind,  really,  sort,  got,  had], [ kind, ...   \n",
       "1387  [[g, f, _, cl, il], [get, _, comp, gen, cl], [...   \n",
       "1950  [[ Poland,  Polish,  Warsaw, ł, <|endoftext|>]...   \n",
       "1879  [[ Core,  Framework,  MVC,  framework,  C], [ ...   \n",
       "\n",
       "                              explainer_top_p_increases  \\\n",
       "924   [[0.2037494779, 0.0732237473, 0.0497660711, 0....   \n",
       "1357  [[0.9784275889, 0.0025607622, 0.0021682424, 0....   \n",
       "1719  [[0.08951304110000001, 0.06269221750000001, 0....   \n",
       "1783  [[0.5680180192000001, 0.0678640008, 0.06189619...   \n",
       "288   [[0.0946576148, 0.059231139700000004, 0.052751...   \n",
       "...                                                 ...   \n",
       "448   [[0.9656823277000001, 0.0, 0.0, 0.0, 0.0], [0....   \n",
       "1501  [[0.499720484, 0.0873650163, 0.032158237, 0.03...   \n",
       "1387  [[0.18580000100000002, 0.15455044810000002, 0....   \n",
       "1950  [[0.6104973555000001, 0.36323639750000003, 0.0...   \n",
       "1879  [[0.8490654826, 0.0435771905, 0.02567716500000...   \n",
       "\n",
       "         scorer_intervention_strengths  explainer_intervention_strength  \\\n",
       "924   [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1357  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1719  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1783  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "288   [0, -10, -32, -100, -320, -1000]                               32   \n",
       "...                                ...                              ...   \n",
       "448   [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1501  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1387  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1950  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "1879  [0, -10, -32, -100, -320, -1000]                               32   \n",
       "\n",
       "                                           scorer_texts  \\\n",
       "924        [The, Description,                         ]   \n",
       "1357  [                        , Professional,      ...   \n",
       "1719                [All, My,                         ]   \n",
       "1783                  [Q, Ev,                         ]   \n",
       "288                   [I, By,                         ]   \n",
       "...                                                 ...   \n",
       "448   [Hummus is a popular Middle Eastern appetizer....   \n",
       "1501  [Victory Castle Conspirer\\n\\nMichael Moore zu ...   \n",
       "1387  [\\n25 B.R. 410 (1982)\\nIn re Alan CAMERON, Deb...   \n",
       "1950  [Podgornoye, Republic of Buryatia\\n\\nPodgornoy...   \n",
       "1879  [Sequential Temporal Dependencies in Associati...   \n",
       "\n",
       "                                        explainer_texts  \\\n",
       "924                                    [Abstract, S, \"]   \n",
       "1357                                       [All, “, Ma]   \n",
       "1719                                      [The, \\n, IN]   \n",
       "1783                               [Professional, S, T]   \n",
       "288   [Q, //////////////////////////////////////////...   \n",
       "...                                                 ...   \n",
       "448   [Archive for the ‘body image’ Category\\n\\nI wa...   \n",
       "1501  [So, while I’m looking forward to tonight’s ev...   \n",
       "1387  [Q:\\n\\nProGuard java.lang.NoSuchMethodExceptio...   \n",
       "1950  [Hepatozoon\\n, Podgornoye, Republic of Bury, {...   \n",
       "1879  [This is an archived article and the informati...   \n",
       "\n",
       "                    predictiveness_score_by_explanation  \\\n",
       "924   {'abstract': -10.4960212708, 'quotes': -10.482...   \n",
       "1357  {'invention': -12.9785531998, 'inventions': -1...   \n",
       "1719  {'a': -10.4409889221, 'factuality': -10.671296...   \n",
       "1783  {'professional': -10.0600194931, 'tech': -10.0...   \n",
       "288   {'punctuation': -9.6595269203, 'code': -9.7632...   \n",
       "...                                                 ...   \n",
       "448   {'pronouns': -12.3778980255, 'by': -12.1976281...   \n",
       "1501  {'kindness': -14.3216005325, 'kind': -14.51095...   \n",
       "1387  {'code': -12.2648371696, 'computer': -12.08115...   \n",
       "1950  {'Poland': -14.961907959, 'Polish': -15.170232...   \n",
       "1879  {'programming': -15.2482095718, 'framework': -...   \n",
       "\n",
       "         normalized_predictiveness_score_by_explanation  \\\n",
       "924   {'abstract': 2.8763084412, 'quotes': 2.8934225...   \n",
       "1357  {'invention': 3.3327795029000002, 'inventions'...   \n",
       "1719  {'a': 2.9604803085, 'factuality': 2.7379817963...   \n",
       "1783  {'professional': 3.015958786, 'tech': 3.052777...   \n",
       "288   {'punctuation': 3.0380820274, 'code': 2.765303...   \n",
       "...                                                 ...   \n",
       "448   {'pronouns': -8.6482129574, 'by': -8.685382938...   \n",
       "1501  {'kindness': -8.8361607552, 'kind': -8.6432816...   \n",
       "1387  {'code': -9.7298735619, 'computer': -9.3340501...   \n",
       "1950  {'Poland': -11.9111870289, 'Polish': -12.24814...   \n",
       "1879  {'programming': -12.8527520657, 'framework': -...   \n",
       "\n",
       "                                  scoring_interventions  \n",
       "924   {'abstract': {'0': [{'prompt': 'The', 'top_tok...  \n",
       "1357  {'invention': {'0': [{'prompt': '             ...  \n",
       "1719  {'a': {'0': [{'prompt': 'All', 'top_tokens': [...  \n",
       "1783  {'professional': {'0': [{'prompt': 'Q', 'top_t...  \n",
       "288   {'punctuation': {'0': [{'prompt': 'I', 'top_to...  \n",
       "...                                                 ...  \n",
       "448   {'pronouns': {'0': [{'prompt': 'Hummus is a po...  \n",
       "1501  {'kindness': {'0': [{'prompt': 'Victory Castle...  \n",
       "1387  {'code': {'0': [{'prompt': '\\n25 B.R. 410 (198...  \n",
       "1950  {'Poland': {'0': [{'prompt': 'Podgornoye, Repu...  \n",
       "1879  {'programming': {'0': [{'prompt': 'Sequential ...  \n",
       "\n",
       "[2000 rows x 18 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"counterfactual_results/neg_pythia-70m-deduped_2layers_1000feats.json\"\n",
    "all_df = pd.read_json(path, orient=\"records\")\n",
    "x = all_df.sort_values(\"max_normalized_predictiveness_score\", ascending=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/2000) \"formatting\" | layer 4, idx 462\n",
      "\texplanations: {'abstract': 1, 'quotes': 2, 'quoting': 1, 'formatting': 1}\n",
      "\tnormalized predictiveness score: 3.1 ± 0.3\n",
      "\tmax normalized predictiveness score: 3.7 (expl=formatting)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Abstract</PROMPT>\n",
      "\t\tMost increased tokens: '’' (+0.204), '”' (+0.073), '</' (+0.05), ']{}' (+0.028), 'Fig' (+0.022)\n",
      "\n",
      "\t<PROMPT>S</PROMPT>\n",
      "\t\tMost increased tokens: '’' (+0.159), ']{}' (+0.157), 'Fig' (+0.056), '”' (+0.055), '\\'' (+0.04)\n",
      "\n",
      "\t<PROMPT>\"</PROMPT>\n",
      "\t\tMost increased tokens: 'Fig' (+0.063), '’' (+0.041), '”' (+0.023), '<' (+0.021), ' “' (+0.016)\n",
      "\n",
      "(2/2000) \"invention\" | layer 2, idx 678\n",
      "\texplanations: {'invention': 4, 'inventions': 1}\n",
      "\tnormalized predictiveness score: 3.3 ± 0.3\n",
      "\tmax normalized predictiveness score: 3.3 (expl=invention)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>All</PROMPT>\n",
      "\t\tMost increased tokens: ' invention' (+0.978), ' facts' (+0.003), ' first' (+0.002), ' Invention' (+0.002), ' Gardner' (+0.001)\n",
      "\n",
      "\t<PROMPT>“</PROMPT>\n",
      "\t\tMost increased tokens: ' invention' (+1.0), ' disclosed' (+0.0), ' foregoing' (+0.0), ' Invention' (+0.0), 'fulness' (+0.0)\n",
      "\n",
      "\t<PROMPT>Ma</PROMPT>\n",
      "\t\tMost increased tokens: ' invention' (+0.981), 'ard' (+0.003), ' Gardner' (+0.001), ' matter' (+0.001), ' Art' (+0.001)\n",
      "\n",
      "(3/2000) \"crime\" | layer 2, idx 859\n",
      "\texplanations: {'a': 1, 'factuality': 1, 'fact': 2, 'crime': 1}\n",
      "\tnormalized predictiveness score: 2.9 ± 0.3\n",
      "\tmax normalized predictiveness score: 3.1 (expl=crime)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>The</PROMPT>\n",
      "' (+0.033), ' so' (+0.027)able' (+0.09), ' fact' (+0.063), ' we' (+0.062), '\n",
      "\n",
      "\t<PROMPT>\\n</PROMPT>\n",
      "' (+0.052), 'like' (+0.047), '_**' (+0.033)crime' (+0.057), '\n",
      "\n",
      "\t<PROMPT>IN</PROMPT>\n",
      "\t\tMost increased tokens: 'able' (+0.439), ' sua' (+0.049), ' ju' (+0.013), ' fact' (+0.012), ' jul' (+0.012)\n",
      "\n",
      "(4/2000) \"tech\" | layer 2, idx 891\n",
      "\texplanations: {'professional': 1, 'tech': 1, 'technology': 3}\n",
      "\tnormalized predictiveness score: 3.0 ± 0.3\n",
      "\tmax normalized predictiveness score: 3.1 (expl=tech)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Professional</PROMPT>\n",
      "\t\tMost increased tokens: ' technologies' (+0.568), ':' (+0.068), ' methods' (+0.062), ' technology' (+0.033), ' techniques' (+0.021)\n",
      "\n",
      "\t<PROMPT>S</PROMPT>\n",
      "\t\tMost increased tokens: ' technologies' (+0.294), 'nai' (+0.13), ':' (+0.054), ',' (+0.046), ' technology' (+0.021)\n",
      "\n",
      "\t<PROMPT>T</PROMPT>\n",
      "\t\tMost increased tokens: 'nai' (+0.401), ' technologies' (+0.128), 'ras' (+0.127), 'ACE' (+0.03), ',' (+0.03)\n",
      "\n",
      "(5/2000) \"punctuation\" | layer 4, idx 144\n",
      "\texplanations: {'punctuation': 3, 'code': 2}\n",
      "\tnormalized predictiveness score: 2.9 ± 0.4\n",
      "\tmax normalized predictiveness score: 3.0 (expl=punctuation)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Q</PROMPT>\n",
      "\t\tMost increased tokens: ' \\\"' (+0.095), ' ' (+0.059), ']{}' (+0.053), '\\_' (+0.044), 'D' (+0.038)\n",
      "\n",
      "\t<PROMPT>////////////////////////////////////////////////////////////////</PROMPT>\n",
      "\t\tMost increased tokens: '*' (+0.158), '\\]' (+0.046), '_' (+0.045), ']{}' (+0.038), ' \\\"' (+0.032)\n",
      "\n",
      "\t<PROMPT>Dist</PROMPT>\n",
      "\t\tMost increased tokens: '\\_' (+0.076), 'CoA' (+0.065), ' Fig' (+0.016), ' tracts' (+0.015), 'ed' (+0.015)\n",
      "\n",
      "(10/2000) \"verbs\" | layer 2, idx 667\n",
      "\texplanations: {'verbs': 2, 'letters': 1, 'time': 1, 'z': 1}\n",
      "\tnormalized predictiveness score: 1.7 ± 0.1\n",
      "\tmax normalized predictiveness score: 2.5 (expl=verbs)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Z</PROMPT>\n",
      "\t\tMost increased tokens: 'olo' (+0.409), 'uma' (+0.177), 'ical' (+0.069), 'ony' (+0.053), 'sz' (+0.03)\n",
      "\n",
      "\t<PROMPT>Q</PROMPT>\n",
      "\t\tMost increased tokens: 'ical' (+0.407), 's' (+0.243), 'olo' (+0.142), 'ios' (+0.03), 'AL' (+0.014)\n",
      "\n",
      "\t<PROMPT>Wednesday</PROMPT>\n",
      "\t\tMost increased tokens: ' by' (+0.265), 's' (+0.174), 'olo' (+0.089), ' involved' (+0.045), 'ical' (+0.039)\n",
      "\n",
      "(50/2000) \"grammar\" | layer 4, idx 505\n",
      "\texplanations: {'code': 3, 'grammar': 1, 'numbers': 1}\n",
      "\tnormalized predictiveness score: 1.0 ± 0.1\n",
      "\tmax normalized predictiveness score: 1.5 (expl=grammar)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>A</PROMPT>\n",
      "\t\tMost increased tokens: ');' (+0.033), 'POS' (+0.015), ' ' (+0.012), '-' (+0.011), ' \\[[@' (+0.011)\n",
      "\n",
      "\t<PROMPT>H</PROMPT>\n",
      "\t\tMost increased tokens: 'eter' (+0.17), 'emic' (+0.028), 'r' (+0.026), 'CO' (+0.019), ' \\[[@' (+0.018)\n",
      "\n",
      "\t<PROMPT>In</PROMPT>\n",
      "\t\tMost increased tokens: ' it' (+0.014), 'r' (+0.014), ' \\[[@' (+0.013), 'mable' (+0.013), ' implemented' (+0.012)\n",
      "\n",
      "(100/2000) \"nouns\" | layer 4, idx 334\n",
      "\texplanations: {'nouns': 1, 'abbreviations': 1, 'lists': 1, 'numbers': 1, 'wild law parties': 1}\n",
      "\tnormalized predictiveness score: 0.6 ± 0.1\n",
      "\tmax normalized predictiveness score: 1.0 (expl=nouns)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>[</PROMPT>\n",
      "\t\tMost increased tokens: '\n",
      "' (+0.088), ' of' (+0.073), 'p' (+0.032), ' respectively' (+0.032), ' as' (+0.028)\n",
      "\n",
      "\t<PROMPT>H</PROMPT>\n",
      "\t\tMost increased tokens: ' of' (+0.138), ' respectively' (+0.048), '”' (+0.032), 'nad' (+0.022), 'unks' (+0.021)\n",
      "\n",
      "\t<PROMPT>Wild</PROMPT>\n",
      "\t\tMost increased tokens: 'ists' (+0.094), ' law' (+0.061), ' parties' (+0.058), 'ders' (+0.046), ' of' (+0.032)\n",
      "\n",
      "(500/2000) \"mid\" | layer 4, idx 673\n",
      "\texplanations: {'response': 1, 'review': 2, 'receiver': 1, 'mid': 1}\n",
      "\tnormalized predictiveness score: -0.6 ± 0.2\n",
      "\tmax normalized predictiveness score: -0.5 (expl=mid)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>/*</PROMPT>\n",
      "\t\tMost increased tokens: 'mid' (+0.05), ' o' (+0.039), ' finally' (+0.039), 'y' (+0.032), 'men' (+0.032)\n",
      "\n",
      "\t<PROMPT>Receiver</PROMPT>\n",
      "\t\tMost increased tokens: ' was' (+0.223), ' review' (+0.088), ' investigation' (+0.027), ' Review' (+0.024), ' response' (+0.022)\n",
      "\n",
      "\t<PROMPT>S</PROMPT>\n",
      "\t\tMost increased tokens: 'ung' (+0.114), 'ivers' (+0.081), ' review' (+0.077), 'herry' (+0.074), 'mid' (+0.072)\n",
      "\n",
      "(1000/2000) \"view\" | layer 4, idx 606\n",
      "\texplanations: {'Q&A': 1, 'questions': 1, 'view': 1, 'punctuation': 1, 'text': 1}\n",
      "\tnormalized predictiveness score: -1.6 ± 0.2\n",
      "\tmax normalized predictiveness score: -1.4 (expl=view)\n",
      "\n",
      "\tExample counterfactuals (intervention strength = 32):\n",
      "\t<PROMPT>Q</PROMPT>\n",
      "\t\tMost increased tokens: '**' (+0.108), ' <' (+0.074), ' Talk' (+0.054), '_' (+0.045), ':' (+0.038)\n",
      "\n",
      "\t<PROMPT>Dear</PROMPT>\n",
      "\t\tMost increased tokens: ' me' (+0.452), ' view' (+0.04), ' us' (+0.036), ' member' (+0.03), ' sight' (+0.014)\n",
      "\n",
      "\t<PROMPT>[</PROMPT>\n",
      "\t\tMost increased tokens: ' <' (+0.189), ' view' (+0.041), ' member' (+0.037), 'y' (+0.034), '_' (+0.031)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in list(range(5)) + [9, 49, 99, 499, 999]:\n",
    "\n",
    "    row = x.iloc[i]\n",
    "    best_expl = max(row.normalized_predictiveness_score_by_explanation, key=row.normalized_predictiveness_score_by_explanation.get)\n",
    "    print(f\"({i + 1}/{len(x)}) \\\"{best_expl}\\\" | layer {row.feat_layer}, idx {row.feat_idx}\")\n",
    "    print(f\"\\texplanations: {row.explanations}\")\n",
    "    print(f\"\\tnormalized predictiveness score: {row.normalized_predictiveness_score:.1f} ± {row.predictiveness_score_stderr:.1f}\")\n",
    "    print(f\"\\tmax normalized predictiveness score: {row.max_normalized_predictiveness_score:.1f} (expl={best_expl})\")\n",
    "    print(\"\\n\\tExample counterfactuals (intervention strength = 32):\")\n",
    "    for pr, tts, tpis in zip(row[\"explainer_prompts\"], row[\"explainer_top_tokens\"], row[\"explainer_top_p_increases\"]):\n",
    "        ex = ExplainerInterventionExample(\n",
    "                    prompt=pr,\n",
    "                    top_tokens=tts,\n",
    "                    top_p_increases=tpis\n",
    "                )\n",
    "        print(\"\\t\" + ex.text().replace(\"\\nMost\", \"\\n\\t\\tMost\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "key = \"max_normalized_predictiveness_score\"\n",
    "plt.hist(all_df[key].values)\n",
    "plt.xlabel(key)\n",
    "plt.title(path.split(\"/\")[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
