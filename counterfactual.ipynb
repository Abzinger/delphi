{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject model\n",
    "# load SAEs without attaching them to the model\n",
    "# for now just use the Islam feature and explanation\n",
    "# load a scorer. The prompt should have the input as well this time\n",
    "# (for now) on random pretraining data, evaluate gpt2 with a hook that \n",
    "# adds a multiple of the Islam feature to the appropriate residual stream layer and position\n",
    "# Get the pre- and post-intervention output distributions of gpt2\n",
    "# (TODO: check if all the Islam features just have similar embeddings)\n",
    "# Show this to the scorer and get a score (scorer should be able to have a good prior without being given the clean output distribution)\n",
    "# Also get a simplicity score for the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_correlation_score</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature0</th>\n",
       "      <td>0.970093</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature19</th>\n",
       "      <td>0.966378</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature0</th>\n",
       "      <td>0.952401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature4</th>\n",
       "      <td>0.952061</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature5</th>\n",
       "      <td>0.949993</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature4</th>\n",
       "      <td>0.941871</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature11</th>\n",
       "      <td>0.930066</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature19</th>\n",
       "      <td>0.918787</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature14</th>\n",
       "      <td>0.906342</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature3</th>\n",
       "      <td>0.897080</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature11</th>\n",
       "      <td>0.883083</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature9</th>\n",
       "      <td>0.868529</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature3</th>\n",
       "      <td>0.810241</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature18</th>\n",
       "      <td>0.805457</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature16</th>\n",
       "      <td>0.782019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature11</th>\n",
       "      <td>0.779133</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature10</th>\n",
       "      <td>0.772255</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature7</th>\n",
       "      <td>0.764754</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature7</th>\n",
       "      <td>0.712047</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature12</th>\n",
       "      <td>0.711850</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature0</th>\n",
       "      <td>0.698656</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature12</th>\n",
       "      <td>0.678797</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature15</th>\n",
       "      <td>0.668821</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature13</th>\n",
       "      <td>0.668621</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature1</th>\n",
       "      <td>0.662348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature2</th>\n",
       "      <td>0.659443</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature13</th>\n",
       "      <td>0.649270</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature3</th>\n",
       "      <td>0.645487</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature6</th>\n",
       "      <td>0.643440</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature17</th>\n",
       "      <td>0.627347</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature8</th>\n",
       "      <td>0.583940</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature15</th>\n",
       "      <td>0.556673</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature4</th>\n",
       "      <td>0.470279</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature6</th>\n",
       "      <td>0.447680</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature7</th>\n",
       "      <td>0.390438</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature18</th>\n",
       "      <td>0.378467</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature14</th>\n",
       "      <td>0.316718</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature9</th>\n",
       "      <td>0.272719</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature10</th>\n",
       "      <td>0.228020</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature13</th>\n",
       "      <td>0.167355</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.2_feature5</th>\n",
       "      <td>0.095751</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.4_feature2</th>\n",
       "      <td>0.023048</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.transformer.h.0_feature15</th>\n",
       "      <td>-0.060541</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ev_correlation_score  layer  feature\n",
       ".transformer.h.2_feature0               0.970093      2        0\n",
       ".transformer.h.2_feature19              0.966378      2       19\n",
       ".transformer.h.0_feature0               0.952401      0        0\n",
       ".transformer.h.4_feature4               0.952061      4        4\n",
       ".transformer.h.0_feature5               0.949993      0        5\n",
       ".transformer.h.2_feature4               0.941871      2        4\n",
       ".transformer.h.2_feature11              0.930066      2       11\n",
       ".transformer.h.4_feature19              0.918787      4       19\n",
       ".transformer.h.0_feature14              0.906342      0       14\n",
       ".transformer.h.0_feature3               0.897080      0        3\n",
       ".transformer.h.0_feature11              0.883083      0       11\n",
       ".transformer.h.0_feature9               0.868529      0        9\n",
       ".transformer.h.2_feature3               0.810241      2        3\n",
       ".transformer.h.2_feature18              0.805457      2       18\n",
       ".transformer.h.0_feature16              0.782019      0       16\n",
       ".transformer.h.4_feature11              0.779133      4       11\n",
       ".transformer.h.0_feature10              0.772255      0       10\n",
       ".transformer.h.0_feature7               0.764754      0        7\n",
       ".transformer.h.2_feature7               0.712047      2        7\n",
       ".transformer.h.0_feature12              0.711850      0       12\n",
       ".transformer.h.4_feature0               0.698656      4        0\n",
       ".transformer.h.4_feature12              0.678797      4       12\n",
       ".transformer.h.4_feature15              0.668821      4       15\n",
       ".transformer.h.4_feature13              0.668621      4       13\n",
       ".transformer.h.2_feature1               0.662348      2        1\n",
       ".transformer.h.2_feature2               0.659443      2        2\n",
       ".transformer.h.0_feature13              0.649270      0       13\n",
       ".transformer.h.4_feature3               0.645487      4        3\n",
       ".transformer.h.2_feature6               0.643440      2        6\n",
       ".transformer.h.4_feature17              0.627347      4       17\n",
       ".transformer.h.4_feature8               0.583940      4        8\n",
       ".transformer.h.2_feature15              0.556673      2       15\n",
       ".transformer.h.0_feature4               0.470279      0        4\n",
       ".transformer.h.0_feature6               0.447680      0        6\n",
       ".transformer.h.4_feature7               0.390438      4        7\n",
       ".transformer.h.0_feature18              0.378467      0       18\n",
       ".transformer.h.4_feature14              0.316718      4       14\n",
       ".transformer.h.4_feature9               0.272719      4        9\n",
       ".transformer.h.4_feature10              0.228020      4       10\n",
       ".transformer.h.2_feature13              0.167355      2       13\n",
       ".transformer.h.2_feature5               0.095751      2        5\n",
       ".transformer.h.4_feature2               0.023048      4        2\n",
       ".transformer.h.0_feature15             -0.060541      0       15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "results_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/results/gpt2_simulation/all_at_once\"\n",
    "results = dict()\n",
    "for fname in Path(results_dir).iterdir():\n",
    "    with open(fname, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "    last = fname.stem.split(\".\")[-1]\n",
    "    layer = int(last.split(\"_\")[0])\n",
    "    feat = int(last[last.index(\"_feature\") + len(\"_feature\"):])\n",
    "    results[fname.stem] = {\"ev_correlation_score\": r[\"ev_correlation_score\"], \"layer\": layer, \"feature\": feat}\n",
    "input_scores_df = pd.DataFrame(results).T\n",
    "input_scores_df[\"layer\"] = input_scores_df[\"layer\"].astype(int)\n",
    "input_scores_df[\"feature\"] = input_scores_df[\"feature\"].astype(int)\n",
    "input_scores_df = input_scores_df.sort_values(\"ev_correlation_score\", ascending=False)\n",
    "unq_layers = input_scores_df[\"layer\"].unique()\n",
    "input_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"pile.jsonl\", \"r\") as f:\n",
    "    pile = random.sample([json.loads(line) for line in f.readlines()], 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/autointerp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m subject_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m subject_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m subject \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m subject_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(subject_name)\n\u001b[1;32m      8\u001b[0m subject_tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m subject_tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2905\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/autointerp/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/autointerp/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/autointerp/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/autointerp/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/autointerp/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "subject_device = \"cuda:1\"\n",
    "\n",
    "subject_name = \"gpt2\"\n",
    "subject = AutoModelForCausalLM.from_pretrained(subject_name).to(subject_device)\n",
    "subject_tokenizer = AutoTokenizer.from_pretrained(subject_name)\n",
    "subject_tokenizer.pad_token = subject_tokenizer.eos_token\n",
    "subject.config.pad_token_id = subject_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hugging Face cache directory is: /home/alex/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import TRANSFORMERS_CACHE\n",
    "\n",
    "print(f\"The Hugging Face cache directory is: {TRANSFORMERS_CACHE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:29<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "scorer_device = \"cuda:0\"\n",
    "scorer_name = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "scorer = AutoModelForCausalLM.from_pretrained(\n",
    "    scorer_name,\n",
    "    device_map={\"\": scorer_device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    ")\n",
    "scorer_tokenizer = AutoTokenizer.from_pretrained(scorer_name)\n",
    "scorer_tokenizer.pad_token = scorer_tokenizer.eos_token\n",
    "scorer.config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "scorer.generation_config.pad_token_id = scorer_tokenizer.eos_token_id\n",
    "\n",
    "# explainer is the same model as the scorer\n",
    "explainer_device = scorer_device\n",
    "explainer = scorer\n",
    "explainer_tokenizer = scorer_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\n",
      "\n",
      "For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\n",
      "\n",
      "The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\n",
      "\n",
      "Neuron 1\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation: fruits\n",
      "\n",
      "Neuron 2\n",
      "<PROMPT>Once</PROMPT>\n",
      "Most increased tokens: ' upon' (+0.22), ' in' (+0.2), ' a' (+0.05), ' long' (+0.04)\n",
      "\n",
      "<PROMPT>Ryan Quarles\\n\\nRyan Francis Quarles (born October 20, 1983)</PROMPT>\n",
      "Most increased tokens: ' once' (+0.03), ' happily' (+0.31), ' for' (+0.01)\n",
      "\n",
      "<PROMPT>MSI Going Full Throttle @ CeBIT</PROMPT>\n",
      "Most increased tokens: ' Once' (+0.02), ' once' (+0.01), ' in' (+0.01), ' the' (+0.01), ' a' (+0.01), ' The' (+0.01)\n",
      "\n",
      "Explanation: storytelling\n",
      "\n",
      "Neuron 3\n",
      "<PROMPT>He owned the watch for a long time. While he never said it was</PROMPT>\n",
      "Most increased tokens: ' hers' (+0.09), ' hers' (+0.06), ' hers' (+0.06)\n",
      "\n",
      "<PROMPT>For some reason</PROMPT>\n",
      "Most increased tokens: ' she' (+0.14), ' her' (+0.01), ' hers' (+0.01)\n",
      "\n",
      "<PROMPT>insurance does not cover</PROMPT>\n",
      "Most increased tokens: ' her' (+0.1), ' women' (+0.02), ' her's' (+0.01)\n",
      "\n",
      "Explanation: she/her pronouns\n",
      "\n",
      "Neuron 4\n",
      "<PROMPT>My favorite food is</PROMPT>\n",
      "Most increased tokens: ' oranges' (+0.81), ' bananas' (+0.09), ' apples' (+0.02)\n",
      "\n",
      "<PROMPT>Whenever I would see</PROMPT>\n",
      "Most increased tokens: ' fruit' (+0.09), ' a' (+0.06), ' apples' (+0.06), ' red' (+0.05)\n",
      "\n",
      "<PROMPT>I like to eat</PROMPT>\n",
      "Most increased tokens: ' fro' (+0.14), ' fruit' (+0.13), ' oranges' (+0.11), ' bananas' (+0.1), ' strawberries' (+0.03)\n",
      "\n",
      "Explanation:\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "@dataclass\n",
    "class ExplainerInterventionExample:\n",
    "    prompt: str\n",
    "    top_tokens: list[str]\n",
    "    top_p_increases: list[float]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.prompt = self.prompt.replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "    def text(self) -> str:\n",
    "        tokens_str = \", \".join(f\"'{tok}' (+{round(p, 3)})\" for tok, p in zip(self.top_tokens, self.top_p_increases))\n",
    "        return f\"<PROMPT>{self.prompt}</PROMPT>\\nMost increased tokens: {tokens_str}\"\n",
    "    \n",
    "@dataclass\n",
    "class ExplainerNeuronFormatter:\n",
    "    intervention_examples: list[ExplainerInterventionExample]\n",
    "    explanation: str | None = None\n",
    "\n",
    "    def text(self) -> str:\n",
    "        text = \"\\n\\n\".join(example.text() for example in self.intervention_examples)\n",
    "        text += \"\\n\\nExplanation:\"\n",
    "        if self.explanation is not None:\n",
    "            text += \" \" + self.explanation\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_explainer_prompt(neuron_prompter: ExplainerNeuronFormatter, few_shot_examples: list[ExplainerNeuronFormatter] | None = None) -> str:\n",
    "    prompt = \"We're studying neurons in a transformer model. We want to know how intervening on them affects the model's output.\\n\\n\" \\\n",
    "        \"For each neuron, we'll show you a few prompts where we intervened on that neuron at the final token position, and the tokens whose logits increased the most.\\n\\n\" \\\n",
    "        \"The tokens are shown in descending order of their probability increase, given in parentheses. Your job is to give a short summary of what outputs the neuron promotes.\\n\\n\"\n",
    "    \n",
    "    i = 1\n",
    "    for few_shot_example in few_shot_examples or []:\n",
    "        assert few_shot_example.explanation is not None\n",
    "        prompt += f\"Neuron {i}\\n\" + few_shot_example.text() + \"\\n\\n\"\n",
    "        i += 1\n",
    "\n",
    "    prompt += f\"Neuron {i}\\n\"\n",
    "    prompt += neuron_prompter.text()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "fs_examples = [\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"My favorite food is\",\n",
    "                top_tokens=[\" oranges\", \" bananas\", \" apples\"],\n",
    "                top_p_increases=[0.81, 0.09, 0.02]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Whenever I would see\",\n",
    "                top_tokens=[\" fruit\", \" a\", \" apples\", \" red\"],\n",
    "                top_p_increases=[0.09, 0.06, 0.06, 0.05]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"I like to eat\",\n",
    "                top_tokens=[\" fro\", \" fruit\", \" oranges\", \" bananas\", \" strawberries\"],\n",
    "                top_p_increases=[0.14, 0.13, 0.11, 0.10, 0.03]\n",
    "            )\n",
    "        ],\n",
    "        explanation=\"fruits\"\n",
    "    ),\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Once\",\n",
    "                top_tokens=[\" upon\", \" in\", \" a\", \" long\"],\n",
    "                top_p_increases=[0.22, 0.2, 0.05, 0.04]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"Ryan Quarles\\\\n\\\\nRyan Francis Quarles (born October 20, 1983)\",\n",
    "                top_tokens=[\" once\", \" happily\", \" for\"],\n",
    "                top_p_increases=[0.03, 0.31, 0.01]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"MSI Going Full Throttle @ CeBIT\",\n",
    "                top_tokens=[\" Once\", \" once\", \" in\", \" the\", \" a\", \" The\"],\n",
    "                top_p_increases=[0.02, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "            ),\n",
    "        ],\n",
    "        explanation=\"storytelling\"\n",
    "    ),\n",
    "    ExplainerNeuronFormatter(\n",
    "        intervention_examples=[\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"He owned the watch for a long time. While he never said it was\",\n",
    "                top_tokens=[\" hers\", \" hers\", \" hers\"],\n",
    "                top_p_increases=[0.09, 0.06, 0.06, 0.5]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"For some reason\",\n",
    "                top_tokens=[\" she\", \" her\", \" hers\"],\n",
    "                top_p_increases=[0.14, 0.01, 0.01]\n",
    "            ),\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=\"insurance does not cover\",\n",
    "                top_tokens=[\" her\", \" women\", \" her's\"],\n",
    "                top_p_increases=[0.10, 0.02, 0.01]\n",
    "            )\n",
    "        ],\n",
    "        explanation=\"she/her pronouns\"\n",
    "    )\n",
    "]\n",
    "\n",
    "neuron_prompter = copy.deepcopy(fs_examples[0])\n",
    "neuron_prompter.explanation = None\n",
    "print(get_explainer_prompt(neuron_prompter, fs_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT> oranges\n",
      "\n",
      "Explanation: ateg\n",
      "<PROMPT>From west to east, the westmost of the seven</PROMPT>WAY\n",
      "\n",
      "Explanation: she/her pronouns\n",
      "<PROMPT>He owned the watch for a long time. While he never said it was</PROMPT> hers\n",
      "\n",
      "Explanation: fruits and vegetables\n",
      "<PROMPT>My favorite food is</PROMPT>\n"
     ]
    }
   ],
   "source": [
    "def get_scorer_simplicity_prompt(explanation):\n",
    "    prefix = \"Explanation\\n\\n\"\n",
    "    return f\"{prefix}{explanation}{scorer_tokenizer.eos_token}\", prefix\n",
    "\n",
    "def get_scorer_predictiveness_prompt(prompt, explanation, few_shot_prompts=None, few_shot_explanations=None, few_shot_tokens=None):\n",
    "    if few_shot_explanations is not None:\n",
    "        assert few_shot_tokens is not None and few_shot_prompts is not None\n",
    "        assert len(few_shot_explanations) == len(few_shot_tokens) == len(few_shot_prompts)\n",
    "        few_shot_prompt = \"\\n\\n\".join(get_scorer_predictiveness_prompt(pr, expl) + token for pr, expl, token in zip(few_shot_prompts, few_shot_explanations, few_shot_tokens)) + \"\\n\\n\"\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    return few_shot_prompt + f\"Explanation: {explanation}\\n<PROMPT>{prompt}</PROMPT>\"\n",
    "\n",
    "few_shot_prompts = [\"My favorite food is\", \"From west to east, the westmost of the seven\", \"He owned the watch for a long time. While he never said it was\"]\n",
    "few_shot_explanations = [\"fruits and vegetables\", \"ateg\", \"she/her pronouns\"]\n",
    "few_shot_tokens = [\" oranges\", \"WAY\", \" hers\"]\n",
    "print(get_scorer_predictiveness_prompt(few_shot_prompts[0], few_shot_explanations[0], few_shot_prompts, few_shot_explanations, few_shot_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def intervene(module, input, output, intervention_strength=10.0, position=-1):\n",
    "    hiddens = output[0]  # the later elements of the tuple are the key value cache\n",
    "    hiddens[:, position, :] += intervention_strength * feat.to(hiddens.device)\n",
    "\n",
    "def get_texts(n, seed=42, randomize_length=True):\n",
    "    random.seed(seed)\n",
    "    texts = []\n",
    "    for _ in range(n):\n",
    "        \n",
    "        # sample a random text from the pile, and stop it at a random token position, less than 64 tokens\n",
    "        text = random.choice(pile)[\"text\"]\n",
    "        tokenized_text = subject_tokenizer.encode(text, add_special_tokens=False, max_length=64, truncation=True)\n",
    "        if len(tokenized_text) < 1:\n",
    "            continue\n",
    "        if randomize_length:\n",
    "            stop_pos = random.randint(1, min(len(tokenized_text), 63))\n",
    "        else:\n",
    "            stop_pos = 63\n",
    "        text = subject_tokenizer.decode(tokenized_text[:stop_pos])\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "n_explainer_texts = 3\n",
    "n_scorer_texts = 10\n",
    "# explainer_texts = get_texts(n_explainer_texts)\n",
    "# explainer_texts = [\"Current religion:\", \"A country that is\", \"Many people believe that\"]\n",
    "# scorer_texts = get_texts(n_scorer_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_vocab = scorer_tokenizer.get_vocab()\n",
    "subject_vocab = subject_tokenizer.get_vocab()\n",
    "\n",
    "# Pre-compute the mapping of subject tokens to scorer tokens\n",
    "subject_to_scorer = {}\n",
    "text_subject_to_scorer = {}\n",
    "for subj_tok, subj_id in subject_vocab.items():\n",
    "    if subj_tok in scorer_vocab:\n",
    "        subject_to_scorer[subj_id] = scorer_vocab[subj_tok]\n",
    "        text_subject_to_scorer[subj_tok] = subj_tok\n",
    "    else:\n",
    "        for i in range(len(subj_tok) - 1, 0, -1):\n",
    "            if subj_tok[:i] in scorer_vocab:\n",
    "                subject_to_scorer[subj_id] = scorer_vocab[subj_tok[:i]]\n",
    "                text_subject_to_scorer[subj_tok] = subj_tok[:i]\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No scorer token found for {subj_tok}\")\n",
    "subject_ids = torch.tensor(list(subject_to_scorer.keys()), device=scorer_device)\n",
    "scorer_ids = torch.tensor(list(subject_to_scorer.values()), device=scorer_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_intervention_examples = 5\n",
    "n_candidate_texts = 1_000\n",
    "candidate_texts = get_texts(n_candidate_texts, randomize_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acrocercops telestis\\n\\nAcrocercops telestis is a moth of the family Gracillariidae. It is known from India (Bihar).\\n\\nThe larvae feed on Mallotus repandus, Trewia species (including Trewia nudiflor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2568.3259,   51.8801,   56.1855,   56.7833,   56.3197,   73.1205,\n",
       "          58.5472,   55.7751,   56.7515,   54.8194,   76.8825,   59.6489,\n",
       "          56.3214,   57.5037,   58.4638,   71.8091,   61.4631,   58.9519,\n",
       "          56.8281,   59.7283,   72.9409,   56.1726,   55.4350,   64.0503,\n",
       "          81.6632,   59.4288,   59.9517,   74.9668,   55.2861,   61.9393,\n",
       "          59.8316,   67.2749,   57.4120,   72.5435,   58.4150,   64.5726,\n",
       "          69.5647,   58.5335,   52.1635,   51.9985,   61.7360,   83.2534,\n",
       "          74.2880,   57.7624,   75.1908,   59.3623,   60.8752,   66.7576,\n",
       "          57.9630,   58.7110,   54.8738,   63.6373,   59.0941,   58.1155,\n",
       "          68.2508,   60.3722,   69.6522,   66.7617,   59.5492,   60.7646,\n",
       "          78.1720,   63.0341,   65.7444], device='cuda:1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36/1000 [00:00<00:05, 174.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=167\n",
      "(feat_acts > 20).sum().item()=944\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=187\n",
      "(feat_acts > 20).sum().item()=496\n",
      "(feat_acts > 20).sum().item()=282\n",
      "(feat_acts > 20).sum().item()=97\n",
      "(feat_acts > 20).sum().item()=127\n",
      "(feat_acts > 20).sum().item()=682\n",
      "(feat_acts > 20).sum().item()=187\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=189\n",
      "(feat_acts > 20).sum().item()=46\n",
      "(feat_acts > 20).sum().item()=79\n",
      "(feat_acts > 20).sum().item()=167\n",
      "(feat_acts > 20).sum().item()=876\n",
      "(feat_acts > 20).sum().item()=332\n",
      "(feat_acts > 20).sum().item()=143\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=69\n",
      "(feat_acts > 20).sum().item()=62\n",
      "(feat_acts > 20).sum().item()=103\n",
      "(feat_acts > 20).sum().item()=117\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=171\n",
      "(feat_acts > 20).sum().item()=102\n",
      "(feat_acts > 20).sum().item()=171\n",
      "(feat_acts > 20).sum().item()=331\n",
      "(feat_acts > 20).sum().item()=1673\n",
      "(feat_acts > 20).sum().item()=78\n",
      "(feat_acts > 20).sum().item()=839\n",
      "(feat_acts > 20).sum().item()=258\n",
      "(feat_acts > 20).sum().item()=468\n",
      "(feat_acts > 20).sum().item()=222\n",
      "(feat_acts > 20).sum().item()=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 72/1000 [00:00<00:05, 174.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=179\n",
      "(feat_acts > 20).sum().item()=118\n",
      "(feat_acts > 20).sum().item()=151\n",
      "(feat_acts > 20).sum().item()=270\n",
      "(feat_acts > 20).sum().item()=163\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=207\n",
      "(feat_acts > 20).sum().item()=31\n",
      "(feat_acts > 20).sum().item()=217\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=364\n",
      "(feat_acts > 20).sum().item()=27\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=183\n",
      "(feat_acts > 20).sum().item()=66\n",
      "(feat_acts > 20).sum().item()=93\n",
      "(feat_acts > 20).sum().item()=104\n",
      "(feat_acts > 20).sum().item()=594\n",
      "(feat_acts > 20).sum().item()=76\n",
      "(feat_acts > 20).sum().item()=3\n",
      "(feat_acts > 20).sum().item()=117\n",
      "(feat_acts > 20).sum().item()=131\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=102\n",
      "(feat_acts > 20).sum().item()=167\n",
      "(feat_acts > 20).sum().item()=156\n",
      "(feat_acts > 20).sum().item()=453\n",
      "(feat_acts > 20).sum().item()=135\n",
      "(feat_acts > 20).sum().item()=60\n",
      "(feat_acts > 20).sum().item()=87\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 90/1000 [00:00<00:05, 174.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=10\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=224\n",
      "(feat_acts > 20).sum().item()=118\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=609\n",
      "(feat_acts > 20).sum().item()=450\n",
      "(feat_acts > 20).sum().item()=252\n",
      "(feat_acts > 20).sum().item()=256\n",
      "(feat_acts > 20).sum().item()=41\n",
      "(feat_acts > 20).sum().item()=431\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=117\n",
      "(feat_acts > 20).sum().item()=264\n",
      "(feat_acts > 20).sum().item()=94\n",
      "(feat_acts > 20).sum().item()=149\n",
      "(feat_acts > 20).sum().item()=215\n",
      "(feat_acts > 20).sum().item()=4\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=265\n",
      "(feat_acts > 20).sum().item()=66\n",
      "(feat_acts > 20).sum().item()=146\n",
      "(feat_acts > 20).sum().item()=91\n",
      "(feat_acts > 20).sum().item()=146\n",
      "(feat_acts > 20).sum().item()=2605\n",
      "(feat_acts > 20).sum().item()=828\n",
      "(feat_acts > 20).sum().item()=580\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=113\n",
      "(feat_acts > 20).sum().item()=160\n",
      "(feat_acts > 20).sum().item()=597\n",
      "(feat_acts > 20).sum().item()=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 126/1000 [00:00<00:05, 174.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=437\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=240\n",
      "(feat_acts > 20).sum().item()=135\n",
      "(feat_acts > 20).sum().item()=24\n",
      "(feat_acts > 20).sum().item()=46\n",
      "(feat_acts > 20).sum().item()=40\n",
      "(feat_acts > 20).sum().item()=53\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=164\n",
      "(feat_acts > 20).sum().item()=141\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=489\n",
      "(feat_acts > 20).sum().item()=149\n",
      "(feat_acts > 20).sum().item()=112\n",
      "(feat_acts > 20).sum().item()=15\n",
      "(feat_acts > 20).sum().item()=416\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=39\n",
      "(feat_acts > 20).sum().item()=275\n",
      "(feat_acts > 20).sum().item()=282\n",
      "(feat_acts > 20).sum().item()=484\n",
      "(feat_acts > 20).sum().item()=92\n",
      "(feat_acts > 20).sum().item()=35\n",
      "(feat_acts > 20).sum().item()=190\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=86\n",
      "(feat_acts > 20).sum().item()=81\n",
      "(feat_acts > 20).sum().item()=131\n",
      "(feat_acts > 20).sum().item()=267\n",
      "(feat_acts > 20).sum().item()=211\n",
      "(feat_acts > 20).sum().item()=221\n",
      "(feat_acts > 20).sum().item()=157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 162/1000 [00:00<00:04, 174.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=143\n",
      "(feat_acts > 20).sum().item()=45\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=201\n",
      "(feat_acts > 20).sum().item()=280\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=34\n",
      "(feat_acts > 20).sum().item()=604\n",
      "(feat_acts > 20).sum().item()=308\n",
      "(feat_acts > 20).sum().item()=84\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=334\n",
      "(feat_acts > 20).sum().item()=874\n",
      "(feat_acts > 20).sum().item()=143\n",
      "(feat_acts > 20).sum().item()=4\n",
      "(feat_acts > 20).sum().item()=61\n",
      "(feat_acts > 20).sum().item()=310\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=155\n",
      "(feat_acts > 20).sum().item()=254\n",
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=153\n",
      "(feat_acts > 20).sum().item()=392\n",
      "(feat_acts > 20).sum().item()=53\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=567\n",
      "(feat_acts > 20).sum().item()=24\n",
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=112\n",
      "(feat_acts > 20).sum().item()=305\n",
      "(feat_acts > 20).sum().item()=34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 198/1000 [00:01<00:04, 174.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=113\n",
      "(feat_acts > 20).sum().item()=164\n",
      "(feat_acts > 20).sum().item()=248\n",
      "(feat_acts > 20).sum().item()=105\n",
      "(feat_acts > 20).sum().item()=436\n",
      "(feat_acts > 20).sum().item()=686\n",
      "(feat_acts > 20).sum().item()=179\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=149\n",
      "(feat_acts > 20).sum().item()=108\n",
      "(feat_acts > 20).sum().item()=10\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=150\n",
      "(feat_acts > 20).sum().item()=949\n",
      "(feat_acts > 20).sum().item()=92\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=41\n",
      "(feat_acts > 20).sum().item()=98\n",
      "(feat_acts > 20).sum().item()=103\n",
      "(feat_acts > 20).sum().item()=81\n",
      "(feat_acts > 20).sum().item()=46\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=271\n",
      "(feat_acts > 20).sum().item()=92\n",
      "(feat_acts > 20).sum().item()=125\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=43\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=73\n",
      "(feat_acts > 20).sum().item()=139\n",
      "(feat_acts > 20).sum().item()=367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 234/1000 [00:01<00:04, 174.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=378\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=145\n",
      "(feat_acts > 20).sum().item()=272\n",
      "(feat_acts > 20).sum().item()=483\n",
      "(feat_acts > 20).sum().item()=56\n",
      "(feat_acts > 20).sum().item()=268\n",
      "(feat_acts > 20).sum().item()=158\n",
      "(feat_acts > 20).sum().item()=176\n",
      "(feat_acts > 20).sum().item()=204\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=34\n",
      "(feat_acts > 20).sum().item()=73\n",
      "(feat_acts > 20).sum().item()=250\n",
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=70\n",
      "(feat_acts > 20).sum().item()=98\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=145\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=40\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=320\n",
      "(feat_acts > 20).sum().item()=275\n",
      "(feat_acts > 20).sum().item()=68\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=182\n",
      "(feat_acts > 20).sum().item()=732\n",
      "(feat_acts > 20).sum().item()=143\n",
      "(feat_acts > 20).sum().item()=224\n",
      "(feat_acts > 20).sum().item()=463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 270/1000 [00:01<00:04, 174.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=391\n",
      "(feat_acts > 20).sum().item()=179\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=196\n",
      "(feat_acts > 20).sum().item()=400\n",
      "(feat_acts > 20).sum().item()=84\n",
      "(feat_acts > 20).sum().item()=177\n",
      "(feat_acts > 20).sum().item()=151\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=806\n",
      "(feat_acts > 20).sum().item()=178\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=176\n",
      "(feat_acts > 20).sum().item()=79\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=253\n",
      "(feat_acts > 20).sum().item()=258\n",
      "(feat_acts > 20).sum().item()=36\n",
      "(feat_acts > 20).sum().item()=271\n",
      "(feat_acts > 20).sum().item()=97\n",
      "(feat_acts > 20).sum().item()=243\n",
      "(feat_acts > 20).sum().item()=134\n",
      "(feat_acts > 20).sum().item()=460\n",
      "(feat_acts > 20).sum().item()=206\n",
      "(feat_acts > 20).sum().item()=561\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=376\n",
      "(feat_acts > 20).sum().item()=62\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=57\n",
      "(feat_acts > 20).sum().item()=224\n",
      "(feat_acts > 20).sum().item()=60\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 306/1000 [00:01<00:03, 174.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=168\n",
      "(feat_acts > 20).sum().item()=100\n",
      "(feat_acts > 20).sum().item()=378\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=111\n",
      "(feat_acts > 20).sum().item()=105\n",
      "(feat_acts > 20).sum().item()=88\n",
      "(feat_acts > 20).sum().item()=300\n",
      "(feat_acts > 20).sum().item()=47\n",
      "(feat_acts > 20).sum().item()=76\n",
      "(feat_acts > 20).sum().item()=49\n",
      "(feat_acts > 20).sum().item()=332\n",
      "(feat_acts > 20).sum().item()=173\n",
      "(feat_acts > 20).sum().item()=254\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=45\n",
      "(feat_acts > 20).sum().item()=117\n",
      "(feat_acts > 20).sum().item()=94\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=402\n",
      "(feat_acts > 20).sum().item()=35\n",
      "(feat_acts > 20).sum().item()=302\n",
      "(feat_acts > 20).sum().item()=255\n",
      "(feat_acts > 20).sum().item()=168\n",
      "(feat_acts > 20).sum().item()=137\n",
      "(feat_acts > 20).sum().item()=74\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=249\n",
      "(feat_acts > 20).sum().item()=39\n",
      "(feat_acts > 20).sum().item()=131\n",
      "(feat_acts > 20).sum().item()=805\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 342/1000 [00:01<00:03, 173.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=159\n",
      "(feat_acts > 20).sum().item()=69\n",
      "(feat_acts > 20).sum().item()=277\n",
      "(feat_acts > 20).sum().item()=177\n",
      "(feat_acts > 20).sum().item()=47\n",
      "(feat_acts > 20).sum().item()=363\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=65\n",
      "(feat_acts > 20).sum().item()=144\n",
      "(feat_acts > 20).sum().item()=153\n",
      "(feat_acts > 20).sum().item()=15\n",
      "(feat_acts > 20).sum().item()=74\n",
      "(feat_acts > 20).sum().item()=97\n",
      "(feat_acts > 20).sum().item()=1192\n",
      "(feat_acts > 20).sum().item()=334\n",
      "(feat_acts > 20).sum().item()=308\n",
      "(feat_acts > 20).sum().item()=144\n",
      "(feat_acts > 20).sum().item()=37\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=221\n",
      "(feat_acts > 20).sum().item()=99\n",
      "(feat_acts > 20).sum().item()=147\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=346\n",
      "(feat_acts > 20).sum().item()=1101\n",
      "(feat_acts > 20).sum().item()=49\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=44\n",
      "(feat_acts > 20).sum().item()=35\n",
      "(feat_acts > 20).sum().item()=396\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=57\n",
      "(feat_acts > 20).sum().item()=157\n",
      "(feat_acts > 20).sum().item()=241\n",
      "(feat_acts > 20).sum().item()=168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 378/1000 [00:02<00:03, 174.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=218\n",
      "(feat_acts > 20).sum().item()=43\n",
      "(feat_acts > 20).sum().item()=159\n",
      "(feat_acts > 20).sum().item()=138\n",
      "(feat_acts > 20).sum().item()=120\n",
      "(feat_acts > 20).sum().item()=70\n",
      "(feat_acts > 20).sum().item()=175\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=283\n",
      "(feat_acts > 20).sum().item()=251\n",
      "(feat_acts > 20).sum().item()=94\n",
      "(feat_acts > 20).sum().item()=315\n",
      "(feat_acts > 20).sum().item()=145\n",
      "(feat_acts > 20).sum().item()=43\n",
      "(feat_acts > 20).sum().item()=275\n",
      "(feat_acts > 20).sum().item()=522\n",
      "(feat_acts > 20).sum().item()=43\n",
      "(feat_acts > 20).sum().item()=36\n",
      "(feat_acts > 20).sum().item()=271\n",
      "(feat_acts > 20).sum().item()=126\n",
      "(feat_acts > 20).sum().item()=130\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=61\n",
      "(feat_acts > 20).sum().item()=44\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=236\n",
      "(feat_acts > 20).sum().item()=201\n",
      "(feat_acts > 20).sum().item()=115\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=134\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=454\n",
      "(feat_acts > 20).sum().item()=98\n",
      "(feat_acts > 20).sum().item()=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 414/1000 [00:02<00:03, 173.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=55\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=859\n",
      "(feat_acts > 20).sum().item()=569\n",
      "(feat_acts > 20).sum().item()=212\n",
      "(feat_acts > 20).sum().item()=52\n",
      "(feat_acts > 20).sum().item()=86\n",
      "(feat_acts > 20).sum().item()=517\n",
      "(feat_acts > 20).sum().item()=236\n",
      "(feat_acts > 20).sum().item()=61\n",
      "(feat_acts > 20).sum().item()=85\n",
      "(feat_acts > 20).sum().item()=65\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=150\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=91\n",
      "(feat_acts > 20).sum().item()=94\n",
      "(feat_acts > 20).sum().item()=42\n",
      "(feat_acts > 20).sum().item()=286\n",
      "(feat_acts > 20).sum().item()=310\n",
      "(feat_acts > 20).sum().item()=125\n",
      "(feat_acts > 20).sum().item()=699\n",
      "(feat_acts > 20).sum().item()=99\n",
      "(feat_acts > 20).sum().item()=427\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=63\n",
      "(feat_acts > 20).sum().item()=176\n",
      "(feat_acts > 20).sum().item()=236\n",
      "(feat_acts > 20).sum().item()=120\n",
      "(feat_acts > 20).sum().item()=213\n",
      "(feat_acts > 20).sum().item()=276\n",
      "(feat_acts > 20).sum().item()=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450/1000 [00:02<00:03, 170.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=162\n",
      "(feat_acts > 20).sum().item()=646\n",
      "(feat_acts > 20).sum().item()=388\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=224\n",
      "(feat_acts > 20).sum().item()=4\n",
      "(feat_acts > 20).sum().item()=116\n",
      "(feat_acts > 20).sum().item()=227\n",
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=250\n",
      "(feat_acts > 20).sum().item()=85\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=512\n",
      "(feat_acts > 20).sum().item()=100\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=428\n",
      "(feat_acts > 20).sum().item()=276\n",
      "(feat_acts > 20).sum().item()=61\n",
      "(feat_acts > 20).sum().item()=85\n",
      "(feat_acts > 20).sum().item()=179\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=168\n",
      "(feat_acts > 20).sum().item()=572\n",
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=82\n",
      "(feat_acts > 20).sum().item()=73\n",
      "(feat_acts > 20).sum().item()=87\n",
      "(feat_acts > 20).sum().item()=287\n",
      "(feat_acts > 20).sum().item()=185\n",
      "(feat_acts > 20).sum().item()=336\n",
      "(feat_acts > 20).sum().item()=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 486/1000 [00:02<00:03, 170.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=153\n",
      "(feat_acts > 20).sum().item()=45\n",
      "(feat_acts > 20).sum().item()=616\n",
      "(feat_acts > 20).sum().item()=156\n",
      "(feat_acts > 20).sum().item()=62\n",
      "(feat_acts > 20).sum().item()=338\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=258\n",
      "(feat_acts > 20).sum().item()=178\n",
      "(feat_acts > 20).sum().item()=331\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=170\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=363\n",
      "(feat_acts > 20).sum().item()=100\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=38\n",
      "(feat_acts > 20).sum().item()=139\n",
      "(feat_acts > 20).sum().item()=327\n",
      "(feat_acts > 20).sum().item()=177\n",
      "(feat_acts > 20).sum().item()=147\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=180\n",
      "(feat_acts > 20).sum().item()=115\n",
      "(feat_acts > 20).sum().item()=49\n",
      "(feat_acts > 20).sum().item()=864\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=53\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=676\n",
      "(feat_acts > 20).sum().item()=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 522/1000 [00:03<00:02, 172.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=99\n",
      "(feat_acts > 20).sum().item()=220\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=150\n",
      "(feat_acts > 20).sum().item()=223\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=170\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=86\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=913\n",
      "(feat_acts > 20).sum().item()=138\n",
      "(feat_acts > 20).sum().item()=165\n",
      "(feat_acts > 20).sum().item()=101\n",
      "(feat_acts > 20).sum().item()=186\n",
      "(feat_acts > 20).sum().item()=27\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=260\n",
      "(feat_acts > 20).sum().item()=162\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=176\n",
      "(feat_acts > 20).sum().item()=616\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=246\n",
      "(feat_acts > 20).sum().item()=1847\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=323\n",
      "(feat_acts > 20).sum().item()=119\n",
      "(feat_acts > 20).sum().item()=283\n",
      "(feat_acts > 20).sum().item()=314\n",
      "(feat_acts > 20).sum().item()=44\n",
      "(feat_acts > 20).sum().item()=57\n",
      "(feat_acts > 20).sum().item()=62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 558/1000 [00:03<00:02, 170.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=113\n",
      "(feat_acts > 20).sum().item()=270\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=217\n",
      "(feat_acts > 20).sum().item()=458\n",
      "(feat_acts > 20).sum().item()=106\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=945\n",
      "(feat_acts > 20).sum().item()=256\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=227\n",
      "(feat_acts > 20).sum().item()=1051\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=70\n",
      "(feat_acts > 20).sum().item()=25\n",
      "(feat_acts > 20).sum().item()=96\n",
      "(feat_acts > 20).sum().item()=208\n",
      "(feat_acts > 20).sum().item()=222\n",
      "(feat_acts > 20).sum().item()=410\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=557\n",
      "(feat_acts > 20).sum().item()=444\n",
      "(feat_acts > 20).sum().item()=127\n",
      "(feat_acts > 20).sum().item()=123\n",
      "(feat_acts > 20).sum().item()=213\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=794\n",
      "(feat_acts > 20).sum().item()=93\n",
      "(feat_acts > 20).sum().item()=177\n",
      "(feat_acts > 20).sum().item()=45\n",
      "(feat_acts > 20).sum().item()=1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 594/1000 [00:03<00:02, 171.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=353\n",
      "(feat_acts > 20).sum().item()=83\n",
      "(feat_acts > 20).sum().item()=284\n",
      "(feat_acts > 20).sum().item()=27\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=192\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=276\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=246\n",
      "(feat_acts > 20).sum().item()=1201\n",
      "(feat_acts > 20).sum().item()=138\n",
      "(feat_acts > 20).sum().item()=39\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=506\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=220\n",
      "(feat_acts > 20).sum().item()=170\n",
      "(feat_acts > 20).sum().item()=119\n",
      "(feat_acts > 20).sum().item()=34\n",
      "(feat_acts > 20).sum().item()=57\n",
      "(feat_acts > 20).sum().item()=61\n",
      "(feat_acts > 20).sum().item()=45\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=440\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=246\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=197\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=152\n",
      "(feat_acts > 20).sum().item()=18\n",
      "(feat_acts > 20).sum().item()=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 630/1000 [00:03<00:02, 171.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=264\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=55\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=27\n",
      "(feat_acts > 20).sum().item()=141\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=169\n",
      "(feat_acts > 20).sum().item()=110\n",
      "(feat_acts > 20).sum().item()=65\n",
      "(feat_acts > 20).sum().item()=66\n",
      "(feat_acts > 20).sum().item()=375\n",
      "(feat_acts > 20).sum().item()=262\n",
      "(feat_acts > 20).sum().item()=83\n",
      "(feat_acts > 20).sum().item()=70\n",
      "(feat_acts > 20).sum().item()=274\n",
      "(feat_acts > 20).sum().item()=121\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=125\n",
      "(feat_acts > 20).sum().item()=144\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=161\n",
      "(feat_acts > 20).sum().item()=254\n",
      "(feat_acts > 20).sum().item()=284\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=142\n",
      "(feat_acts > 20).sum().item()=257\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=49\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=308\n",
      "(feat_acts > 20).sum().item()=112\n",
      "(feat_acts > 20).sum().item()=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 666/1000 [00:03<00:01, 168.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=385\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=223\n",
      "(feat_acts > 20).sum().item()=304\n",
      "(feat_acts > 20).sum().item()=76\n",
      "(feat_acts > 20).sum().item()=83\n",
      "(feat_acts > 20).sum().item()=256\n",
      "(feat_acts > 20).sum().item()=135\n",
      "(feat_acts > 20).sum().item()=25\n",
      "(feat_acts > 20).sum().item()=383\n",
      "(feat_acts > 20).sum().item()=92\n",
      "(feat_acts > 20).sum().item()=234\n",
      "(feat_acts > 20).sum().item()=227\n",
      "(feat_acts > 20).sum().item()=84\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=127\n",
      "(feat_acts > 20).sum().item()=141\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=327\n",
      "(feat_acts > 20).sum().item()=361\n",
      "(feat_acts > 20).sum().item()=153\n",
      "(feat_acts > 20).sum().item()=367\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=63\n",
      "(feat_acts > 20).sum().item()=191\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=1\n",
      "(feat_acts > 20).sum().item()=27\n",
      "(feat_acts > 20).sum().item()=428\n",
      "(feat_acts > 20).sum().item()=93\n",
      "(feat_acts > 20).sum().item()=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 702/1000 [00:04<00:01, 171.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=74\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=90\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=93\n",
      "(feat_acts > 20).sum().item()=252\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=278\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=497\n",
      "(feat_acts > 20).sum().item()=59\n",
      "(feat_acts > 20).sum().item()=56\n",
      "(feat_acts > 20).sum().item()=428\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=335\n",
      "(feat_acts > 20).sum().item()=396\n",
      "(feat_acts > 20).sum().item()=136\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=191\n",
      "(feat_acts > 20).sum().item()=121\n",
      "(feat_acts > 20).sum().item()=26\n",
      "(feat_acts > 20).sum().item()=63\n",
      "(feat_acts > 20).sum().item()=32\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=13\n",
      "(feat_acts > 20).sum().item()=189\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=186\n",
      "(feat_acts > 20).sum().item()=85\n",
      "(feat_acts > 20).sum().item()=91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 738/1000 [00:04<00:01, 172.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=183\n",
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=278\n",
      "(feat_acts > 20).sum().item()=238\n",
      "(feat_acts > 20).sum().item()=94\n",
      "(feat_acts > 20).sum().item()=70\n",
      "(feat_acts > 20).sum().item()=162\n",
      "(feat_acts > 20).sum().item()=292\n",
      "(feat_acts > 20).sum().item()=626\n",
      "(feat_acts > 20).sum().item()=201\n",
      "(feat_acts > 20).sum().item()=159\n",
      "(feat_acts > 20).sum().item()=190\n",
      "(feat_acts > 20).sum().item()=133\n",
      "(feat_acts > 20).sum().item()=49\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=6\n",
      "(feat_acts > 20).sum().item()=706\n",
      "(feat_acts > 20).sum().item()=178\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=78\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=69\n",
      "(feat_acts > 20).sum().item()=65\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=38\n",
      "(feat_acts > 20).sum().item()=1143\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=313\n",
      "(feat_acts > 20).sum().item()=122\n",
      "(feat_acts > 20).sum().item()=199\n",
      "(feat_acts > 20).sum().item()=21\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 774/1000 [00:04<00:01, 173.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=122\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=50\n",
      "(feat_acts > 20).sum().item()=52\n",
      "(feat_acts > 20).sum().item()=125\n",
      "(feat_acts > 20).sum().item()=14\n",
      "(feat_acts > 20).sum().item()=60\n",
      "(feat_acts > 20).sum().item()=736\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=611\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=31\n",
      "(feat_acts > 20).sum().item()=40\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=252\n",
      "(feat_acts > 20).sum().item()=158\n",
      "(feat_acts > 20).sum().item()=36\n",
      "(feat_acts > 20).sum().item()=395\n",
      "(feat_acts > 20).sum().item()=122\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=403\n",
      "(feat_acts > 20).sum().item()=319\n",
      "(feat_acts > 20).sum().item()=4\n",
      "(feat_acts > 20).sum().item()=288\n",
      "(feat_acts > 20).sum().item()=191\n",
      "(feat_acts > 20).sum().item()=8\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=35\n",
      "(feat_acts > 20).sum().item()=322\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=41\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=107\n",
      "(feat_acts > 20).sum().item()=662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 810/1000 [00:04<00:01, 169.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=457\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=15\n",
      "(feat_acts > 20).sum().item()=31\n",
      "(feat_acts > 20).sum().item()=96\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=100\n",
      "(feat_acts > 20).sum().item()=359\n",
      "(feat_acts > 20).sum().item()=60\n",
      "(feat_acts > 20).sum().item()=264\n",
      "(feat_acts > 20).sum().item()=521\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=412\n",
      "(feat_acts > 20).sum().item()=46\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=264\n",
      "(feat_acts > 20).sum().item()=106\n",
      "(feat_acts > 20).sum().item()=130\n",
      "(feat_acts > 20).sum().item()=186\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=150\n",
      "(feat_acts > 20).sum().item()=87\n",
      "(feat_acts > 20).sum().item()=87\n",
      "(feat_acts > 20).sum().item()=108\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=118\n",
      "(feat_acts > 20).sum().item()=382\n",
      "(feat_acts > 20).sum().item()=30\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=152\n",
      "(feat_acts > 20).sum().item()=53\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 845/1000 [00:04<00:00, 170.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=405\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=123\n",
      "(feat_acts > 20).sum().item()=156\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=155\n",
      "(feat_acts > 20).sum().item()=29\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=1827\n",
      "(feat_acts > 20).sum().item()=59\n",
      "(feat_acts > 20).sum().item()=284\n",
      "(feat_acts > 20).sum().item()=261\n",
      "(feat_acts > 20).sum().item()=23\n",
      "(feat_acts > 20).sum().item()=219\n",
      "(feat_acts > 20).sum().item()=235\n",
      "(feat_acts > 20).sum().item()=215\n",
      "(feat_acts > 20).sum().item()=138\n",
      "(feat_acts > 20).sum().item()=283\n",
      "(feat_acts > 20).sum().item()=182\n",
      "(feat_acts > 20).sum().item()=223\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=37\n",
      "(feat_acts > 20).sum().item()=829\n",
      "(feat_acts > 20).sum().item()=869\n",
      "(feat_acts > 20).sum().item()=131\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=231\n",
      "(feat_acts > 20).sum().item()=266\n",
      "(feat_acts > 20).sum().item()=44\n",
      "(feat_acts > 20).sum().item()=83\n",
      "(feat_acts > 20).sum().item()=63\n",
      "(feat_acts > 20).sum().item()=22\n",
      "(feat_acts > 20).sum().item()=96\n",
      "(feat_acts > 20).sum().item()=96\n",
      "(feat_acts > 20).sum().item()=79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 863/1000 [00:05<00:00, 171.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=529\n",
      "(feat_acts > 20).sum().item()=362\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=135\n",
      "(feat_acts > 20).sum().item()=1565\n",
      "(feat_acts > 20).sum().item()=97\n",
      "(feat_acts > 20).sum().item()=102\n",
      "(feat_acts > 20).sum().item()=77\n",
      "(feat_acts > 20).sum().item()=480\n",
      "(feat_acts > 20).sum().item()=99\n",
      "(feat_acts > 20).sum().item()=91\n",
      "(feat_acts > 20).sum().item()=509\n",
      "(feat_acts > 20).sum().item()=654\n",
      "(feat_acts > 20).sum().item()=4\n",
      "(feat_acts > 20).sum().item()=150\n",
      "(feat_acts > 20).sum().item()=15\n",
      "(feat_acts > 20).sum().item()=1\n",
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=20\n",
      "(feat_acts > 20).sum().item()=326\n",
      "(feat_acts > 20).sum().item()=78\n",
      "(feat_acts > 20).sum().item()=250\n",
      "(feat_acts > 20).sum().item()=57\n",
      "(feat_acts > 20).sum().item()=709\n",
      "(feat_acts > 20).sum().item()=3\n",
      "(feat_acts > 20).sum().item()=68\n",
      "(feat_acts > 20).sum().item()=163\n",
      "(feat_acts > 20).sum().item()=11\n",
      "(feat_acts > 20).sum().item()=96\n",
      "(feat_acts > 20).sum().item()=140\n",
      "(feat_acts > 20).sum().item()=203\n",
      "(feat_acts > 20).sum().item()=40\n",
      "(feat_acts > 20).sum().item()=381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 899/1000 [00:05<00:00, 171.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=135\n",
      "(feat_acts > 20).sum().item()=36\n",
      "(feat_acts > 20).sum().item()=390\n",
      "(feat_acts > 20).sum().item()=54\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=409\n",
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=502\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=52\n",
      "(feat_acts > 20).sum().item()=424\n",
      "(feat_acts > 20).sum().item()=117\n",
      "(feat_acts > 20).sum().item()=46\n",
      "(feat_acts > 20).sum().item()=105\n",
      "(feat_acts > 20).sum().item()=489\n",
      "(feat_acts > 20).sum().item()=85\n",
      "(feat_acts > 20).sum().item()=34\n",
      "(feat_acts > 20).sum().item()=251\n",
      "(feat_acts > 20).sum().item()=19\n",
      "(feat_acts > 20).sum().item()=330\n",
      "(feat_acts > 20).sum().item()=5\n",
      "(feat_acts > 20).sum().item()=201\n",
      "(feat_acts > 20).sum().item()=25\n",
      "(feat_acts > 20).sum().item()=25\n",
      "(feat_acts > 20).sum().item()=89\n",
      "(feat_acts > 20).sum().item()=33\n",
      "(feat_acts > 20).sum().item()=55\n",
      "(feat_acts > 20).sum().item()=9\n",
      "(feat_acts > 20).sum().item()=51\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=7\n",
      "(feat_acts > 20).sum().item()=143\n",
      "(feat_acts > 20).sum().item()=58\n",
      "(feat_acts > 20).sum().item()=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 935/1000 [00:05<00:00, 172.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=2\n",
      "(feat_acts > 20).sum().item()=101\n",
      "(feat_acts > 20).sum().item()=223\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=821\n",
      "(feat_acts > 20).sum().item()=315\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=240\n",
      "(feat_acts > 20).sum().item()=129\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=183\n",
      "(feat_acts > 20).sum().item()=108\n",
      "(feat_acts > 20).sum().item()=12\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=231\n",
      "(feat_acts > 20).sum().item()=213\n",
      "(feat_acts > 20).sum().item()=95\n",
      "(feat_acts > 20).sum().item()=64\n",
      "(feat_acts > 20).sum().item()=75\n",
      "(feat_acts > 20).sum().item()=196\n",
      "(feat_acts > 20).sum().item()=396\n",
      "(feat_acts > 20).sum().item()=104\n",
      "(feat_acts > 20).sum().item()=185\n",
      "(feat_acts > 20).sum().item()=52\n",
      "(feat_acts > 20).sum().item()=71\n",
      "(feat_acts > 20).sum().item()=43\n",
      "(feat_acts > 20).sum().item()=233\n",
      "(feat_acts > 20).sum().item()=62\n",
      "(feat_acts > 20).sum().item()=17\n",
      "(feat_acts > 20).sum().item()=53\n",
      "(feat_acts > 20).sum().item()=78\n",
      "(feat_acts > 20).sum().item()=3\n",
      "(feat_acts > 20).sum().item()=16\n",
      "(feat_acts > 20).sum().item()=63\n",
      "(feat_acts > 20).sum().item()=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 971/1000 [00:05<00:00, 172.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=79\n",
      "(feat_acts > 20).sum().item()=402\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=35\n",
      "(feat_acts > 20).sum().item()=194\n",
      "(feat_acts > 20).sum().item()=80\n",
      "(feat_acts > 20).sum().item()=15\n",
      "(feat_acts > 20).sum().item()=1570\n",
      "(feat_acts > 20).sum().item()=48\n",
      "(feat_acts > 20).sum().item()=152\n",
      "(feat_acts > 20).sum().item()=249\n",
      "(feat_acts > 20).sum().item()=76\n",
      "(feat_acts > 20).sum().item()=72\n",
      "(feat_acts > 20).sum().item()=429\n",
      "(feat_acts > 20).sum().item()=295\n",
      "(feat_acts > 20).sum().item()=66\n",
      "(feat_acts > 20).sum().item()=106\n",
      "(feat_acts > 20).sum().item()=112\n",
      "(feat_acts > 20).sum().item()=441\n",
      "(feat_acts > 20).sum().item()=154\n",
      "(feat_acts > 20).sum().item()=28\n",
      "(feat_acts > 20).sum().item()=298\n",
      "(feat_acts > 20).sum().item()=124\n",
      "(feat_acts > 20).sum().item()=24\n",
      "(feat_acts > 20).sum().item()=56\n",
      "(feat_acts > 20).sum().item()=469\n",
      "(feat_acts > 20).sum().item()=200\n",
      "(feat_acts > 20).sum().item()=213\n",
      "(feat_acts > 20).sum().item()=67\n",
      "(feat_acts > 20).sum().item()=152\n",
      "(feat_acts > 20).sum().item()=331\n",
      "(feat_acts > 20).sum().item()=91\n",
      "(feat_acts > 20).sum().item()=342\n",
      "(feat_acts > 20).sum().item()=73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 172.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(feat_acts > 20).sum().item()=93\n",
      "(feat_acts > 20).sum().item()=1149\n",
      "(feat_acts > 20).sum().item()=258\n",
      "(feat_acts > 20).sum().item()=299\n",
      "(feat_acts > 20).sum().item()=145\n",
      "(feat_acts > 20).sum().item()=68\n",
      "(feat_acts > 20).sum().item()=390\n",
      "(feat_acts > 20).sum().item()=165\n",
      "(feat_acts > 20).sum().item()=146\n",
      "(feat_acts > 20).sum().item()=161\n",
      "(feat_acts > 20).sum().item()=232\n",
      "(feat_acts > 20).sum().item()=148\n",
      "(feat_acts > 20).sum().item()=459\n",
      "(feat_acts > 20).sum().item()=321\n",
      "(feat_acts > 20).sum().item()=66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading autoencoder...\", end=\"\")\n",
    "path = f\"{weight_dir}/{layer}.pt\"\n",
    "state_dict = torch.load(path)\n",
    "ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "feat = ae.decoder.weight[:, feat_idx].to(subject_device)\n",
    "ae.encoder.to(subject_device)\n",
    "ae.activation.to(subject_device)\n",
    "\n",
    "### Find examples where the feature activates\n",
    "# Remove any hooks\n",
    "for l in range(len(subject.transformer.h)):\n",
    "    subject.transformer.h[l]._forward_hooks.clear()\n",
    "\n",
    "for text in tqdm(candidate_texts, total=len(candidate_texts)):\n",
    "    input_ids = subject_tokenizer(text, return_tensors=\"pt\").input_ids.to(subject_device)\n",
    "    with torch.inference_mode():\n",
    "        out = subject(input_ids, output_hidden_states=True)\n",
    "        # hidden_states is actually one longer than the number of layers, because it includes the input embeddings\n",
    "        h = out.hidden_states[layer + 1].squeeze(0)\n",
    "        feat_acts = ae.encoder(h[-1, :])\n",
    "        topk = ae.activation(feat_acts)\n",
    "        print(f\"{(feat_acts > 20).sum().item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "from sae_auto_interp.autoencoders.OpenAI.model import Autoencoder\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "weight_dir = \"/mnt/ssd-1/gpaulo/SAE-Zoology/weights/gpt2_128k\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "feat_idxs = list(range(10))\n",
    "feat_layers = [2, 6, 11]\n",
    "total_iterations = len(feat_idxs) * len(feat_layers)\n",
    "for feat_idx, feat_layer in tqdm(product(feat_idxs, feat_layers), total=total_iterations):\n",
    "    scorer_intervention_strengths = [10, 32, 100, 320, 1000]\n",
    "    explainer_intervention_strength = 32\n",
    "\n",
    "    print(\"Loading autoencoder...\", end=\"\")\n",
    "    path = f\"{weight_dir}/{layer}.pt\"\n",
    "    state_dict = torch.load(path)\n",
    "    ae = Autoencoder.from_state_dict(state_dict=state_dict)\n",
    "    feat = ae.decoder.weight[:, feat_idx].to(subject_device)\n",
    "    encoder_feat = ae.encoder.weight[feat_idx, :].to(subject_device)\n",
    "    # ae.encoder.to(device)\n",
    "    # ae.activation.to(device)\n",
    "\n",
    "    ### Find examples where the feature activates\n",
    "    # Remove any hooks\n",
    "    for l in range(len(subject.transformer.h)):\n",
    "        subject.transformer.h[l]._forward_hooks.clear()\n",
    "    print(\"done\")\n",
    "\n",
    "    subtexts = []\n",
    "    subtext_acts = []\n",
    "    for text in tqdm(candidate_texts, total=len(candidate_texts)):\n",
    "        input_ids = subject_tokenizer(text, return_tensors=\"pt\").input_ids.to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            out = subject(input_ids, output_hidden_states=True)\n",
    "            # hidden_states is actually one longer than the number of layers, because it includes the input embeddings\n",
    "            h = out.hidden_states[layer + 1].squeeze(0)\n",
    "            # feat_acts = ae.activation(ae.encoder(h))[:, feat_idx]\n",
    "            feat_acts = h @ encoder_feat\n",
    "            # the first token position just has way higher norm all the time for some reason\n",
    "            feat_acts[0] = 0\n",
    "\n",
    "        for i in range(1, len(feat_acts) + 1):\n",
    "            reassembled_text = subject_tokenizer.decode(input_ids[0, :i])\n",
    "            subtexts.append(reassembled_text)\n",
    "            subtext_acts.append(feat_acts[i - 1].item())\n",
    "\n",
    "    del ae\n",
    "    # get a random sample of activating contexts\n",
    "    subtext_acts = torch.tensor(subtext_acts)\n",
    "    n_candidates = 500\n",
    "    candidate_indices = subtext_acts.topk(n_candidates).indices\n",
    "    sampled_indices = np.random.choice(candidate_indices.numpy(), n_scorer_texts + n_explainer_texts, replace=False)\n",
    "    \n",
    "    # Get top k subtexts and their activations\n",
    "    sampled_subtexts = [subtexts[i] for i in sampled_indices]\n",
    "    sampled_activations = subtext_acts[sampled_indices]\n",
    "\n",
    "    # Print top k results\n",
    "    print(\"Top subtexts with highest feature activation:\")\n",
    "    for i, (subtext, activation) in enumerate(zip(sampled_subtexts, sampled_activations), 1):\n",
    "        print(f\"{i}. Activation: {activation:.4f}\")\n",
    "        print(f\"   Text: {subtext}\")\n",
    "        print()\n",
    "\n",
    "    random.shuffle(sampled_subtexts)  # just as assurance\n",
    "    scorer_texts = sampled_subtexts[:n_scorer_texts]\n",
    "    explainer_texts = sampled_subtexts[n_scorer_texts:]\n",
    "\n",
    "    # get explanation\n",
    "    def get_subject_logits(text, layer, intervention_strength=0.0, position=-1):\n",
    "        for l in range(len(subject.transformer.h)):\n",
    "            subject.transformer.h[l]._forward_hooks.clear()\n",
    "        subject.transformer.h[layer].register_forward_hook(partial(intervene, intervention_strength=intervention_strength, position=-1))\n",
    "\n",
    "        inputs = subject_tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(subject_device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = subject(**inputs)\n",
    "\n",
    "        return outputs.logits[0, -1, :]\n",
    "\n",
    "    intervention_examples = []\n",
    "    for text in explainer_texts:\n",
    "        clean_logits = get_subject_logits(text, feat_layer, intervention_strength=0.0)\n",
    "        intervened_logits = get_subject_logits(text, feat_layer, intervention_strength=explainer_intervention_strength)\n",
    "        top_probs = (intervened_logits.softmax(dim=-1) - clean_logits.softmax(dim=-1)).topk(n_intervention_examples)\n",
    "        \n",
    "        top_tokens = [subject_tokenizer.decode(i) for i in top_probs.indices]\n",
    "        top_p_increases = top_probs.values.tolist()\n",
    "        intervention_examples.append(\n",
    "            ExplainerInterventionExample(\n",
    "                prompt=text,\n",
    "                top_tokens=top_tokens,\n",
    "                top_p_increases=top_p_increases\n",
    "            )\n",
    "        )\n",
    "\n",
    "    neuron_prompter = ExplainerNeuronFormatter(\n",
    "        intervention_examples=intervention_examples\n",
    "    )\n",
    "\n",
    "    # TODO: improve the few-shot examples\n",
    "    explainer_prompt = get_explainer_prompt(neuron_prompter, fs_examples)\n",
    "    explainer_input_ids = explainer_tokenizer(explainer_prompt, return_tensors=\"pt\").input_ids.to(explainer_device)\n",
    "    with torch.inference_mode():\n",
    "        samples = explainer.generate(explainer_input_ids, max_new_tokens=100, eos_token_id=explainer_tokenizer.encode(\"\\n\\n\")[-1], num_return_sequences=5)[:, explainer_input_ids.shape[1]:]\n",
    "    explanations = Counter([explainer_tokenizer.decode(sample).split(\"\\n\\n\")[0].strip() for sample in samples])\n",
    "    explanation = explanations.most_common(1)[0][0]\n",
    "    print(explanations)\n",
    "\n",
    "    predictiveness_scores = []\n",
    "    max_intervened_probs = []\n",
    "    for scorer_intervention_strength in tqdm(scorer_intervention_strengths):\n",
    "        \n",
    "        predictiveness_score = torch.tensor(0.0, device=scorer_device)\n",
    "        max_intervened_prob = 0.0\n",
    "        \n",
    "        for text in scorer_texts:\n",
    "            \n",
    "            intervened_probs = get_subject_logits(text, feat_layer, intervention_strength=scorer_intervention_strength).softmax(dim=-1).to(scorer_device)\n",
    "            max_intervened_prob = max(max_intervened_prob, intervened_probs.max().item())\n",
    "\n",
    "            # get the explanation predictiveness\n",
    "            scorer_predictiveness_prompt = get_scorer_predictiveness_prompt(text, explanation, few_shot_prompts, few_shot_explanations, few_shot_tokens)\n",
    "            scorer_input_ids = scorer_tokenizer(scorer_predictiveness_prompt, return_tensors=\"pt\").input_ids.to(scorer_device)\n",
    "            with torch.inference_mode():\n",
    "                scorer_logits = scorer(scorer_input_ids).logits[0, -1, :]\n",
    "                scorer_logp = scorer_logits.log_softmax(dim=-1)\n",
    "            \n",
    "            predictiveness_score += (intervened_probs[subject_ids] * scorer_logp[scorer_ids]).sum()\n",
    "\n",
    "        max_intervened_probs.append(max_intervened_prob)\n",
    "        predictiveness_scores.append(predictiveness_score.item() / len(scorer_texts))\n",
    "        \n",
    "    predictiveness_score = sum(predictiveness_scores) / len(predictiveness_scores)\n",
    "    max_intervened_prob = max(max_intervened_probs)\n",
    "    all_results.append({\n",
    "        \"feat_idx\": feat_idx,\n",
    "        \"feat_layer\": feat_layer,\n",
    "        \"explanation\": explanation,\n",
    "        \"predictiveness_score\": predictiveness_score,\n",
    "        \"intervention_examples\": intervention_examples,\n",
    "        \"max_intervened_prob\": max_intervened_prob,\n",
    "        \"scorer_intervention_strengths\": scorer_intervention_strengths,\n",
    "        \"explainer_intervention_strength\": explainer_intervention_strength,\n",
    "        \"scorer_texts\": scorer_texts,\n",
    "        \"explainer_texts\": explainer_texts,\n",
    "        \"predictiveness_scores\": predictiveness_scores,\n",
    "        \"max_intervened_probs\": max_intervened_probs,\n",
    "    })\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>intervention_examples</th>\n",
       "      <th>max_intervened_prob</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_scores</th>\n",
       "      <th>max_intervened_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>6</td>\n",
       "      <td>commas</td>\n",
       "      <td>-9.046546</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Larry Sh...</td>\n",
       "      <td>0.989414</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[1998 Piberstein Styrian Open – Singles\\n\\nBar...</td>\n",
       "      <td>[Larry Sharpe\\n\\nLarry Sharpe may refer to:\\n\\...</td>\n",
       "      <td>[-6.448710632324219, -6.621741485595703, -9.79...</td>\n",
       "      <td>[0.9894140958786011, 0.9211587905883789, 0.721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>6</td>\n",
       "      <td>verbs</td>\n",
       "      <td>-9.411000</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Continuo...</td>\n",
       "      <td>0.995093</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Arvid Kramer\\n\\nArvid Kramer (born October 3,...</td>\n",
       "      <td>[Continuous external counterpressure during cl...</td>\n",
       "      <td>[-6.721965789794922, -6.747789764404297, -7.75...</td>\n",
       "      <td>[0.9905104041099548, 0.9950931072235107, 0.993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>-9.426538</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Effects ...</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[1. Field of the Invention\\nThis invention gen...</td>\n",
       "      <td>[Effects of acute olanzapine exposure on centr...</td>\n",
       "      <td>[-8.017688751220703, -8.224576568603515, -10.1...</td>\n",
       "      <td>[0.9996201992034912, 0.9982855916023254, 0.506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>11</td>\n",
       "      <td>punctuation</td>\n",
       "      <td>-10.581014</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='z(w) = -...</td>\n",
       "      <td>0.949169</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[\\nIf you're, What to do with Doubt... The ver...</td>\n",
       "      <td>[z(w) = -w**3 - 3*w**2 - 3*w - 1. Suppose 4*l ...</td>\n",
       "      <td>[-7.878225708007813, -7.977081298828125, -8.57...</td>\n",
       "      <td>[0.9491687417030334, 0.9463328123092651, 0.859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10001</td>\n",
       "      <td>11</td>\n",
       "      <td>capitalization\\n&lt;|end_of_text|&gt;&lt;|begin_of_text...</td>\n",
       "      <td>-10.795758</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='He procl...</td>\n",
       "      <td>0.816027</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Background {#Sec1}\\n==========\\n\\nHereditary ...</td>\n",
       "      <td>[He proclaims Ireland's most famous day \"a gre...</td>\n",
       "      <td>[-8.285784149169922, -8.428328704833984, -9.03...</td>\n",
       "      <td>[0.8160274028778076, 0.7745303511619568, 0.651...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2</td>\n",
       "      <td>capitalization</td>\n",
       "      <td>-10.861929</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='El ojo d...</td>\n",
       "      <td>0.940045</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[From the dome to your home. Ramblings, mutter...</td>\n",
       "      <td>[El ojo de vidrio\\n\\nEl ojo de vidrio may refe...</td>\n",
       "      <td>[-7.5106353759765625, -7.581437683105468, -12....</td>\n",
       "      <td>[0.9400449991226196, 0.9275412559509277, 0.855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_idx  feat_layer                                        explanation  \\\n",
       "1     10000           6                                             commas   \n",
       "4     10001           6                                              verbs   \n",
       "0     10000           2                                                the   \n",
       "2     10000          11                                        punctuation   \n",
       "5     10001          11  capitalization\\n<|end_of_text|><|begin_of_text...   \n",
       "3     10001           2                                     capitalization   \n",
       "\n",
       "   predictiveness_score                              intervention_examples  \\\n",
       "1             -9.046546  [ExplainerInterventionExample(prompt='Larry Sh...   \n",
       "4             -9.411000  [ExplainerInterventionExample(prompt='Continuo...   \n",
       "0             -9.426538  [ExplainerInterventionExample(prompt='Effects ...   \n",
       "2            -10.581014  [ExplainerInterventionExample(prompt='z(w) = -...   \n",
       "5            -10.795758  [ExplainerInterventionExample(prompt='He procl...   \n",
       "3            -10.861929  [ExplainerInterventionExample(prompt='El ojo d...   \n",
       "\n",
       "   max_intervened_prob scorer_intervention_strengths  \\\n",
       "1             0.989414      [10, 32, 100, 320, 1000]   \n",
       "4             0.995093      [10, 32, 100, 320, 1000]   \n",
       "0             0.999620      [10, 32, 100, 320, 1000]   \n",
       "2             0.949169      [10, 32, 100, 320, 1000]   \n",
       "5             0.816027      [10, 32, 100, 320, 1000]   \n",
       "3             0.940045      [10, 32, 100, 320, 1000]   \n",
       "\n",
       "   explainer_intervention_strength  \\\n",
       "1                               32   \n",
       "4                               32   \n",
       "0                               32   \n",
       "2                               32   \n",
       "5                               32   \n",
       "3                               32   \n",
       "\n",
       "                                        scorer_texts  \\\n",
       "1  [1998 Piberstein Styrian Open – Singles\\n\\nBar...   \n",
       "4  [Arvid Kramer\\n\\nArvid Kramer (born October 3,...   \n",
       "0  [1. Field of the Invention\\nThis invention gen...   \n",
       "2  [\\nIf you're, What to do with Doubt... The ver...   \n",
       "5  [Background {#Sec1}\\n==========\\n\\nHereditary ...   \n",
       "3  [From the dome to your home. Ramblings, mutter...   \n",
       "\n",
       "                                     explainer_texts  \\\n",
       "1  [Larry Sharpe\\n\\nLarry Sharpe may refer to:\\n\\...   \n",
       "4  [Continuous external counterpressure during cl...   \n",
       "0  [Effects of acute olanzapine exposure on centr...   \n",
       "2  [z(w) = -w**3 - 3*w**2 - 3*w - 1. Suppose 4*l ...   \n",
       "5  [He proclaims Ireland's most famous day \"a gre...   \n",
       "3  [El ojo de vidrio\\n\\nEl ojo de vidrio may refe...   \n",
       "\n",
       "                               predictiveness_scores  \\\n",
       "1  [-6.448710632324219, -6.621741485595703, -9.79...   \n",
       "4  [-6.721965789794922, -6.747789764404297, -7.75...   \n",
       "0  [-8.017688751220703, -8.224576568603515, -10.1...   \n",
       "2  [-7.878225708007813, -7.977081298828125, -8.57...   \n",
       "5  [-8.285784149169922, -8.428328704833984, -9.03...   \n",
       "3  [-7.5106353759765625, -7.581437683105468, -12....   \n",
       "\n",
       "                                max_intervened_probs  \n",
       "1  [0.9894140958786011, 0.9211587905883789, 0.721...  \n",
       "4  [0.9905104041099548, 0.9950931072235107, 0.993...  \n",
       "0  [0.9996201992034912, 0.9982855916023254, 0.506...  \n",
       "2  [0.9491687417030334, 0.9463328123092651, 0.859...  \n",
       "5  [0.8160274028778076, 0.7745303511619568, 0.651...  \n",
       "3  [0.9400449991226196, 0.9275412559509277, 0.855...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "# all_df.to_pickle(f\"counterfactual_results/3layers_200feats.pkl\")\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExplainerInterventionExample(prompt='Larry Sharpe\\\\n\\\\nLarry Sharpe may refer to:\\\\n\\\\nLarry Sharpe (politician) (born 1968), American business consultant and political activist \\\\nLarry Sharpe (wrestler) (1951–2017', top_tokens=[')', ');', ',', ' and', ':'], top_p_increases=[0.0795612633228302, 0.0013725794851779938, 0.0009166579693555832, 0.0003981509362347424, 0.0002951501519419253]),\n",
       " ExplainerInterventionExample(prompt='Reflux gastritis and dysplasia.\\\\nIn order to evaluate if duodenogastric reflux (DGR) is associated with a different frequency of gastric dysplasia in comparison with the absence of DGR. 40', top_tokens=[',', '-', ',', ' All', '–'], top_p_increases=[0.09463351964950562, 0.029508035629987717, 0.024005532264709473, 0.003698201384395361, 0.003605559468269348]),\n",
       " ExplainerInterventionExample(prompt='Careers\\\\n\\\\nOpen Positions\\\\n\\\\nAt', top_tokens=[' the', ' least', ' The', ' our', ' a'], top_p_increases=[0.15025703608989716, 0.020413324236869812, 0.006305559538304806, 0.00379744078963995, 0.0035728327929973602])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.iloc[0].intervention_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1998 Piberstein Styrian Open – Singles\\n\\nBarbara Schett was the defending champion but lost in the quarterfinals to Emmanuelle Gagliardi.\\n\\nPatty Schnyder won in the final 6–2, 4–6, 6–3 against Gala León Garc',\n",
       " 'Q:\\n\\nfilesystem for archiving\\n\\nI have some complex read-only data in',\n",
       " 'Happy to help/happy to be here: Identifying components of successful clinical placements for undergraduate nursing students',\n",
       " 'Introduction {#Sec1}\\n============\\n\\nMultiple sclerosis (MS) is an autoimmune inflammatory demyelinating disease, pathologically characterized by perivascular CD4^+^ T cells and mon',\n",
       " 'Silica nanoparticle sols. Part 3: Monitoring the state of agglomeration at the air/water interface',\n",
       " \"According to a newly published report by Dell'Oro Group, the number of point-to-point\",\n",
       " 'West Virginia History OnView (WVHOV) in the West Virginia & Regional History Center’s online database that includes over 50,000 images digitized from our rich and',\n",
       " 'I am giving this daylily a neutral rating because of the height of the scapes. It is listed as blooming on 24 inch scapes, but my plant has always been much, much shorter. The flowers are large and very pretty, though. I just wish',\n",
       " 'Unterhausen',\n",
       " 'Q:\\n\\nLooping over strings\\n\\nHaving fun with strings, I found three different macros to loop over a string character by character.\\nHowever, I am not very sure how they work exactly.\\nCan']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.iloc[0].scorer_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(all_results)\n",
    "all_df = all_df.sort_values(\"predictiveness_score\", ascending=False)\n",
    "all_df.to_pickle(f\"counterfactual_results/{len(feat_layers)}layers_{len(feat_idxs)}feats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_idx</th>\n",
       "      <th>feat_layer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>predictiveness_score</th>\n",
       "      <th>intervention_examples</th>\n",
       "      <th>max_intervened_prob</th>\n",
       "      <th>scorer_intervention_strengths</th>\n",
       "      <th>explainer_intervention_strength</th>\n",
       "      <th>scorer_texts</th>\n",
       "      <th>explainer_texts</th>\n",
       "      <th>predictiveness_scores</th>\n",
       "      <th>max_intervened_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>182</td>\n",
       "      <td>6</td>\n",
       "      <td>1-2-3-4-0-9</td>\n",
       "      <td>-8.721251</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Road', t...</td>\n",
       "      <td>0.231213</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Project, March, 1, Sim, 2019, 2016, 1, A, Hou...</td>\n",
       "      <td>[Road, A, Value]</td>\n",
       "      <td>[-10.622151947021484, -10.456940460205079, -8....</td>\n",
       "      <td>[0.11679935455322266, 0.1188754290342331, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>1,2,3,4</td>\n",
       "      <td>-8.875147</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Road', t...</td>\n",
       "      <td>0.226274</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Project, March, 1, Sim, 2019, 2016, 1, A, Hou...</td>\n",
       "      <td>[Road, A, Value]</td>\n",
       "      <td>[-10.631166076660156, -10.523173522949218, -8....</td>\n",
       "      <td>[0.11783038079738617, 0.12202408164739609, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>185</td>\n",
       "      <td>6</td>\n",
       "      <td>2019, value, house</td>\n",
       "      <td>-8.937399</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='2019', t...</td>\n",
       "      <td>0.431593</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Q, 200, 538, A, A, A, 1, ', 1]</td>\n",
       "      <td>[2019, Value, House]</td>\n",
       "      <td>[-10.816554260253906, -10.416304779052734, -9....</td>\n",
       "      <td>[0.11420270800590515, 0.11989623308181763, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>185</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>-8.978640</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='2019', t...</td>\n",
       "      <td>0.435408</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Q, 200, 538, A, A, A, 1, ', 1]</td>\n",
       "      <td>[2019, Value, House]</td>\n",
       "      <td>[-10.726454925537109, -10.404013061523438, -9....</td>\n",
       "      <td>[0.11452289670705795, 0.12135178595781326, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>-9.364605</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='Project'...</td>\n",
       "      <td>0.943791</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[Sim, 2019, G, Ay, A, A, A, 200, ', L]</td>\n",
       "      <td>[Project, G, 2016]</td>\n",
       "      <td>[-10.697615814208984, -10.711712646484376, -9....</td>\n",
       "      <td>[0.07175029814243317, 0.06015954166650772, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>-13.192244</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='2019', t...</td>\n",
       "      <td>0.992656</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Sim, 1, ', A, A, 1, X, A, H]</td>\n",
       "      <td>[2019, 200, House]</td>\n",
       "      <td>[-11.074691772460938, -11.361431121826172, -12...</td>\n",
       "      <td>[0.12653400003910065, 0.16054397821426392, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>134</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>-13.266629</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='2019', t...</td>\n",
       "      <td>0.962138</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Sim, 1, ', A, A, 1, X, A, H]</td>\n",
       "      <td>[2019, 200, House]</td>\n",
       "      <td>[-10.987806701660157, -11.11118392944336, -11....</td>\n",
       "      <td>[0.11369820684194565, 0.11930322647094727, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>179</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>-13.326119</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='2019', t...</td>\n",
       "      <td>0.962642</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Q, 1, 200, A, A, 1, 538, A, Value]</td>\n",
       "      <td>[2019, ', X]</td>\n",
       "      <td>[-10.844153594970702, -10.966635131835938, -11...</td>\n",
       "      <td>[0.11189045011997223, 0.11463377624750137, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>92</td>\n",
       "      <td>11</td>\n",
       "      <td>1-200</td>\n",
       "      <td>-13.544800</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='A', top_...</td>\n",
       "      <td>0.951868</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Road, L, 1, *, A, 538, Value, A, ']</td>\n",
       "      <td>[A, 200, 1]</td>\n",
       "      <td>[-10.737577056884765, -10.89888916015625, -11....</td>\n",
       "      <td>[0.11086613684892654, 0.1106131449341774, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>1-1000</td>\n",
       "      <td>-13.582887</td>\n",
       "      <td>[ExplainerInterventionExample(prompt='A', top_...</td>\n",
       "      <td>0.997486</td>\n",
       "      <td>[10, 32, 100, 320, 1000]</td>\n",
       "      <td>32</td>\n",
       "      <td>[2016, Road, L, 1, *, A, 538, Value, A, ']</td>\n",
       "      <td>[A, 200, 1]</td>\n",
       "      <td>[-10.897045135498047, -11.134815216064453, -11...</td>\n",
       "      <td>[0.11117079854011536, 0.11355404555797577, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_idx  feat_layer         explanation  predictiveness_score  \\\n",
       "547       182           6         1-2-3-4-0-9             -8.721251   \n",
       "546       182           2             1,2,3,4             -8.875147   \n",
       "556       185           6  2019, value, house             -8.937399   \n",
       "555       185           2                2019             -8.978640   \n",
       "183        61           2                2016             -9.364605   \n",
       "..        ...         ...                 ...                   ...   \n",
       "402       134           2                2019            -13.192244   \n",
       "404       134          11                2019            -13.266629   \n",
       "539       179          11                2019            -13.326119   \n",
       "278        92          11               1-200            -13.544800   \n",
       "276        92           2              1-1000            -13.582887   \n",
       "\n",
       "                                 intervention_examples  max_intervened_prob  \\\n",
       "547  [ExplainerInterventionExample(prompt='Road', t...             0.231213   \n",
       "546  [ExplainerInterventionExample(prompt='Road', t...             0.226274   \n",
       "556  [ExplainerInterventionExample(prompt='2019', t...             0.431593   \n",
       "555  [ExplainerInterventionExample(prompt='2019', t...             0.435408   \n",
       "183  [ExplainerInterventionExample(prompt='Project'...             0.943791   \n",
       "..                                                 ...                  ...   \n",
       "402  [ExplainerInterventionExample(prompt='2019', t...             0.992656   \n",
       "404  [ExplainerInterventionExample(prompt='2019', t...             0.962138   \n",
       "539  [ExplainerInterventionExample(prompt='2019', t...             0.962642   \n",
       "278  [ExplainerInterventionExample(prompt='A', top_...             0.951868   \n",
       "276  [ExplainerInterventionExample(prompt='A', top_...             0.997486   \n",
       "\n",
       "    scorer_intervention_strengths  explainer_intervention_strength  \\\n",
       "547      [10, 32, 100, 320, 1000]                               32   \n",
       "546      [10, 32, 100, 320, 1000]                               32   \n",
       "556      [10, 32, 100, 320, 1000]                               32   \n",
       "555      [10, 32, 100, 320, 1000]                               32   \n",
       "183      [10, 32, 100, 320, 1000]                               32   \n",
       "..                            ...                              ...   \n",
       "402      [10, 32, 100, 320, 1000]                               32   \n",
       "404      [10, 32, 100, 320, 1000]                               32   \n",
       "539      [10, 32, 100, 320, 1000]                               32   \n",
       "278      [10, 32, 100, 320, 1000]                               32   \n",
       "276      [10, 32, 100, 320, 1000]                               32   \n",
       "\n",
       "                                          scorer_texts       explainer_texts  \\\n",
       "547  [Project, March, 1, Sim, 2019, 2016, 1, A, Hou...      [Road, A, Value]   \n",
       "546  [Project, March, 1, Sim, 2019, 2016, 1, A, Hou...      [Road, A, Value]   \n",
       "556              [2016, Q, 200, 538, A, A, A, 1, ', 1]  [2019, Value, House]   \n",
       "555              [2016, Q, 200, 538, A, A, A, 1, ', 1]  [2019, Value, House]   \n",
       "183             [Sim, 2019, G, Ay, A, A, A, 200, ', L]    [Project, G, 2016]   \n",
       "..                                                 ...                   ...   \n",
       "402                [2016, Sim, 1, ', A, A, 1, X, A, H]    [2019, 200, House]   \n",
       "404                [2016, Sim, 1, ', A, A, 1, X, A, H]    [2019, 200, House]   \n",
       "539          [2016, Q, 1, 200, A, A, 1, 538, A, Value]          [2019, ', X]   \n",
       "278         [2016, Road, L, 1, *, A, 538, Value, A, ']           [A, 200, 1]   \n",
       "276         [2016, Road, L, 1, *, A, 538, Value, A, ']           [A, 200, 1]   \n",
       "\n",
       "                                 predictiveness_scores  \\\n",
       "547  [-10.622151947021484, -10.456940460205079, -8....   \n",
       "546  [-10.631166076660156, -10.523173522949218, -8....   \n",
       "556  [-10.816554260253906, -10.416304779052734, -9....   \n",
       "555  [-10.726454925537109, -10.404013061523438, -9....   \n",
       "183  [-10.697615814208984, -10.711712646484376, -9....   \n",
       "..                                                 ...   \n",
       "402  [-11.074691772460938, -11.361431121826172, -12...   \n",
       "404  [-10.987806701660157, -11.11118392944336, -11....   \n",
       "539  [-10.844153594970702, -10.966635131835938, -11...   \n",
       "278  [-10.737577056884765, -10.89888916015625, -11....   \n",
       "276  [-10.897045135498047, -11.134815216064453, -11...   \n",
       "\n",
       "                                  max_intervened_probs  \n",
       "547  [0.11679935455322266, 0.1188754290342331, 0.14...  \n",
       "546  [0.11783038079738617, 0.12202408164739609, 0.1...  \n",
       "556  [0.11420270800590515, 0.11989623308181763, 0.1...  \n",
       "555  [0.11452289670705795, 0.12135178595781326, 0.1...  \n",
       "183  [0.07175029814243317, 0.06015954166650772, 0.4...  \n",
       "..                                                 ...  \n",
       "402  [0.12653400003910065, 0.16054397821426392, 0.1...  \n",
       "404  [0.11369820684194565, 0.11930322647094727, 0.1...  \n",
       "539  [0.11189045011997223, 0.11463377624750137, 0.0...  \n",
       "278  [0.11086613684892654, 0.1106131449341774, 0.08...  \n",
       "276  [0.11117079854011536, 0.11355404555797577, 0.1...  \n",
       "\n",
       "[600 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_df = pd.read_pickle(f\"counterfactual_results/3layers_200feats.pkl\")\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
